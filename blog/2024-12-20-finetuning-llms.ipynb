{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "author: Sachin Abeywardana\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- LLM\n",
    "date: '2024-12-20'\n",
    "description: \"Tutorial on finetuning LLMs via HF transformers library with wandb logging\"\n",
    "image: ../images/finetune_llm.jpg\n",
    "title: \"LLM Finetuning: Demystifying Huggingface Trainer ðŸš€\"\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/finetune_llm.jpg)\n",
    "For a long time, I avoided using the Hugging Face Trainer because it didnâ€™t offer the level of fine-grained control I preferred compared to pure PyTorch. Additionally, I struggled to find a comprehensive tutorial that demonstrated how to log examples post-trainingâ€”something I consider essential for evaluating any training run. In this blog, Iâ€™ll walk you through training a large language model (LLM), integrating Weights & Biases (wandb) for tracking, and highlight some key gotchas to watch out for along the way.\n",
    "\n",
    "In this tutorial, weâ€™ll use the `Qwen2.5-1.5B-Instruct` model to fine-tune a detector for prompt injection attempts, leveraging the `xTRam1/safe-guard-prompt-injection` dataset. While this dataset is a great starting point, itâ€™s worth noting that its labels can be subjectiveâ€”after reviewing the examples, I found that I would have classified some cases differently. If youâ€™re building your own prompt-injection detection model, itâ€™s important to critically evaluate and curate your dataset.\n",
    "\n",
    "The full implementation is available in [this kaggle kernel](https://www.kaggle.com/code/sachin/finetune-llm). If you find it helpful, please consider upvoting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "For Hugging Face models to compute the loss, they require `input_ids`, `attention_mask`, and `labels` to be provided. This means we need a custom collate function that generates these three items in a dictionary. Additionally, since weâ€™re using an `*-instruct` model, the input must adhere to a specific format. While the model can technically train without this formatting, doing so can significantly slow down convergence.\n",
    "\n",
    "To add another layer of complexity, different models might have slightly varying prompt structures. However, the format outlined below appears to be the most widely used and serves as a good starting point.\n",
    "\n",
    "```python\n",
    "def _format_message(tokenizer: transformers.PreTrainedTokenizer, query: str, answer: Optional[int] = None) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. You reply with just yes or no!\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Is the following a prompt injection attempt. {query}\"},\n",
    "    ]\n",
    "    if answer is not None:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": \"yes\" if answer == 1 else \"no\"})\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=answer is None,\n",
    "    )\n",
    "    return text.strip()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Loss\n",
    "\n",
    "In most tutorials Iâ€™ve encountered, the loss is either calculated over the entire sequence or masked only for padded values. However, when dealing with a scenario where the input includes a query and the goal is to generate a corresponding answer, itâ€™s unnecessary to train the model to regenerate the query itself. Instead, we should focus the loss calculation on the generated answer portion of the sequence.\n",
    "\n",
    "To achieve this, we can use the following snippet to calculate the appropriate loss mask:\n",
    "\n",
    "```python\n",
    "def _get_assistant_mask(labels: torch.Tensor, assistant_token: int) -> torch.Tensor:\n",
    "    last_indices = (labels == assistant_token).cumsum(dim=1).argmax(dim=1)\n",
    "    mask = torch.arange(labels.size(1), device=labels.device).unsqueeze(0) <= last_indices.unsqueeze(1)\n",
    "    return mask\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "To mask everything before the word â€œassistant,â€ we first identify its position within the sequence. In our example, we know the output only contains â€œyesâ€ or â€œno.â€ However, if your output may also include occurrences of the word â€œassistant,â€ youâ€™ll need to adapt this approach accordingly.\n",
    "\n",
    "Here, `labels` is simply the same as `input_ids`. The line `(labels == assistant_token).cumsum(dim=1)` identifies all positions where the token for â€œassistantâ€ appears by performing a cumulative sum along the sequence. This ensures we find the last occurrence of the word â€œassistant.â€ Using `argmax`, we then locate the position where this maximum cumulative value occurs, effectively giving us the final index of â€œassistantâ€ in each sequence.\n",
    "\n",
    "To create the mask efficiently and avoid using a for loop, we construct an array of shape `(1, labels.shape[1])` with values ranging from `0` to `labels.shape[1] - 1`. Next, we reshape the `last_indices` we calculated earlier to `(labels.shape[0], 1)` using `unsqueeze(1)`. Broadcasting is then applied to generate a matrix of the same shape as `labels`, allowing us to create the mask that selectively includes only the relevant portions of the sequence for loss calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks and Wandb logging\n",
    "\n",
    "Callbacks are one of the features that elevate the Hugging Face Trainer into a fully-fledged PyTorch powerhouse. While the loss calculation is abstracted within the `transformers` `Trainer` class, callbacks provide us with the flexibility to manipulate or extend the training process as needed.\n",
    "\n",
    "In this example, weâ€™ll use callbacks to generate model responses over the validation set. This will be done both at the start of the training cycle and at the end of each epoch. This allows us to monitor the modelâ€™s performance and behavior throughout the training process.\n",
    "\n",
    "A complete list of available callback options can be found [here](https://huggingface.co/docs/transformers/en/main_classes/callback#transformers.TrainerCallback)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class WandbPredictionProgressCallback(transformers.TrainerCallback):\n",
    "    def __init__(self, trainer, tokenizer, val_dataset, num_samples=100):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.valid_dataloader = DataLoader(\n",
    "            val_dataset.select(range(num_samples)),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            pin_memory=True,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def log_examples(self, state, model):\n",
    "        logger.info(f\"Starting to log examples at global step {state.global_step}\")\n",
    "        model.eval()\n",
    "        output_texts = []\n",
    "        prompt_texts = []\n",
    "        for batch in self.valid_dataloader:\n",
    "            texts = [_format_message(self.tokenizer, text) for text in batch[\"text\"]]\n",
    "            prompt_texts.extend(texts)\n",
    "            tokenized_text = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, padding_side=\"left\").to(model.device)\n",
    "            output_ids = model.generate(\n",
    "                **tokenized_text,\n",
    "                max_new_tokens=5,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            output_texts.extend(\n",
    "                [\n",
    "                    self.tokenizer.decode(\n",
    "                        output[attention_mask.argmax().item():][attention_mask.sum().item():], \n",
    "                        skip_special_tokens=True\n",
    "                    ) for attention_mask, output in zip(tokenized_text.attention_mask, output_ids)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"query_text\": self.valid_dataloader.dataset[\"text\"],\n",
    "                \"actual_answer\": [\"yes\" if label == 1 else \"no\" for label in self.valid_dataloader.dataset[\"label\"]],\n",
    "                \"predicted_answer\": output_texts,\n",
    "                \"prompt_text\": prompt_texts,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        wandb.log({f\"validation_results_gs_{state.global_step}\": wandb.Table(dataframe=df)})\n",
    "        logger.info(f\"Finsihed logging examples at global step {state.global_step}\")\n",
    "        model.train()\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        super().on_evaluate(args, state, control, **kwargs)\n",
    "        self.log_examples(state, self.trainer.model)\n",
    "        \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        super().on_evaluate(args, state, control, **kwargs)\n",
    "        self.log_examples(state, self.trainer.model)\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "\n",
    "- We access the model by doing `trainer.model`. Not sure if this was necessary.\n",
    "- `model.generate` only generates up to 5 new tokens, and does so in a deterministic way.\n",
    "- To ensure proper alignment when generating text, we need to pad from the left. This is why we set `padding_side=\"left\"`. Padding from the right, which is the default behaviour, can result in the shorter sequence receiving padding in the middle of the input. This disrupts the modelâ€™s ability to generate coherent text, as the padded tokens interfere with the sequence structure. By padding on the left, the input sequence remains contiguous, preserving the logical flow for text generation.\n",
    "- To decode just the answer we need to:\n",
    "    - `output[attention_mask.argmax().item():]` for that row because padding is from the left.\n",
    "    - `[attention_mask.sum().item():]` of the result of above to remove input text.\n",
    "    - `skip_special_tokens=True` to remove special tokens that make the output look messy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The final training code is as follows:\n",
    "\n",
    "```python\n",
    "collate_fn = CollateFn(tokenizer)\n",
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    optim=\"paged_adamw_8bit\", # for 8-bit, keep this, else adamw_hf\n",
    "    bf16=True, #Â underlying precision for 8bit\n",
    "    output_dir=f\"./{model_name}-prompt-injection\",\n",
    "    hub_model_id=f\"sachin/{model_name}-prompt-injection\",\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_ds,\n",
    ")\n",
    "wandb_callback = WandbPredictionProgressCallback(trainer, tokenizer, valid_ds)\n",
    "trainer.add_callback(wandb_callback)\n",
    "\n",
    "trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gotchas\n",
    "\n",
    "- I had to use QLora to train the model despite the model only being a 1.5B model. LORA unfortunately didnâ€™t cut it. Perhaps due to having some sentences that were too long to fit in memory.\n",
    "- Donâ€™t forget to set `add_generation_prompt=True` when you are evaluating text, i.e. after training and when you are testing for the solution.\n",
    "\n",
    "```python\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=answer is None,\n",
    ")\n",
    "```\n",
    "\n",
    "- Set `model.eval()` when inferring over the validation example and `model.train()` once you are done. Also use `@torch.inference_mode()` decorator over the function that you validating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Full results can be found in [this wandb report](https://wandb.ai/sachinruk/prompt_injection/runs/xuq4oymr?nw=nwusersachinruk).\n",
    "\n",
    "### Before training\n",
    "\n",
    "As can be seen the predictions have extra text on top of being wrong.\n",
    "\n",
    "![image of wandb table of actual and predicted answers](https://i.imgur.com/IfYLrdz.png)\n",
    "\n",
    "### After training\n",
    "\n",
    "And after training for 514 steps (of batch size 4) we can see that the answers match up. We can also see that it selects the lower case setting as opposed to some of the upper case answers we saw earlier.\n",
    "![image of wandb table of actual and predicted answers](https://i.imgur.com/TAnWWrc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "I have summarised the above in four take-aways:\n",
    "\n",
    "1. **Introduction to Hugging Face Trainer**\n",
    "\n",
    "While the Hugging Face Trainer simplifies many aspects of training, its lack of fine-grained control initially made it less appealing. Logging examples post-training was also not well-documented. This tutorial demonstrates training a large language model (LLM), using Weights & Biases (wandb) for tracking, and tackling common challenges.\n",
    "\n",
    "1. **Collate Function Requirements**\n",
    "\n",
    "Hugging Face models need input_ids, attention_mask, and labels for loss calculation. Custom collate functions ensure these are correctly generated. For `*-instruct` models, inputs must follow a specific format to optimize convergence. This format varies slightly by model but has common patterns.\n",
    "\n",
    "1. **Targeted Loss Masking**\n",
    "\n",
    "When the input includes a query and the goal is to generate an answer, itâ€™s unnecessary to calculate loss for regenerating the query. A custom mask ensures the loss focuses only on the answer portion. Techniques like cumulative sum and broadcasting are used to efficiently identify and mask everything before the target token (e.g., â€œassistantâ€).\n",
    "\n",
    "1. **Using Callbacks for Enhanced Functionality**\n",
    "\n",
    "Callbacks in the Hugging Face Trainer enable customization of the training loop. In this example, callbacks generate responses over the validation set at the beginning of training and at the end of each epoch. This provides insights into model performance during training. The full range of callbacks is available in the Hugging Face documentation.\n",
    "\n",
    "By integrating these techniques, you can fine-tune models effectively, gain detailed insights during training, and handle challenges like loss masking and sequence formatting with precision. The full implementation can be found in [this Kaggle kernel](https://www.kaggle.com/code/sachin/finetune-llm).\n",
    "\n",
    "## References + Kudos\n",
    "\n",
    "1. [Merve noyanâ€™s blog](https://github.com/huggingface/smollm/blob/main/finetuning/Smol_VLM_FT.ipynb), \"Fine-tune SmolVLM on Visual Question Answering using Consumer GPU with QLoRA\"\n",
    "\n",
    "2. [Wandb docs on transformers](https://docs.wandb.ai/guides/integrations/huggingface/#custom-logging-log-and-view-evaluation-samples-during-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
