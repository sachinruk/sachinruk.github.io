{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /pytorch/2022/03/22/ANN-part2\n",
    "author: Sachin Abeywardana\n",
    "branch: master\n",
    "categories:\n",
    "- pytorch\n",
    "date: '2022-03-22'\n",
    "description: Implementing Approximate Nearest Neighbours Oh Yeah (ANNOY)\n",
    "image: ../images/a_vector_database.png\n",
    "output-file: 2022-03-22-ann-part2.html\n",
    "title: Vector Database from Scratch\n",
    "toc: true\n",
    "use_math: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b81fbb-c155-4350-b402-a3fbc5e9daf2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Now that embeddings are becoming a vital part of search algorithms, the next question is how do we do that at scale. There are a lot of vendor Vector Databases popping up, and here we will explore one of those algorithms, ANNOY.\n",
    "We will be implement [Approximate Nearest Neighbours Oh Yeah](https://github.com/spotify/annoy) from scratch. We will use a synthetic dataset of a million points (N) and 768 dimensions (D). If we have K query points the run time of brute force search is $O(KND)$. The ANNOY algorithm aims to bring that down to $O(K \\log N D)$.\n",
    "\n",
    "Disclaimer: More often than not you will find that brute force search is fast enough. Especially if the number of vectors you have is <1M. If you have a GPU you can stretch this even further due to the embarassingly parallel nature of matrix multiplication.\n",
    "\n",
    "![A vector database as generated by Stable Diffusion](../images/a_vector_database.png)\n",
    "> A vector database as generated by Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1476c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29249f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "N = 1000000\n",
    "D = 768\n",
    "x = F.normalize(torch.randn(N, D), dim=-1)\n",
    "\n",
    "x_new = F.normalize(torch.randn(1000, D), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596ece8-c0e5-42ea-b968-f686428afddb",
   "metadata": {},
   "source": [
    "## ANNOY algorithm\n",
    "The premise of the algorithm lies in recursively partitioning the space of data points until `min_leaf` number of data points are left in that sub-space. The following high level steps shows how to construct a tree.\n",
    "1. Initialise the list of indices to include all data points.\n",
    "2. Choose a subset of x based on indices.\n",
    "3. If the number of data points in subset is less that `min_leaf` stop.\n",
    "4. Choose two data points, complete randomly. Store these two.\n",
    "5. Choose data points close to first data point, and set indices to that. Go to 2.\n",
    "6. Choose data points close to second data point, and set indices to that. Go to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "843a8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "class AnnoyTree:\n",
    "    def __init__(self, max_depth, min_leaf, dim):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf = min_leaf\n",
    "        self.labels = None\n",
    "        self.max_level = 2 ** max_depth\n",
    "        # self.centers = torch.zeros(self.max_level + 1, dim)\n",
    "        self.centers = {}\n",
    "        self.leaf = {}\n",
    "        \n",
    "    def fit(self, x, idx=None, current_label=0):\n",
    "        if self.labels is None:\n",
    "            self.labels = np.zeros(len(x), dtype=np.int32)\n",
    "            idx = self.labels == 0\n",
    "            \n",
    "        next_label = 2 * current_label + 1\n",
    "        \n",
    "        x_subset = x[idx]\n",
    "        if len(x_subset) <= self.min_leaf or current_label >= self.max_level:\n",
    "            self.leaf[next_label] = x_subset\n",
    "            return\n",
    "        \n",
    "        # choose 2 points at random\n",
    "        center_idx = np.random.choice(len(x_subset), 2, replace=False)\n",
    "        x_centers = x_subset[center_idx]\n",
    "        self.centers[next_label] = x_centers # save centers\n",
    "        self.labels[idx] = next_label + (x_subset @ x_centers.T).argmax(dim=-1) # trick of 2n + 1, 2n + 2\n",
    "        \n",
    "        # assign left to cluster l\n",
    "        self.fit(x, self.labels == next_label, next_label)\n",
    "        self.fit(x, self.labels == next_label + 1, next_label + 1)\n",
    "        \n",
    "    def predict(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        vecs, similarities = zip(*[self.get_closest(row, 0) for row in x])\n",
    "        return torch.stack(vecs), similarities\n",
    "    \n",
    "    def get_closest(self, x: torch.FloatTensor, idx: int) -> torch.FloatTensor:\n",
    "        current_index = 2 * idx + 1\n",
    "        if current_index in self.leaf:\n",
    "            val, idx = (x @ self.leaf[current_index].T).topk(1)\n",
    "            return self.leaf[current_index][idx.item()], val.item()\n",
    "        # closest_index = 2 * idx + 1 + (x @ self.centers[[2 * idx + 1, 2 * idx + 2]].T).argmax(dim=-1)\n",
    "        closest_index = current_index + (x @ self.centers[current_index].T).argmax(dim=-1)\n",
    "        return self.get_closest(x, closest_index.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405e6ff-d404-40cb-a5e3-64de464ef86d",
   "metadata": {},
   "source": [
    "## `fit` method\n",
    "Let's go through the `fit` method in the above class.\n",
    "```python\n",
    "x_subset = x[idx]\n",
    "if len(x_subset) <= self.min_leaf or current_label >= self.max_level:\n",
    "        self.leaf[2 * current_label + 1] = x_subset\n",
    "        return\n",
    "```\n",
    "The above code saves the subset of datapoints if the conditions for a leaf are met. This is done so that we can compare a query datapoints against our data at a leaf level. Note that this does mean we have $O(ND)$ storage costs. \n",
    "\n",
    "Note that we can use the $2n+1, 2n+2$ trick to make sure that we don't overlap labels. This also ensures that if we need a parent label we can simply do `current_label // 2` to get to the parent label. This avoids us needing to have `left` and `right` nodes.\n",
    "\n",
    "```python\n",
    "center_idx = np.random.choice(len(x_subset), 2, replace=False)\n",
    "x_centers = x_subset[center_idx]\n",
    "self.centers[next_label] = x_centers # save centers\n",
    "self.labels[idx] = next_label + (x_subset @ x_centers.T).argmax(dim=-1)\n",
    "```\n",
    "This code block chooses 2 datapoints randomly and stores them. It also uses this line to assign what the level ought to be from `2n+1, 2n+2`, `next_label + (x_subset @ x_centers.T).argmax(dim=-1)`. This is done since `argmax will return 0 or 1` and we simply add that to `2n+1` to get the child label.\n",
    "\n",
    "The final two lines simply recursively calls the fit method until a stop condition is met.\n",
    "\n",
    "## `predict` method\n",
    "The predict method takes the tree constructed in above step and compares them against the stored branches until a leaf node is reached. Once at a leaf node it does a brute force search to get the closest element in that block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5d08805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "tree = AnnoyTree(15, 1000, x.shape[1])\n",
    "tree.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b1466-36af-46f9-a167-ec32398507b8",
   "metadata": {},
   "source": [
    "## Results\n",
    "As can be seen below, to predict closest vector on 1000 vectors takes 344ms while a full brute force search takes 17 seconds. That's a 50x scale up in speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "14dbf65e-7344-459e-903e-a210e79624cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 342 ms, sys: 3.23 ms, total: 345 ms\n",
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vecs, similarities = tree.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e78d875a-953f-4c92-97dc-3566813febe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.1 s, sys: 1.39 s, total: 34.5 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_similarity, max_idx = (x_new @ x.T).topk(1, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb4df3-f4eb-4589-a8cc-d26df1792976",
   "metadata": {},
   "source": [
    "Given the actual maximum similarity below, we can see that approximate method captures a close vector, but not the best. If you are wondering why the numbers are relatively small (~0.18) keep in mind that two random vectors are highly likely to be orthogonal and very close to zero the higher the number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9c1bb336-9b72-4fb9-9ee6-b84b30580fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1821, 0.1769, 0.1651, 0.1692, 0.1798, 0.1711, 0.1764, 0.1796, 0.1751,\n",
       "        0.1716])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_similarity.squeeze()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2cb70b5f-356c-4244-8144-acd89de6ac8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14211197197437286,\n",
       " 0.10861558467149734,\n",
       " 0.12875324487686157,\n",
       " 0.13691002130508423,\n",
       " 0.12784186005592346,\n",
       " 0.13487347960472107,\n",
       " 0.11535458266735077,\n",
       " 0.11262157559394836,\n",
       " 0.11334729194641113,\n",
       " 0.11169558018445969)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "similarities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d88885-a179-4fcd-a454-a15ac1534dce",
   "metadata": {},
   "source": [
    "## Random Forest Approach\n",
    "In similar spirit to random forests, we can easily extend a single tree into multiple trees. Below we construct 10 trees. Unlike random forest where we average results across trees, here we take the data point with maximum similarity across the best datapoints chosen by each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bb9343aa-831e-4319-8d25-9781213e6a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccd56e9e87f40ae9ac323262093905f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_TREE = 10\n",
    "trees = [AnnoyTree(15, 1000, x.shape[1]) for _ in range(N_TREE)]\n",
    "for tree in tqdm(trees):\n",
    "    tree.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fb3d1476-e017-4121-adc6-fb2b29145006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d51d4e8f526405696272ac7fad168e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vecs, similarities = zip(*[tree.predict(x_new) for tree in tqdm(trees)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42f875-6083-49ec-aac8-8d91f796ca58",
   "metadata": {},
   "source": [
    "The similarity scores shown below are better than the original numbers obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d2db05e7-92bd-4686-b470-e951598408bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1430, 0.1509, 0.1453, 0.1444, 0.1554, 0.1476, 0.1374, 0.1420, 0.1582,\n",
       "        0.1419])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.Tensor(similarity) for similarity in similarities]).amax(dim=0)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36977a2d-b00e-45df-8d7d-edabf35250d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
