{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "author: Sachin Abeywardana\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- LLM\n",
    "date: '2024-11-03'\n",
    "description: \"This study finds NuExtract performs best for structured outputs, with KV caching improving speed and accuracy for larger models despite some hallucinations.\"\n",
    "image: ../images/structured_outputs.webp\n",
    "title: \"Supercharging Structured Outputs with Open Source Models üöÄ\"\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/structured_outputs.webp)\n",
    "In this exploration, I set out to speed up structured outputs by comparing three open-source models. Since caching a common prompt isn‚Äôt directly possible via an API, I opted for the following models:\n",
    "\n",
    "\t1.\tNuExtract-tiny-v1.5\n",
    "\t2.\tQwen-2.5-0.5B\n",
    "\t3.\tPhi-3.5-mini-instruct\n",
    "\n",
    "We specifically chose NuExtract because it claims to outperform GPT-4o for structure extraction (see [their blog](https://numind.ai/blog/nuextract-1-5---multilingual-infinite-context-still-small-and-better-than-gpt-4o) for more details). To follow along see my [Kaggle Kernel](https://www.kaggle.com/code/sachin/structured-outputs/) (please upvovote).\n",
    "\n",
    "For instance, let‚Äôs take this sample text from NuExtract:\n",
    "```plaintext\n",
    "We introduce Mistral 7B, a 7‚Äìbillion-parameter language model engineered for\n",
    "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
    "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
    "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
    "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
    "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
    "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
    "Mistral 7B ‚Äì Instruct, that surpasses Llama 2 13B ‚Äì chat model both on human and\n",
    "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
    "Code: <https://github.com/mistralai/mistral-src>\n",
    "Webpage: <https://mistral.ai/news/announcing-mistral-7b/>\n",
    "```\n",
    "\n",
    "With a target structure like this:\n",
    "```json\n",
    "{\n",
    "    \"Model\": {\n",
    "        \"Name\": \"\",\n",
    "        \"Number of parameters\": \"\",\n",
    "        \"Number of max token\": \"\",\n",
    "        \"Architecture\": []\n",
    "    },\n",
    "    \"Usage\": {\n",
    "        \"Use case\": [],\n",
    "        \"Licence\": \"\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Code üíª\n",
    "\n",
    "Here‚Äôs the code that uses a batch prompt to extract the structure:\n",
    "```python\n",
    "batch_prompts = [f\"\"\"<|input|>\\n### Template:\\n{template}\\n### Text:\\n{text}\\n\\n<|output|>\"\"\"]\n",
    "batch_encodings = tokenizer(batch_prompts, return_tensors=\"pt\", truncation=True, padding=True, max_length=1000).to(model.device)\n",
    "with torch.inference_mode():\n",
    "    pred_ids = model.generate(**batch_encodings, max_new_tokens=200)\n",
    "    output = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    print(output[-1].split(\"<|output|>\")[1])\n",
    "```\n",
    "giving the output (where u2013 is the unicode character, dash).\n",
    "```json\n",
    "{\n",
    "    \"Model\": {\n",
    "        \"Name\": \"Mistral 7B\",\n",
    "        \"Number of parameters\": \"7\\u2013billion\",\n",
    "        \"Number of max token\": \"\",\n",
    "        \"Architecture\": [\n",
    "            \"grouped-query attention (GQA)\",\n",
    "            \"sliding window attention (SWA)\"\n",
    "        ]\n",
    "    },\n",
    "    \"Usage\": {\n",
    "        \"Use case\": [\n",
    "            \"superior performance and efficiency\",\n",
    "            \"reasoning\",\n",
    "            \"mathematics\",\n",
    "            \"code generation\"\n",
    "        ],\n",
    "        \"Licence\": \"Apache 2.0\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "‚ö° Execution Time: ~3.3 seconds on a P100-enabled Kaggle kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Flattening JSON Structure üß±\n",
    "Given that NuExtract uses a specific chat template, for this exercise we are going to drop down to the parent model that NuExtract fine tuned on, Qwen2.5-0.5B-Instruct. We stick to the same size to make a fairer comparison.\n",
    "\n",
    "In flattening the json structure for we can request for the individual keys, `\"Model Name\", \"Number of Model parameters\"` etc.\n",
    "\n",
    "In the previous example we sent the template as the first argument. However, in order to cache the prompt we need to inject the text as the first argument. Therefore we use the following code to extract the keys we are after. \n",
    "\n",
    "Things to note:\n",
    "\n",
    "1. There is a different chat template to the previous example.\n",
    "2. The order of text and the key is swapped.\n",
    "3. We pass in one key at a time.\n",
    "\n",
    "```python\n",
    "def get_text_with_chat_template(text: str, key: str) -> str:\n",
    "    prompt = f\"### Text:\\n{text}\\n### Required Key:\\n{key}\\n\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \"content\": \"You are a helpful assistant that can extract values given a requested key and data type. \\\n",
    "            If you don't know, output \\\"unknown\\\". Be concise and precise. \\\n",
    "            Don't repeat the key in the answer!\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text_with_chat_template = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text_with_chat_template\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_key_value(text_with_chat_template):\n",
    "    batch_encodings = tokenizer([text_with_chat_template], return_tensors=\"pt\", truncation=True, padding=True, max_length=1000).to(model.device)\n",
    "    pred_ids = model.generate(**batch_encodings, max_new_tokens=200)\n",
    "    output = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    return output[-1].split(\"assistant\\n\")[1].strip()\n",
    "\n",
    "keys = [\"Model Name\", \"Number of parameters\", \"Number of max token\", \"Architecture\", \"Use case\", \"Licence\"]\n",
    "for key in keys:\n",
    "    text_with_chat_template = get_text_with_chat_template(text, key)\n",
    "    print(key, \": \", get_key_value(text_with_chat_template))\n",
    "```\n",
    "\n",
    "Resulting in:\n",
    "```plaintext\n",
    "Model Name :  Mistral 7B\n",
    "Number of parameters :  7\n",
    "Number of max token :  7\n",
    "Architecture :  Architecture\n",
    "Use case :  Inferencing\n",
    "Licence :  Apache 2.0\n",
    "```\n",
    "The results are clearly worse off. However, this took only 949ms! A 3x speed up. Most likely due to a shorter prompt. This is not a guaranteed speed up since this very much depends on the length of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: KV Caching üóÑÔ∏è\n",
    "In the above we run the model through a for loop over the number of keys. However, there is a lot in common with each run inside `text_with_chat_template`. We can ‚Äúcache‚Äù this common prompt with the hope of running the O(n^2) operation only once. We run the following code to achieve the same results as above. However, unfortunately, this does take 1.14s meaning for some reason, this is slower.\n",
    "\n",
    "```python\n",
    "root_prompt = text_with_chat_template.split(\"Required Key:\\n\")[0] + \"Required Key:\\n\"\n",
    "root_inputs = tokenizer(text=root_prompt, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    kv_cache = model(**root_inputs, return_dict=True).past_key_values\n",
    "\n",
    "prompt_end = \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for key in keys:\n",
    "        batch_encodings = tokenizer(text= key + prompt_end, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        batch_encodings[\"input_ids\"] = torch.cat([root_inputs[\"input_ids\"], batch_encodings[\"input_ids\"]], dim=-1)\n",
    "        batch_encodings[\"attention_mask\"] = torch.cat([root_inputs[\"attention_mask\"], batch_encodings[\"attention_mask\"]], dim=-1)\n",
    "        pred_ids = model.generate(**batch_encodings, past_key_values=kv_cache, max_new_tokens=200)\n",
    "        output = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        print(key, \": \", output[0].split(\"assistant\\n\")[1].strip())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Larger Model Performance üí™\n",
    "I reattempted the above with Phi-3.5-mini-instruct, a model that has 3B parameters (6x larger). Having more parameters clearly corresponds to higher accuracy. However the following model still hallucinates despite the specific instruction saying `If you don't know, output \"unknown\". Be concise and precise`.\n",
    "```plaintext\n",
    "Model Name :  Mistral 7B\n",
    "Number of parameters :  7 billion\n",
    "Number of max token :  1024\n",
    "Architecture :  Grouped-query attention (GQA) and sliding window attention (SWA)\n",
    "Use case :  Mistral 7B's use case includes superior performance and efficiency in language modeling, outperforming Llama 2 and Llama 1 models in reasoning, mathematics, and code generation. It also features grouped-query attention (GQA) and sliding window attention (SWA) for faster inference and handling long sequences with reduced cost. Additionally, Mistral 7B ‚Äì Instruct is fine-tuned for following instructions, surpassing Llama 2 on both human and automated benchmarks.\n",
    "Licence :  Apache 2.0\n",
    "```\n",
    "\n",
    "However, in this case doing kv caching did speed up the output. This suggests that caching is most useful with larger models. The prompt without caching took 44.8s while caching ran it in 42s. \n",
    "\n",
    "Larger models as expected has the unfortunate side effect of being slower. However, in this case it was slower due to the long winded output of the `Use case` key. We can either limit the number of generated tokens or play around with prompts to have a more concise answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways üìù\n",
    "There are a few observations that‚Äôs worth bringing up.\n",
    "\n",
    "1. NuExtract seems to work best regardless of if we flatten the structure or not. However, please take the above example with a grain of salt as I only tested on one example input, and this input was provided by NuExtract.\n",
    "2. KV caching only helps with larger sized models.\n",
    "3. The other models do tend to hallucinate answers instead of stating ‚Äúunknown‚Äù.\n",
    "\n",
    "It is somewhat disappointing to see that we cannot use off the shelf models to do structured outputs. However, I do wonder if we were to train these models to answer a single key if it would do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
