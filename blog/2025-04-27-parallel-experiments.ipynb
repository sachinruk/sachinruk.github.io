{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8f3988",
   "metadata": {},
   "source": [
    "---\n",
    "author: Sachin Abeywardana\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- MLOps\n",
    "date: '2025-04-27'\n",
    "description: \"This guide dives into setting up large-scale machine learning experiments with a clean code structure, local testing, and efficient parallelization using tools like Typer, Pydantic, and Argo Workflows. Learn how to move beyond notebooks, structure ML projects for scalability, and log results effectively to accelerate model development.\"\n",
    "image: ../images/massively_parallel_experiments.png\n",
    "title: \"Massively Parallel Experimentation: Ship Better Models, Faster ğŸš€\"\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a9916c",
   "metadata": {},
   "source": [
    "![](../images/massively_parallel_experiments.png)\n",
    "\n",
    "\n",
    "In the world of machine learning, success isnâ€™t just about finding the perfect modelâ€”itâ€™s about running as many experiments as possible. Yet, I often see teams tweaking one model at a time (or worse, running everything in notebooks). This is fine if your model trains in minutes, but deep learning? Thatâ€™s an hours-to-days kind of commitment. So, if youâ€™re serious about getting your work production-ready, itâ€™s time to level up.\n",
    "\n",
    "And letâ€™s be realâ€”MLEs love debating whether Approach A is better than Approach B. I say, why argue when you can just test both? Code it up, run the experiments, and let the results speak for themselves.\n",
    "\n",
    "And hereâ€™s a ğŸŒ¶ï¸Â topic, notebooks. Theyâ€™re great for cleaning and structuring data, but once youâ€™ve got that sorted, itâ€™s time to move on to Python files. This makes your project structured, reviewable, and way easier to collaborate on.\n",
    "\n",
    "## Structuring Code (Because Future You Will Thank You) ğŸ› ï¸\n",
    "\n",
    "Every ML project has three main phases:\n",
    "\n",
    "1. **Data** â€“ Get your data cleaned and ready.\n",
    "2. **Train** â€“ Build and test your model.\n",
    "3. **Serve** â€“ Deploy it for real-world use.\n",
    "\n",
    "For deep learning projects, I stick to the following file structure:\n",
    "\n",
    "- **`main.py`** â€“ Orchestrates everything and provides a CLI interface.\n",
    "- **`data.py`** â€“ Handles preprocessing, PyTorch `Dataset`, and collate functions. If things get messy, I split it into smaller files.\n",
    "- **`model.py`** â€“ Defines the model, taking a config that includes hyperparameters (base model name, extra hidden layers, etc.).\n",
    "- **`trainer.py`** â€“ Handles data ingestion, loss calculations, and logs key metrics (precision, recall, false positives).\n",
    "\n",
    "This structure has served me well over the years, but feel free to tweak it to your needs.\n",
    "\n",
    "## Side Quest: Clean Code (Your Teammates Will Love You) ğŸ§¹\n",
    "\n",
    "Before we go any further, letâ€™s have a moment of appreciation for clean code. Itâ€™s not just about looking niceâ€”it helps teams actually collaborate and ship faster. Hereâ€™s my personal checklist:\n",
    "\n",
    "1. **Keep PRs short** â€“ Ideally <100 lines. Anything longer and reviewers start looking like this: [Joma Tech video](https://www.youtube.com/watch?v=rR4n-0KYeKQ&ab_channel=JomaTech).\n",
    "2. **Functions should be <15 lines** â€“ One function, one job.\n",
    "3. **Inject dependencies** â€“ Pass objects into functions instead of creating them inside. This also makes writing tests so much easier!\n",
    "4. **Files should be <150 lines** â€“ If a file is getting too long, refactor and break it up (`data.py` â†’ `data_process.py`, `data_warehouse.py`).\n",
    "5. **Name things well** â€“ `colour_image = image.convert(\"RGB\")` is way better than `img = img.convert(\"RGB\")`.\n",
    "\n",
    "Of course, rules arenâ€™t always absoluteâ€”just do your best.\n",
    "\n",
    "## Parallelising Hypotheses ğŸŸ°\n",
    "\n",
    "At my previous workplaces we used Argo WorkFlows (ArgoWF) to send our models into training. This is an abstraction on top of kubernetes that has worked well for us. However, there are other platforms that does similarly well in this space including AWS Steps, Ray Anyscale, KubeFlow etc.\n",
    "\n",
    "What is important here regardless of the platform is that you spam the platform with all your hypotheses. In order to do so you need to structure your code to handle these cases and log metrics so that you can compare. These metrics need to be able to choose a winner and therefore should be independent of which model type is being tests.\n",
    "\n",
    "In order to do this I use two tools:\n",
    "\n",
    "1. `typer`, a pretty cli tool for python that surpasses `argparse`.\n",
    "2. `pydantic`, for configurations and parsing in arguments from the cli.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "\n",
    "class ModelConfig(pydantic.BaseModel):\n",
    "\t\tvision_model_name: str = \"edgenext_small\"\n",
    "\t\tembed_dims: int = 512\n",
    "\n",
    "class TrainerConfig(pydantic.BaseModel):\n",
    "\t\tis_local: bool = False\n",
    "\t\tlearning_rate: float = 1e-3\n",
    "\t\tbatch_size: int = 8\n",
    "\t\tepochs: int = 2\n",
    "\t\t_model_config: ModelConfig = ModelConfig()\n",
    "```\n",
    "\n",
    "And within [`main.py`](http://main.py) I do:\n",
    "\n",
    "```python\n",
    "import typer\n",
    "\n",
    "app = typer.Typer()\n",
    "\n",
    "@app.command()\n",
    "def train(trainer_config_json: str = typer.Option(\"{}\", help=\"Trainer config JSON string\")):\n",
    "    if \"HF_TOKEN\" not in os.environ:\n",
    "        raise ValueError(\"Please set the HF_TOKEN environment variable.\")\n",
    "    trainer_config = config.TrainerConfig.model_validate_json(trainer_config_json)\n",
    "    transform = vision_model.get_vision_transform(trainer_config._model_config)\n",
    "    train_dl, valid_dl = data.get_dataset(\n",
    "        transform=transform, tokenizer=tokenizer, hyper_parameters=trainer_config  # type: ignore\n",
    "    )\n",
    "    model = vision_model.get_vision_model(trainer_config._model_config)\n",
    "\t\t...\n",
    "\t\t\n",
    "if __name__ == \"__main__\":\n",
    "    app()\n",
    "```\n",
    "\n",
    "## **Local Execution: Sanity Check Before You Burn $$$** ğŸ’¸\n",
    "\n",
    "Before we spam our experimentation platform with costly experiments that require massive GPUs, it is important to get a subset working locally.\n",
    "\n",
    "When executing locally, we do not need to run the full training load. We simply need to make sure that each component of the pipeline from data transformations, to the model, to the training logic, work end to end. Therefore pushing a few batches (but not simply one) over 2 epochs should suffice. I usually choose 2 epochs over 1 to make sure that I donâ€™t see any funny errors popping up at the end of training.\n",
    "\n",
    "I have logic in my training code similar to the following:\n",
    "\n",
    "```python\n",
    "if trainer_config.is_local:\n",
    "\t\tdf = df.iloc[:200]\n",
    "\t\ttrainer_config.epochs = 2\n",
    "```\n",
    "\n",
    "## **Tracking Experiments: Logs or It Didnâ€™t Happen** ğŸ‘€\n",
    "\n",
    "Logging is important so that we can compare each run. Whilst I generally use wandb, there are plenty of other commercial and open source alternatives available. As far as logging goes there are two things that are important:\n",
    "\n",
    "1. Log a metric that makes product sense.\n",
    "2. If your problem is based on images, text, or other media, log visual examples.\n",
    "\n",
    "In terms of logging a metric, this cannot be simply the loss. Loss does not say anything about how â€œaccurateâ€ your model is. While precision and recall are good metrics to track, keep in mind that we can adjust precision at the expense of recall (and vice versa). When it comes to classification (multi-label or otherwise) average AUROC is a good metric. Keep in mind this was only an example of being creative with your metrics, and by all means adjust this to what makes sense.\n",
    "\n",
    "At the end of training it is important to log the results of your model so that you can visually inspect the results. With a recent project that we did, we logged the false positives and false negatives. The surprising result that we observed was that the false positives were more often than not mis labelled as opposed to the model getting it wrong. \n",
    "\n",
    "The following shows the results logged at the end of a captioning model.\n",
    "\n",
    "![image.png](../images/caption_model_results.jpg)\n",
    "\n",
    "## Summary ğŸš€\n",
    "\n",
    "- Run **many** experiments, not just one at a time.\n",
    "- Move from notebooks to Python files once initial exploration is done.\n",
    "- Structure your ML projects into **data, train, and serve** phases.\n",
    "- Keep code **clean and modular** to boost collaboration.\n",
    "- Use **Argo Workflows (or similar)** to parallelise training jobs.\n",
    "- Always **sanity check locally** before running expensive cloud experiments.\n",
    "- **Log everything** so you can compare and debug later.\n",
    "\n",
    "Now go forth and experiment! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e867967b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
