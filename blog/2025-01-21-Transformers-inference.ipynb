{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "author: Sachin Abeywardana\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- LLM\n",
    "- pytorch\n",
    "date: '2025-01-21'\n",
    "description: \"Optimization tricks used for speeding up transformer inference\"\n",
    "image: ../images/hugging_face_rocket.webp\n",
    "title: \"Transformers Inference Optimizations ‚è∞üöÄ\"\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/hugging_face_rocket.webp)\n",
    "Here‚Äôs a quick (and definitely not all-inclusive) write-up of the optimizations I stumbled upon in the last two days. It's a bit rushed, so bear with me on this one! üí®\n",
    "\n",
    "## Install Flash Attention 2 üöÄ\n",
    "\n",
    "Flash Attention 2 is the way to go‚ÄîFlash Attention 3 exists, but as far as I know, it‚Äôs exclusive to H100 machines. Installing it was a fucking headache üòÖ, but the trick was to use a `-devel` Docker container. Also, don‚Äôt forget to install `ninja`; it speeds up the build process like a champ.\n",
    "\n",
    "Here‚Äôs what worked for me:\n",
    "\n",
    "```docker\n",
    "FROM pytorch/pytorch:2.5.1-cuda12.4-cudnn9-devel\n",
    "RUN python --version\n",
    "WORKDIR /app\n",
    "\n",
    "# Install build dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    cmake \\\n",
    "    git \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy the requirements.txt file into the container at /app\n",
    "COPY requirements.txt /app/\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install ninja\n",
    "RUN pip install --no-cache-dir flash-attn --no-build-isolation\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "# # Install CUDA extensions for fused dense\n",
    "# RUN pip install git+https://github.com/Dao-AILab/flash-attention@v2.6.3#subdirectory=csrc/fused_dense_lib\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY ./src/ /my/working/directory/src/\n",
    "\n",
    "RUN addgroup --system somebody && \\\n",
    "    adduser --system --home /app --ingroup somebody somebody && \\\n",
    "    chown -R somebody:somebody /app\n",
    "\n",
    "USER somebody\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONPATH=/app\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "\n",
    "```\n",
    "\n",
    "## Torch tricks üîß‚ú®\n",
    "\n",
    "Here‚Äôs a neat trick: **always compile your model first**! This applies to any model, whether you‚Äôre using transformers or something else. I was surprised to learn it‚Äôs not just about running `model = torch.compile(model)`. Torch uses **lazy compilation**, meaning the first data pass actually kicks off the compile process. So, you‚Äôll want to include a few warm-up iterations.\n",
    "\n",
    "Here‚Äôs what that looked like for me. Note how I used a dummy input of `\"Hello, how can I help you?\"`.\n",
    "\n",
    "```python\n",
    "self.model = torch.compile(self.model)\n",
    "for i in range(5):\n",
    "    starting_time = time.time()\n",
    "    _ = _generate_response(\n",
    "        self.model, self.tokenizer, \"Hello, how can I help you?\", self.generation_kwargs\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Model response in {time.time() - starting_time:.2f} seconds for iteration {i}\"\n",
    "    )\n",
    "```\n",
    "\n",
    "And don‚Äôt forget to use `torch.inference_mode()`! If you skip it, Torch will try to compute gradients, which adds unnecessary overhead. I personally prefer slapping it on as a decorator rather than using `with torch.inference_mode():`. Saves a bit of boilerplate and looks cleaner, too! üßπ\n",
    "\n",
    "## Loading the Model ‚ö°Ô∏èü§ñ\n",
    "\n",
    "When loading your model, two big things to remember: **use Flash Attention** and **set `torch_dtype` to bfloat16**. This combo works wonders for performance. However, I‚Äôm still on the fence about quantization. On an L4 GPU (similar to AWS‚Äôs A10), loading in 8-bit mode actually slowed things down for me. Turns out, dequantizing weights during inference eats up compute time. Ditching 8-bit mode cut my inference time in half. üïí‚úÇÔ∏è\n",
    "\n",
    "Here‚Äôs my typical model-loading setup:\n",
    "\n",
    "```python\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "Also, size matters! Smaller models generally run faster, but it‚Äôs not always a sure thing. For example, the `ibm-granite-1b-instruct` model was slower than `qwen2.5-1.5b-instruct`, even though the former is smaller. Model architecture plays a role here, so don‚Äôt assume smaller is always quicker ü§î. It is worth noting that `ibm-granite` scored higher in the [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?params=-1%2C4&official=true).\n",
    "\n",
    "Oh, and if you‚Äôre serving models, go for `*-instruct` versions over `*-chat`. While chat models shine for chatbot applications, most use cases benefit more from instruction-tuned models.\n",
    "\n",
    "## Future Directions üîÆüí°\n",
    "\n",
    "Here‚Äôs a quick list of things I didn‚Äôt fully explore (or barely scratched the surface of) but think are worth diving into:\n",
    "\n",
    "1. **Use Unsloth**: I‚Äôve heard great things about this library‚Äîit‚Äôs supposed to save loads of time on research like this. Definitely on my to-try list! ü¶•‚ùå\n",
    "2. **Try Sampling for Inference**: In my limited testing, sampling gave me some‚Ä¶let‚Äôs just say *interesting* outputs (a.k.a. gibberish üòÇ). I suspect there‚Äôs a better set of hyperparameters that could give more cohesive results. Here‚Äôs what I tried:\n",
    "    \n",
    "    ```python\n",
    "    python\n",
    "    CopyEdit\n",
    "    self.generation_kwargs = {\n",
    "        \"do_sample\": True,  # Enables sampling instead of beam search\n",
    "        \"top_k\": 50,  # Limits sampling to the top 50 tokens (controls diversity)\n",
    "        \"top_p\": 0.95,  # Uses nucleus sampling (cumulative probability threshold)\n",
    "        \"temperature\": 0.3,  # Scales logits before sampling (higher = more randomness)\n",
    "        \"no_repeat_ngram_size\": 2,  # Avoids repetitive sequences\n",
    "        \"early_stopping\": True,  # Stops generation once max_length is reached\n",
    "        \"use_cache\": True,\n",
    "        \"max_new_tokens\": 192,  # Maximum number of tokens to generate\n",
    "    }\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "That‚Äôs it for now! I‚Äôll keep experimenting, but these tweaks already gave me some solid improvements. If you‚Äôve got any tips or questions, drop me a line! ‚úåÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
