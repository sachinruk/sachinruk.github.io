[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Creating a Caption Model from Scratch\n\n\n\n\n\n\n\nLLM\n\n\nmultimodal-models\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAnnotated DDPM\n\n\n\n\n\n\n\ndeep-learning\n\n\n\n\nTraining MNIST via DDPM\n\n\n\n\n\n\nApr 18, 2023\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nFine Tuning T5 for Grammar Correction\n\n\n\n\n\n\n\ndeep-learning\n\n\nLLM\n\n\n\n\nFine-tuning T5 for Sequence to Sequence tasks\n\n\n\n\n\n\nNov 7, 2022\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nFine Tuning GPT2 for Grammar Correction\n\n\n\n\n\n\n\ndeep-learning\n\n\nLLM\n\n\n\n\nFine-tuning GPT2 for Sequence to Sequence tasks\n\n\n\n\n\n\nSep 25, 2022\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nTransformer Model Compression (Attempt)\n\n\n\n\n\n\n\ndeep-learning\n\n\n\n\nA failed attempt at model compression using student teacher learning\n\n\n\n\n\n\nAug 7, 2022\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nUnit Testing for Data Science\n\n\n\n\n\n\n\npython\n\n\nsoftware engineering\n\n\n\n\nPython testing for Machine Learning\n\n\n\n\n\n\nJul 3, 2022\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nVector Database from Scratch\n\n\n\n\n\n\n\npytorch\n\n\n\n\nImplementing Approximate Nearest Neighbours Oh Yeah (ANNOY)\n\n\n\n\n\n\nMar 22, 2022\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nKMeans in PyTorch with Cosine Distanceü•ßüî¶\n\n\n\n\n\n\n\npytorch\n\n\n\n\nImplementing kmeans with cosine distance\n\n\n\n\n\n\nMar 20, 2022\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch prefetch or rather the lack of it\n\n\n\n\n\n\n\npytorch\n\n\n\n\nHow prefetch_factor did not help in streaming data\n\n\n\n\n\n\nFeb 13, 2022\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nGenerating captions with ViT and GPT2 using ü§ó Transformers - Part 2\n\n\n\n\n\n\n\npytorch\n\n\nhuggingface\n\n\n\n\nUsing Encoder Decoder models in HF to combine vision and text\n\n\n\n\n\n\nJan 26, 2022\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nGenerating captions with ViT and GPT2 using ü§ó Transformers\n\n\n\n\n\n\n\npytorch\n\n\nhuggingface\n\n\n\n\nUsing Encoder Decoder models in HF to combine vision and text\n\n\n\n\n\n\nDec 28, 2021\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nHuggingFace Tokenizers as Collate Functions Timing ü§ó ü§ñ\n\n\n\n\n\n\n\npytorch\n\n\nhuggingface\n\n\n\n\nTiming comparison of tokenizer as collate function and after batching\n\n\n\n\n\n\nNov 17, 2021\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nZero Shot Classification with Huggingface + Sentence Transformers ü§ó ü§ñ\n\n\n\n\n\n\n\npytorch\n\n\nhuggingface\n\n\n\n\nFast Zero Shot classification of text\n\n\n\n\n\n\nOct 10, 2021\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nAn Intuitive Loss for Imbalanced Classification\n\n\n\n\n\nAn alternative approach to Focal Loss\n\n\n\n\n\n\nAug 28, 2021\n\n\n\n\n\n\n  \n\n\n\n\nCoco Semantic Segmentation in PyTorch - Data Prep\n\n\n\n\n\n\n\npytorch\n\n\ndata\n\n\n\n\nHow to prepare and transform image data for segmentation\n\n\n\n\n\n\nAug 21, 2021\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nDINO Self Supervised Vision Transformers\n\n\n\n\n\n\n\npytorch\n\n\npytorch lightning\n\n\nloss function\n\n\n\n\nGetting image embeddings with no negative samples\n\n\n\n\n\n\nAug 1, 2021\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch Image Patches\n\n\n\n\n\nGetting image patches for Visual Transformer\n\n\n\n\n\n\nJul 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\nCollate function tutorial\n\n\n\n\n\nPyTorch Collate function tutorial\n\n\n\n\n\n\nJun 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\nThe Annotated TabNet\n\n\n\n\n\n\n\nTensorflow\n\n\n\n\nCreating TabNet from Scratch in Tensorflow 2.0\n\n\n\n\n\n\nApr 5, 2021\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nMultilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°\n\n\n\n\n\n\n\npytorch\n\n\npytorch lightning\n\n\nloss function\n\n\nGPU\n\n\n\n\nTraining OpenAI‚Äôs CLIP on google colab\n\n\n\n\n\n\nMar 7, 2021\n\n\nSachin Abeywardana\n\n\n\n\n\n\n  \n\n\n\n\nTensorflow Learning Rate Finder\n\n\n\n\n\nUsing Callbacks to get Optimal Learning Rate\n\n\n\n\n\n\nFeb 15, 2021\n\n\n\n\n\n\n  \n\n\n\n\nFocal Loss for Multi-class Classification\n\n\n\n\n\nExtending normal Focal Loss\n\n\n\n\n\n\nNov 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\nDocker for Data Science\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2017\n\n\n\n\n\n\n  \n\n\n\n\nDeepSchool.io\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJul 4, 2017\n\n\n\n\n\n\n  \n\n\n\n\nKeras LSTMs\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2016\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning Quantile Regression - Keras\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2016\n\n\n\n\n\n\n  \n\n\n\n\nXgBoost - Machine Learning made EASY!\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2016\n\n\n\n\n\n\n  \n\n\n\n\nReversible jump MCMC\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2015\n\n\n\n\n\n\n  \n\n\n\n\nChinese Restuarant Process\n\n\n\n\n\n\n\nstatistics, bayesian\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2015\n\n\n\n\n\n\n  \n\n\n\n\nvon Mises-Fisher Distribution\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2015\n\n\n\n\n\n\n  \n\n\n\n\nSample Variance\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2015\n\n\n\n\n\n\n  \n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2015\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming_tips.html",
    "href": "programming_tips.html",
    "title": "Programming Tips",
    "section": "",
    "text": "Thoughts on CosineEmbeddingLoss\n\n\n\n\n\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPydantic Post Init\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTimm choosing a model\n\n\n\n\n\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPytorch Lightning Learning rate finder\n\n\n\n\n\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming_tips/cosine-embedding-loss.html",
    "href": "programming_tips/cosine-embedding-loss.html",
    "title": "Thoughts on CosineEmbeddingLoss",
    "section": "",
    "text": "\\[\n\\begin{align}\n\\text{loss}(x, y) =\n\\begin{cases}\n1 - \\cos(x_1, x_2) & \\text{if } y = 1 \\\\\n\\max(0, \\cos(x_1, x_2) - \\text{margin}) & \\text{if } y = -1\n\\end{cases}\n\\end{align}\n\\]\nnn.CosineEmbeddingLoss needs 3 arguments, the predicted embedding (\\(x_1\\)), label embedding (\\(x_2\\)) as well as a label (\\(y\\)) indicating that we need to move these embeddings closer (\\(y = 1\\)) and further if not (\\(y = 0\\)).\nThe distance between two points (\\(1 - \\cos(x_1, x_2)\\)) is minimised, while in the \\(y = 0\\) case, the similarity \\(\\cos(x_1, x_2)\\) is minimised. However, if the similarity is lower than a certain margin (defaulting to 0.5) nothing happens. This is due to the clipping that happens with the max function. This causes these examples to have zero gradient."
  },
  {
    "objectID": "programming_tips/pydantic-post-init.html",
    "href": "programming_tips/pydantic-post-init.html",
    "title": "Pydantic Post Init",
    "section": "",
    "text": "import pydantic\n\nclass HyperParameters(pydantic.BaseModel):\n    batch_size: int = 32\n\n    @property\n    def upload_batch_counts(self):\n        return 1_000_000 // self.batch_size\n\n    class Config: # will not allow undefined fields\n        extra = pydantic.Extra.forbid\nIf you want to update an existing property, you can do this:\nclass User(BaseModel):\n    name: str\n    age: int\n\n    def __init__(self, **data):\n        if data.get(\"name\") == \"Charlie\":\n            data[\"age\"] = data.get(\"age\", 0) + 30  # increment age by 30 if name is \"Charlie\"\n        super().__init__(**data)"
  },
  {
    "objectID": "blog/2023-08-30-caption-models.html#introduction",
    "href": "blog/2023-08-30-caption-models.html#introduction",
    "title": "Creating a Caption Model from Scratch",
    "section": "Introduction",
    "text": "Introduction\nBefore we dive into the details, I want to give you a heads up that this was just an experiment. The results leave a lot to be desired, and you can see for yourself by checking out the results section. However, if you stick around, I‚Äôll share some interesting techniques that I learned along the way.\nYou can find the code for this blog post in this kaggle kernel."
  },
  {
    "objectID": "blog/2023-08-30-caption-models.html#pretrained-models",
    "href": "blog/2023-08-30-caption-models.html#pretrained-models",
    "title": "Creating a Caption Model from Scratch",
    "section": "Pretrained Models",
    "text": "Pretrained Models\nTo create our captioning model, I started with the hypothesis that we could take a powerful image model and plug it into a pretrained LLM. That‚Äôs exactly what I did, but with a twist: I purposely chose a non-transformer image model. This meant that we couldn‚Äôt use HF transformer‚Äôs EncoderDecoderModels.\nFor the LLM, we used the decoder of Flan-T5. We chose this model because we didn‚Äôt need to introduce cross attention layers. Cross-Attention (CA) layers are how we can connect two transformer models. In a nutshell, if the decoder is of shape batch_size x decoder_seq_len x dim and the encoder is of shape batch_size x encoder_seq_len x dim, the CA layer creates a batch_size x encoder_seq_len x decoder_seq_len attention matrix. The attention matrix projects the encoder sequence into the decoder space, allowing ‚Äúinstructions‚Äù from the image model to mix in with the LLM decoder to create a caption.\nYou might be wondering how we can get a CNN architecture to mimic a transformer output. To do this, we need to do some dimension transposing gymnastics. However, we use einops to make things easier. Here‚Äôs an example of how we use einops to rearrange the image features:\n    image_features = self.image_model.forward_features(images)\n    image_features = einops.rearrange(image_features, \"bs num_features w h -&gt; bs (w h) num_features\")\n    encoder_outputs = self.projector(image_features)\nforward_features is used by all timm models to get the layer before the (1000 class) classification layer. In the example above, this leads to a batch_size x num_features x width x height shape. num_features in this case is analogous to the dimensionality in a transformer model, while width and height can be thought of as the tokens. We combine the width and height tokens and move the num_features dimensions to the end. Finally, there‚Äôs no guarantee that this dimension size is the same as that of the LLM, so we have a few linear layers to project it to the correct size.\nIn order to train Flan-T5, we generally need to provide four things: input_ids and attention_mask on the encoder side, and decoder_input_ids and decoder_attention_mask on the decoder side. However, we can bypass the encoder inputs by providing encoder_outputs. In our case, this is the output from the image model.\nThe final model can be seen below.\n\nclass Model(nn.Module):\n    def __init__(self, image_model, language_model, num_projections: int):\n        super().__init__()\n        language_model.encoder.block = None\n        self.image_model = image_model\n        self.language_model = language_model\n        self.projector = nn.Sequential(\n            *projection_layers(image_model.num_features, language_model.model_dim, num_projections)\n        )\n        self.start_token_id = language_model.config.decoder_start_token_id\n        \n    def project_image_features(self, images: torch.Tensor):\n        image_features = self.image_model.forward_features(images)\n        image_features = einops.rearrange(image_features, \"bs num_features w h -&gt; bs (w h) num_features\")\n        encoder_outputs = self.projector(image_features)\n        return transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=encoder_outputs,\n        )\n        \n    def forward(self, images: torch.Tensor, tokenized_text: dict[str, torch.Tensor]):\n        encoder_outputs = self.project_image_features(images)\n        return self.language_model(\n            encoder_outputs=encoder_outputs,\n            decoder_input_ids=tokenized_text[\"input_ids\"],\n            decoder_attention_mask=tokenized_text[\"attention_mask\"],\n        )\n    \n    def generate(self, images: torch.Tensor, generator_kwargs: dict[str, Union[int, float]]):\n        encoder_outputs = self.project_image_features(images)\n        return self.language_model.generate(\n            encoder_outputs=encoder_outputs,\n            **generator_kwargs\n        )"
  },
  {
    "objectID": "blog/2023-08-30-caption-models.html#training",
    "href": "blog/2023-08-30-caption-models.html#training",
    "title": "Creating a Caption Model from Scratch",
    "section": "Training",
    "text": "Training\nThere are a few compulsory things to do when training, but also a few tricks that I used, but I am not sure if they helped. I will go through them all. - We need to prepend a starting token to the decoder input ids. This is because we want to predict the next token and we need a starting token to do so. This also means prepending a 1 to the decoder attention mask. - The loss function is there to predict the next token. This means if we have the caption ‚ÄúA dog is running‚Äù we want to predict ‚Äúdog‚Äù when we see ‚ÄúA‚Äù. Therefore, we need to shift the decoder input ids by one. This means that the input token ids are the the same as the output token ids, but shifted by one. loss_fn is simply nn.CrossEntropyLoss()\ndef calculate_loss_fn(loss_fn, logits: torch.Tensor, labels: torch.Tensor) -&gt; torch.Tensor:\n    shift_logits = logits[:, :-1, :].contiguous() # (batch_size, seq_len - 1, dim)\n    shift_labels = labels[:, 1:].contiguous() # (batch_size, seq_len - 1)\n    return loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\nKeep in mind that the projector in the above model is the only part that is untrained. Usually, we would freeze the pretrained models, however, in my experiments I found that it was better to train the entire model. I do not however train any of the token embeddings. This is due to the fact that in the COCO dataset I only see a limited number of tokens. Therefore, I do not want to train the embeddings to overfit to the COCO dataset. If you look at the LightningModule you can see (in __init__) how I leave it as an option to freeze the image encoder and the LLM.\nFinally, in configure_optimizers I set the learning rate of the LLM to be a quarter of the projection layer, and the image model to be a half of the projection layer. This is because of my assumption that the LLM has seen far more training data than an image model, and I do not want to lose that information. Oh, and I have made it a habit to use OneCycleLR scheduler. I find it more effective than a using an optimizer on its own.\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        params = [\n            {\"params\": self.model.language_model.decoder.block.parameters(), \"lr\": self.lr / 4},\n            {\"params\": self.model.language_model.decoder.final_layer_norm.parameters(), \"lr\": self.lr / 4},\n            {\"params\": self.model.image_model.parameters(), \"lr\": self.lr / 2},\n            {\"params\": self.model.projector.parameters(), \"lr\": self.lr},\n        ]\n        optimizer = torch.optim.Adam(params)\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=[param_group[\"lr\"] for param_group in optimizer.param_groups],\n            total_steps=self.trainer.estimated_stepping_batches,\n        )\n        return [optimizer], [scheduler]"
  },
  {
    "objectID": "blog/2023-08-30-caption-models.html#results",
    "href": "blog/2023-08-30-caption-models.html#results",
    "title": "Creating a Caption Model from Scratch",
    "section": "Results",
    "text": "Results\nLet‚Äôs take a look at the results of our captioning model. In the first set of results, the second and fifth captions are correct, but the second to last caption is a complete miss. This was when we had only five linear layers in the projection layer. \nWhen we increased the number of linear layers to 12, the validation loss improved (as expected). In the second set of results, the third, fourth, and fifth captions are correct, but the first caption is a complete miss, talking about tennis. This is likely due to the model overfitting, as there are quite a few tennis images in the COCO dataset."
  },
  {
    "objectID": "blog/2023-08-30-caption-models.html#conclusion",
    "href": "blog/2023-08-30-caption-models.html#conclusion",
    "title": "Creating a Caption Model from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nThis model will clearly not be winning any awards, but none the less it was a fun experiment. I hope you learnt something new. If you have any questions or comments please leave them below."
  },
  {
    "objectID": "blog/2021-06-05-pytorch-collatefn.html",
    "href": "blog/2021-06-05-pytorch-collatefn.html",
    "title": "Collate function tutorial",
    "section": "",
    "text": "Suppose we have the following hypothetical dataset.\nclass Dataset:\n    def __init__(self):\n        super().__init__()\n        \n    def __len__(self):\n        return 32\n    \n    def __getitem__(self, idx):\n        return f\"hello {idx}\", random.randint(0, 3)\n    \nrand_ds = Dataset()\nrand_dl = DataLoader(rand_ds, batch_size=4)\nPrinting out the first batch, notice how the first element is just a tuple of strings and the second item has automagically been converted into a tensor.\nnext(iter(rand_dl))\n\n[('hello 0', 'hello 1', 'hello 2', 'hello 3'), tensor([0, 1, 1, 0])]"
  },
  {
    "objectID": "blog/2021-06-05-pytorch-collatefn.html#the-collate-function",
    "href": "blog/2021-06-05-pytorch-collatefn.html#the-collate-function",
    "title": "Collate function tutorial",
    "section": "The Collate Function",
    "text": "The Collate Function\nWith the collate function we can convert these strings to a tensor as well. This leads to cleaner code in that data preprocessing is kept away from model code. In my case it actually led to a slightly faster run time per epoch, but I‚Äôm not entirely sure why.\nThe following code takes in a list of size batch size, where each element is a string and it‚Äôs corresponding label. Then it parses the strings through the tokenizer, which converts into numerical values thanks to the huggingface tokenizer. But more importantly, note how now you have to convert the y to torch.LongTensor, as otherwise it would remain a tuple. This is certainly an extra step that pytorch was internally taking care of for you.\n\nclass CollateFn:\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n    def __call__(\n        self, batch: List[Tuple[str, int]]\n    ) -&gt; Tuple[Dict[str, torch.LongTensor], torch.LongTensor]:\n        x, y = zip(*batch)\n        return self.tokenizer(list(x)), torch.LongTensor(y)\n\nWe can add an instance of the above class to our dataloader, which leads us to the following results:\n\ncollate_fn = CollateFn()\nrand_dl = DataLoader(rand_ds, batch_size=4, collate_fn=collate_fn)\nnext(iter(rand_dl))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n({'input_ids': [[101, 19082, 121, 102], [101, 19082, 122, 102], [101, 19082, 123, 102], [101, 19082, 124, 102]], 'token_type_ids': [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]},\n tensor([2, 1, 3, 1]))"
  },
  {
    "objectID": "blog/2021-06-05-pytorch-collatefn.html#summary",
    "href": "blog/2021-06-05-pytorch-collatefn.html#summary",
    "title": "Collate function tutorial",
    "section": "Summary",
    "text": "Summary\n\nCollate functions are used to transform data.\nYou need to transform all outputs, not just simply the one you possibly want."
  },
  {
    "objectID": "blog/2021-06-05-pytorch-collatefn.html#shameless-self-promotion",
    "href": "blog/2021-06-05-pytorch-collatefn.html#shameless-self-promotion",
    "title": "Collate function tutorial",
    "section": "Shameless self promotion",
    "text": "Shameless self promotion\nIf you enjoyed the tutorial buy me a coffee, or better yet buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2021-08-01-dino-self-supervised-vision-transformers.html",
    "href": "blog/2021-08-01-dino-self-supervised-vision-transformers.html",
    "title": "DINO Self Supervised Vision Transformers",
    "section": "",
    "text": "Photo credit Nicola Stocchi"
  },
  {
    "objectID": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#introduction",
    "href": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#introduction",
    "title": "DINO Self Supervised Vision Transformers",
    "section": "Introduction",
    "text": "Introduction\nThis implementation is based off of this paper by FAIR. The reason I‚Äôm excited about this paper is because 1. I was able to implement this by reading the paper (don‚Äôt underestimate how convoluted this step is) 2. We have a setting where we ignore any kind of labels altogether, i.e.¬†completely self supervised. 3. We don‚Äôt need negative labels.\nIn a nutshell what the paper attempts to do, is to take two different augmentations of the same image, and try and push these embeddings closer together. Most (and possibly all) other such tasks attempt to do a triplet loss where the image is compared to a similar image (positive example), and different image(s) (negative examples). What‚Äôs amazing about this paper is that it ignores negative examples altogether and is still able to get a meaningful embedding space for a given image.\nIf you wish to run this live, please visit this kaggle kernel instead of the colab link above.\n\n\nCode\n# Image parameters\nTRAIN_FILES = \"/kaggle/input/coco-2017-dataset/coco2017/train2017/\"\nIMAGE_SIZE = 256\nPATCH_SIZE = 16\nZERO_PCT = 0.1\nPATCHES_PER_ROW = (IMAGE_SIZE // PATCH_SIZE)\nNUM_PATCHES = PATCHES_PER_ROW ** 2\nRGB_CHANNELS = 3\nNUM_PIXELS = PATCH_SIZE ** 2 * RGB_CHANNELS\nVALID_IMAGES = 5\nTOPK = 5\n\n# Training parameters\nBATCH_SIZE = 16\nEPOCHS = 5\nLR = 1e-4\n\n# Transformer parameters\nN_HEADS = 8\nN_LAYERS = 6\n\n# Update constants\nTEMPERATURE_S = 0.1\nTEMPERATURE_T = 0.05\nCENTER_MOMENTUM = 0.9\nTEACHER_MOMENTUM = 0.995"
  },
  {
    "objectID": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#data",
    "href": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#data",
    "title": "DINO Self Supervised Vision Transformers",
    "section": "Data",
    "text": "Data\nThe following shows how a given image is passed through two transforms (RandomResizedCrop) with one guaranteed to be atleast half as large as the original image. A collate function is then used to break it up into 16x16 patches, and stack those patches into a sequence, so that we can fit it into a transformer. The collate function is a minor detail in the overall picture, but if you wish to read about it you can do so here.\n\nclass ImageData(Dataset):\n    def __init__(self, files: List[str]):\n        self.files = files\n        self.randcrop_big = transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(0.5, 1.0))\n        self.randcrop_small = transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE))\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, i):\n        img = io.read_image(self.files[i])\n        img1 = self.randcrop_big(img)\n        img2 = self.randcrop_small(img)\n        if img.shape[0] == 1:\n            img1 = torch.cat([img1]*3)\n            img2 = torch.cat([img2]*3)\n\n        return img1, img2\n\n\n\nCode\nclass CollateFn:\n    def reshape(self, batch):\n        patches = torch.stack(batch)\\\n                    .unfold(2, PATCH_SIZE, PATCH_SIZE)\\\n                    .unfold(3, PATCH_SIZE, PATCH_SIZE)\n\n        num_images = len(patches)\n        patches = patches.reshape(\n            num_images,\n            RGB_CHANNELS, \n            NUM_PATCHES, \n            PATCH_SIZE, \n            PATCH_SIZE\n        )\n        patches.transpose_(1, 2)\n        \n        return patches.reshape(num_images, NUM_PATCHES, -1) / 255.0 - 0.5\n        \n    def __call__(\n        self, batch: List[Tuple[torch.Tensor, torch.Tensor]]\n    ) -&gt; Tuple[torch.FloatTensor, torch.FloatTensor]:\n        x1, x2 = zip(*batch)\n\n        return self.reshape(x1), self.reshape(x2)"
  },
  {
    "objectID": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#model",
    "href": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#model",
    "title": "DINO Self Supervised Vision Transformers",
    "section": "Model",
    "text": "Model\nThe model is simply the 256 image patches passed through an encoder transformer with a CLS token. However, a few things that I think newcomers to the transformer/ pytorch field ought to know is as follows (also I made these mistakes üòÖ). - Make sure to use self.register_parameter when declaring trainable variables in pytorch. doing self.myvar = nn.Parameter(...) is not enough. - I have used LayerNorm everywhere possible to keep gradient sizes reasonable for a optimizer with constant learning rate.\nThe walkthough of the model operation ignoring normalisations is as follows: 1. Take the flattened 3x16x16 image patches and append them to a positional encoding. 2. Pass this through a linear layer and append the CLS token embedding to this. 3. Pass this through transformer, and take the 0th token since this correspond to the CLS token. 4. Normalize this vector to unit length so that you have a final image embedding.\n\nclass Model(nn.Module):\n    def __init__(self, d_model, n_head, n_layers):\n        super().__init__()\n        # transformer\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n\n        # positional embedding\n        w_pos = torch.randn(NUM_PATCHES, d_model) / d_model ** 0.5\n        cls_token = torch.randn(1, d_model) / d_model ** 0.5\n        self.register_parameter(\"pos_embed\", nn.Parameter(w_pos))\n        self.register_parameter(\"cls_token\", nn.Parameter(cls_token))\n\n        # pixel projection\n        self.linear = nn.Linear(2 * d_model, d_model)\n        self.norm1 = nn.LayerNorm(2 * d_model, elementwise_affine=False)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        batch_size = len(x)\n        position = torch.stack([self.pos_embed] * batch_size)\n        x = torch.cat([x, position], dim=-1)\n        pixel_proj = self.norm2(F.relu(self.linear(self.norm1(x))))\n        batched_cls_token = torch.stack([self.cls_token]*batch_size)\n        cls_x = torch.cat([batched_cls_token, pixel_proj], dim=1)\n        \n        cls_x.transpose_(0, 1)\n        return F.normalize(self.encoder(cls_x)[0, ...], dim=-1)"
  },
  {
    "objectID": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#cross-entropy-loss-sort-of",
    "href": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#cross-entropy-loss-sort-of",
    "title": "DINO Self Supervised Vision Transformers",
    "section": "Cross Entropy Loss, sort of",
    "text": "Cross Entropy Loss, sort of\nWe need a loss function that will push the output vector of the above model towards each other for the two augmented images. The way that this paper does it, is by treating the vector as a (log of) a histogram, and trying to line it up with its augmented version. Why is this interesting? Usually, most authors tend to simply take the dot product and maximise that to indicate that vectors need to line up (be close). This paper achieves the same result but by asking histograms to line up instead.\nThe cross entropy loss takes on the form of \\(-\\sum_{j=1}^J y_j\\log p(x_j)\\). This is minimised when \\(p(x_j)\\) tends towards \\(y_j\\). Similarly if we were to replace \\(y_j\\) with a probability distribution \\(q_j\\in[0, 1]\\text{ s.t. }\\sum_j q_j = 1\\), p_j is minimised when the distributions line up.\nMy personal intuition as to why this might be better might be as follows: (Feel free to correct me if I am wrong). When trying to maximise dot products, we are asking two points on a sphere to move towards each other. Now there is an obvious shortest path distance when you visualise it by drawing a line between the two points. However, going away from the point will also, eventually get you to the same point but by taking a longer time.\nBy using the following equation we need two histograms to line up, which is a simpler derivative, and therefore more likely to get to a local minima faster. \\[ \\frac{1}{N}\\sum_{n=1}^N\\sum_{j=1}^J q_j\\log p_j\\]\nNote that in the implementation below, we have extra two concepts of centering, and using a temperature. I‚Äôm not sure about the centering, but the temperature variable (which is positive but less than one), sharpens the histogram, making the peaks more prominent.\n\nclass HLoss:\n    def __init__(self, temperature_t: float, temperature_s: float):\n        self.temperature_t = temperature_t\n        self.temperature_s = temperature_s\n        \n    def __call__(\n        self, \n        t: torch.FloatTensor, \n        s: torch.FloatTensor, \n        center: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        t = F.softmax((t.detach() - center) / self.temperature_t, dim=1)\n        log_s = F.log_softmax(s / self.temperature_s, dim=1)\n\n        return -(t * log_s).sum(dim=1).mean()"
  },
  {
    "objectID": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#training",
    "href": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#training",
    "title": "DINO Self Supervised Vision Transformers",
    "section": "Training",
    "text": "Training\nThe LightningModule below goes through the training step. The main steps are: 1. Create two copies of the model with the exact same parameters. One would be considered teacher (with the gradients not being calculated at backprop) and the student. 2. Pass both augmentations through both student and teacher. 3. Calculate the loss function shown above for the two augmentations, but with one embedding from teacher and the other from the student. 4. Calculate the new exponentially weighted teacher parameters with the corresponding student parameters. 5. Calculate a new (exponentially weighted) center parameter from the embeddings passed through the teacher only. Steps 1-4 being the most important aspects IMO. 5th step feels more like a trick needed to stabalize optimization.\nThe following gif from the official repo is an overview of the algorithm: \nThe non-standard (and important to note) things I‚Äôve done in the LightningModule are as follows: - Set all parameters in teacher model to non-trainable. - Register a buffer (not parameter) center to track the output of the teacher. - At each validation_epoch_end randomly pick an image from validation set and find 5 closest images. Use these results and push it to weights and biases as a table of images. - Every 50th batch save a histogram of gradients to weights and biases. This ensures that there is no gradient explosion or degredation as training evolves.\n\n\nCode\nclass LightningModel(pl.LightningModule):\n    def __init__(\n        self,\n        teacher: nn.Module,\n        lr: float,\n        loss_fn: Callable,\n        valid_files: List[str],\n        dim: int,\n        center_momentum: float,\n        param_momentum: float,\n    ):\n        super().__init__()\n        self.teacher = teacher\n        self.student = copy.deepcopy(teacher)\n        self.lr = lr\n        self.loss_fn = loss_fn\n        self.c_mom = center_momentum\n        self.p_mom = param_momentum\n        self.register_buffer(\"center\", torch.zeros((1, dim)).float())\n        self.valid_files = valid_files\n        \n        for p in self.teacher.parameters():\n            p.requires_grad = False\n\n    def loss_calculation(\n        self,\n        batch: Tuple[torch.FloatTensor, torch.FloatTensor],\n    ) -&gt; torch.FloatTensor:\n        x1, x2 = batch\n        \n        s1, s2 = self.student(x1), self.student(x2)\n        t1, t2 = self.teacher(x1), self.teacher(x2)\n        \n        loss = self.loss_fn(t1, s2, self.center) + self.loss_fn(t2, s1, self.center)\n        \n        emperical_center = F.normalize(\n            torch.cat([t1, t2]).mean(dim=0, keepdims=True),\n            dim=-1,\n        )\n        return loss, emperical_center\n\n    def training_step(\n        self, batch: Tuple[torch.FloatTensor, torch.FloatTensor], *args: List[Any]\n    ) -&gt; torch.Tensor:\n        loss, emperical_center = self.loss_calculation(batch)\n        self.log(name=\"Training loss\", value=loss, on_step=True, on_epoch=True)\n        \n        self.center = F.normalize(\n            self.c_mom * self.center + (1 - self.c_mom) * emperical_center,\n            dim=-1,\n        )\n        for s_p, t_p in zip(self.student.parameters(), self.teacher.parameters()):\n            t_p.data = self.p_mom * t_p.data + (1 - self.p_mom) * s_p.data\n            \n        return loss\n    \n    def validation_step(self, images: torch.FloatTensor, *args: List[Any]) -&gt; None:\n        return self.teacher(images)\n        \n    def validation_epoch_end(self, validation_step_outputs):\n        valid_embeds = torch.cat([pred for pred in validation_step_outputs])\n        columns = [\"image\"] + [f\"closest_{i+1}\" for i in range(TOPK)]\n        indices = np.random.choice(len(self.valid_files), VALID_IMAGES, replace=False)\n        rows = [get_closest_wandb_images(valid_embeds, i, self.valid_files) for i in indices]\n        table = wandb.Table(data=rows, columns=columns)\n        self.logger.experiment.log({f\"epoch {self.current_epoch} results\": table})\n        \n    def on_after_backward(self):\n        if self.trainer.global_step % 50 == 0:  # don't make the tf file huge\n            global_step = self.trainer.global_step\n            for name, param in self.student.named_parameters():\n                if \"weight\" in name and not \"norm\" in name and param.requires_grad:\n                    self.logger.experiment.log(\n                        {f\"{name}_grad\": wandb.Histogram(param.grad.cpu())}\n                    )\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        return FusedAdam(self.student.parameters(), lr=self.lr)\n\n\nThanks to PyTorch Lightning and WandB I can easily do half precision training and log the results to a beautiful dashboard, with results in the link below.\n\n!mkdir /kaggle/working/logs\nteacher = Model(NUM_PIXELS, N_HEADS, N_LAYERS)\nh_loss = HLoss(TEMPERATURE_T, TEMPERATURE_S)\nlightning_model = LightningModel(\n    teacher, \n    LR,\n    h_loss,\n    valid_files,\n    NUM_PIXELS,\n    CENTER_MOMENTUM, \n    TEACHER_MOMENTUM,\n)\n\nlogger = WandbLogger(\"DINO\", \"/kaggle/working/logs/\", project=\"DINO\")\ntrainer = pl.Trainer(\n    max_epochs=EPOCHS,\n    gpus=torch.cuda.device_count(),\n    gradient_clip_val=1.0,\n    logger=logger,\n    precision=16,\n    num_sanity_val_steps=0,\n)\ntrainer.fit(lightning_model, train_dl, valid_dl)\n\nUsing /root/.cache/torch_extensions as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/fused_adam...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/fused_adam/build.ninja...\nBuilding extension module fused_adam...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module fused_adam...\nTime to load fused_adam op: 29.516473054885864 seconds\n\n\nwandb: Currently logged in as: sachinruk (use `wandb login --relogin` to force relogin)\n\n\n\n                Tracking run with wandb version 0.11.0\n                Syncing run DINO to Weights & Biases (Documentation).\n                Project page: https://wandb.ai/sachinruk/DiNo\n                Run page: https://wandb.ai/sachinruk/DiNo/runs/m5bwe0bf\n                Run data is saved locally in /kaggle/working/logs/wandb/run-20210726_123052-m5bwe0bf\n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1"
  },
  {
    "objectID": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#evaluate-results",
    "href": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#evaluate-results",
    "title": "DINO Self Supervised Vision Transformers",
    "section": "Evaluate results",
    "text": "Evaluate results\nFinally lets look at some random images with its most closest images according to the model. Note that at this point, we throw away the student and simply take the teacher, even though it is only the student that used gradient information directly. The following image shows the weights and biases table that I created during training using only the validation dataset. The results that follow use the entire set of images and it‚Äôs corresponding closest images. Considering that this is a self supervised task, this is not ‚Äúcheating‚Äù. \n\nteacher = teacher.eval().to(device)\nembedding = []\nwith torch.no_grad():\n    for x in tqdm(image_orig_dl):\n        out = teacher(x.to(device))\n        embedding.append(out.cpu())\n        \n    embedding = torch.cat(embedding, dim=0)\n\n\n\n\n\ni = 64\nplot_closest_pairs(embedding, i, files)\n\n\n\n\n\n\n\n\ni = 42\nplot_closest_pairs(embedding, i, files)\n\n\n\n\n\n\n\n\ni = 21\nplot_closest_pairs(embedding, i, files)"
  },
  {
    "objectID": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#shameless-self-promotion",
    "href": "blog/2021-08-01-dino-self-supervised-vision-transformers.html#shameless-self-promotion",
    "title": "DINO Self Supervised Vision Transformers",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html",
    "href": "blog/2022-07-03-ds-tests.html",
    "title": "Unit Testing for Data Science",
    "section": "",
    "text": "Writing tests has always been poison to me. And I‚Äôm still not at the point of writing tests for everything, but I have been coming around. Enough to say that I am actually having fun writing them. The way I see it the point of unit tests is to catch bugs, and catch them early.\nFor die hard Jupyter worshippers like myself, the question is, what do you mean catch them early? You just copy and paste your tested code into a .py file and call it a day right? Unfortunately, most of the time the code in a single Jupyter notebook is too messy for a enterprise level monorepo. While projects like nbdev exist, introducing such a library to an existing repo is not trivial. None the less it may even be an organisational requirement to have high code coverage by testing as much as possible.\nThis tutorial is some of the tricks I have picked up along the way, including best practices. These include how to test large Deep Learning models. I do not claim to be a testing guru or anywhere near it.\n\n\n\nbird chomping on biscuit and screaming meme"
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#introduction",
    "href": "blog/2022-07-03-ds-tests.html#introduction",
    "title": "Unit Testing for Data Science",
    "section": "",
    "text": "Writing tests has always been poison to me. And I‚Äôm still not at the point of writing tests for everything, but I have been coming around. Enough to say that I am actually having fun writing them. The way I see it the point of unit tests is to catch bugs, and catch them early.\nFor die hard Jupyter worshippers like myself, the question is, what do you mean catch them early? You just copy and paste your tested code into a .py file and call it a day right? Unfortunately, most of the time the code in a single Jupyter notebook is too messy for a enterprise level monorepo. While projects like nbdev exist, introducing such a library to an existing repo is not trivial. None the less it may even be an organisational requirement to have high code coverage by testing as much as possible.\nThis tutorial is some of the tricks I have picked up along the way, including best practices. These include how to test large Deep Learning models. I do not claim to be a testing guru or anywhere near it.\n\n\n\nbird chomping on biscuit and screaming meme"
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#basic-unit-test-structure-conventions",
    "href": "blog/2022-07-03-ds-tests.html#basic-unit-test-structure-conventions",
    "title": "Unit Testing for Data Science",
    "section": "Basic Unit Test structure (conventions)",
    "text": "Basic Unit Test structure (conventions)\nUsually you would have a tests folder which will contain test files that starts with test_*.py. These file usually correspond 1 to 1 with whatever is in your src directory that you are testing (eg. src/a.py would have a tests/test_a.py). Each function/ class that you are testing would similarly have a def test_*() function. All testable functions must start with test_. And finally, usually you would have an assert statement inside these tests, but testing goes beyond these statements, and are not a necessity.\nIn order to run them you can simply run pytest /path/to/folders/tests/."
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#dependency-injection",
    "href": "blog/2022-07-03-ds-tests.html#dependency-injection",
    "title": "Unit Testing for Data Science",
    "section": "Dependency Injection",
    "text": "Dependency Injection\nSince these are usually run in CICD framework, it is important that these tests are run quickly. Therefore, we should not instantiate large NLP/ CV models inside a test. One way to get around this is to inject the dependency to a function.\nConsider the following two functions:\ndef create_classification_model(num_classes: int) -&gt; nn.Module:\n    model = models.resnet34(pretrained=True)\n    return torch.nn.Sequential(\n        *(\n            list(model.children())[:-1] + [nn.Linear(512, num_classes)]\n        )\n    )\n\n# don't name it with_injection, this is just for illustration\ndef create_classification_model_with_injection(base_model: nn.Module, num_classes: int) -&gt; nn.Module:\n    return torch.nn.Sequential(\n        *(\n            list(base_model.children())[:-1] + [nn.Linear(512, num_classes)]\n        )\n    )\nOut of the two, the second is more testable as we do not 1. need to instatiate a large model, 2. Dowload anything from the internet. When testing we could pass in something as simple as test_base_model = nn.Conv2D(3, 512). While it‚Äôs true we are not testing out a full resnet model, we are still able to check for bugs that may be caused by running above."
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#pytest-fixtures-and-conftest.py",
    "href": "blog/2022-07-03-ds-tests.html#pytest-fixtures-and-conftest.py",
    "title": "Unit Testing for Data Science",
    "section": "Pytest Fixtures and conftest.py",
    "text": "Pytest Fixtures and conftest.py\nSuppose that you needed a model definition for multiple test functions. While we can instantiate a dummy model inside a test_* function, one way to write this instantion once, is to write a function called def dummy_model() -&gt; nn.Module and decorate it with @pytest.fixture. Once this is done, we can pass it into the test functions as an argument, and pytest will take care of passing in a instantiated version. If this model definition is required in other files for testing, we can move it into a conftest.py which will make it accessible for all files in that tests directory. Here is an example of a dummy transformer model and tokenizer in a conftest.py file.\n@pytest.fixture\ndef model() -&gt; transformers.PreTrainedModel:\n    config = transformers.DistilBertConfig(\n        vocab_size=4,  # must be the same as the vocab size in the tokenizer\n        n_layers=1,\n        n_heads=1,\n        dim=4,\n        hidden_dim=4,\n    )\n    model = transformers.DistilBertModel(config)\n    return model\n\n\n@pytest.fixture\ndef tokenizer(tmp_path: pathlib.Path) -&gt; transformers.PreTrainedTokenizer:\n    with open(tmp_path / \"vocab.txt\", \"w\") as f:\n        f.write(\"[CLS]\\n[SEP]\\n[MASK]\\n[UNK]\\n\")\n\n    tokenizer = transformers.DistilBertTokenizer(tmp_path / \"vocab.txt\")\n    return tokenizer\n\n@pytest.fixture\ndef test_sentences() -&gt; list[str]:\n    return [\n        \"Never gonna give you up\",\n        \"Never gonna let you down\",\n        \"Never gonna run around and desert you\",\n    ]\nAnd the usage in a test file (not conftest) is shown below:\ndef test_model_output(model, tokenizer, test_sentences):\n    values = model(**tokenizer(test_sentences))\n    assert len(values) == len(test_sentences)"
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#mocking",
    "href": "blog/2022-07-03-ds-tests.html#mocking",
    "title": "Unit Testing for Data Science",
    "section": "Mocking",
    "text": "Mocking\nDepending on complexity, and use case you may not want to construct a dummy object. Instead, we may create unittest.mock.Mock objects. The magic about these objects are that 1. You can call them with infinitely many methods (apart from some assert_* methods), meaning you do not need to implement methods associated with those instances.\nLet‚Äôs consider the function create_classification_model_with_injection. In this case, instead of creating a fake test model, let‚Äôs do the following:\ndef test_create_classification_model_with_injection():\n    mock_model = mock.Mock()\n    create_classification_model_with_injection(mock_model, 10)\n\n    mock_model.children.assert_called_once()\nIn the above what we are testing is that children attribute of the model was called. This means that any future implementation would require children to be called in its implementation, unless the tests are changed. I will refer you to this excellent blog for further magic you can do with mock classes.\nBefore moving on, I want to stress the point that unit testing does not need to be about matching inputs to expected outputs."
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#patching",
    "href": "blog/2022-07-03-ds-tests.html#patching",
    "title": "Unit Testing for Data Science",
    "section": "Patching",
    "text": "Patching\nSome functions require you to perform actions that you cannot test. Downloading is one such example. Suppose I have this function:\n# in models.py\ndef get_model_and tokenizer(model_name: str):\n    model = AutoModel.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    return model, tokenizer\nOne way to test this is to ‚Äúpatch‚Äù the AutoModel.from_pretrained and AutoTokenizer.from_pretrained functions.\ndef test_get_model(model, tokenizer):\n    with mock.patch.object(\n        models.AutoModel, \"from_pretrained\", return_value=model\n    ) as mock_model, mock.patch.object(\n        models.AutoTokenizer, \"from_pretrained\", return_value=tokenizer\n    ) as mock_tokenizer:\n        model_returned, tokenizer_returned = models.get_model_and_tokenizer(\"bert\")\n\n    assert model == model_returned\n    assert tokenizer == tokenizer_returned\nIn the above we case we are effectively testing that from_pretrained gets called during the function.\nIn order to use mock.patch.object the first argument goes models.AutoModel, despite the fact that AutoModel comes from the transformers library. This is because the ‚Äúinstance‚Äù that we are patching is in the models.py file. The second argument is a string of the function that we are calling, and finally the the return_value argument forces that function to return this despite whatever argument."
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#parametrizing",
    "href": "blog/2022-07-03-ds-tests.html#parametrizing",
    "title": "Unit Testing for Data Science",
    "section": "Parametrizing",
    "text": "Parametrizing\nYou may want to test for varying values of a certain input. While it is possible to do so using a for loop, pytest offers the pytest.mark.parametrize decorator. Suppose we have a fake base model for the image classification model we defined above. In the following example we can test multiple num_classes without resorting to an ugly for loop.\n@pytest.mark.parametrize(\"num_classes\", [10, 15])\ndef test_create_classification_model(\n    base_model: nn.Module, # this comes from a fixture\n    num_classes: int,\n):\n    model = create_classification_model_with_injection(base_model, num_classes)\n    fake_input = torch.randn(16, 3, 28, 28) \n    assert model(fake_input).shape[-1] == num_classes"
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#conclusion",
    "href": "blog/2022-07-03-ds-tests.html#conclusion",
    "title": "Unit Testing for Data Science",
    "section": "Conclusion",
    "text": "Conclusion\nIn my concluding remarks, I would like to stress that some tests are better than none. I personally don‚Äôt believe that tests have to be exhaustive, but I can understand if this is a point of contention.\nAlso occasionally there are tests which do not include any assert statements. It simply checks if a group of functions simply run end to end.\nBest of luck with your testing journey!"
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#kudos",
    "href": "blog/2022-07-03-ds-tests.html#kudos",
    "title": "Unit Testing for Data Science",
    "section": "Kudos",
    "text": "Kudos\nKudos to Ryan Lin for all the help with writing tests."
  },
  {
    "objectID": "blog/2022-07-03-ds-tests.html#shameless-self-promotion",
    "href": "blog/2022-07-03-ds-tests.html#shameless-self-promotion",
    "title": "Unit Testing for Data Science",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2023-04-18-ddpm.html",
    "href": "blog/2023-04-18-ddpm.html",
    "title": "Annotated DDPM",
    "section": "",
    "text": "Image saying ‚ÄòAnnotated DDPM‚Äô on black background"
  },
  {
    "objectID": "blog/2023-04-18-ddpm.html#introduction",
    "href": "blog/2023-04-18-ddpm.html#introduction",
    "title": "Annotated DDPM",
    "section": "Introduction",
    "text": "Introduction\nIt took me many hours and weeks to understand DDPM. There were many intracies to understand from the maths to the code. This blog post is meant to cover both the maths side as well as coding. Hopefully, you will not need to venture outside this blog post. I will however assume familiarity with pytorch and some high level understanding of a UNet.\nBefore we get going Kudos to the fast.ai explanation of DDPM.\nI will also ask you to throw away presumptions about stable diffusion. DDPM while being one of the first papers that kicked off this area of Deep Learning does not take in a text input. However, hopefully you might see how to add such conditional information as we walk through this."
  },
  {
    "objectID": "blog/2023-04-18-ddpm.html#diffusion-models",
    "href": "blog/2023-04-18-ddpm.html#diffusion-models",
    "title": "Annotated DDPM",
    "section": "Diffusion Models",
    "text": "Diffusion Models\nThe whole point of diffusion models is to model the data distribution \\(p(x)\\). This is done by transforming a Gaussian distribution iteratively through a neural network. This is different to GANs in that this transformation happens only once in GANs. Despite the multiple steps, the performance of diffusion models are significantly higher.\nIn this section we will step through the maths behind diffusion models. If this does not interest you, feel free to jump to the next section.\nThe rough idea behind diffusion models is the following integral: \\[\np(x) = \\int p_\\theta(x|x_1)p_\\theta(x_1|x_2)...p_\\theta(x_{T-1}|x_T)p(x_T)dx_1...dx_T\n\\] The variables \\(x_1,...,x_T\\) are latent (hidden) variables. In the final inference we throw away these variables.\nIn order to make this problem tractable we noise our input via a Gaussian distribution \\(q(x_t|x_{t-1})=\\mathcal{N}(\\sqrt{\\alpha_t}x_{t-1}, (1-\\alpha_t)\\mathbf{I})\\). These alpha values are varied between 0 and 1. As alpha is close to zero \\(q\\) is close to the standard normal while, at 1 it is close to being deterministically equal to the previous x timestep. The following diagram below shows how adding Gaussian noise moves you closer to a standard normal distribution on the right. \nSource: Nvidia tutorial.\nSo how do these q values come into play? Lucky for us, we can manipulate the above equation as following: \\[\n\\begin{align}\n\\log p(x) &=  \\log \\int p_\\theta(x|x_1)p_\\theta(x_1|x_2)...p_\\theta(x_{T-1}|x_T)dx_1...dx_T \\\\\n\\log p(x) &= \\log \\int p_\\theta(x|x_1)p_\\theta(x_1|x_2)...p_\\theta(x_{T-1}|x_T)\\frac{q(x_1|x)q(x_2|x_1)...q(x_T|x_{T-1})}{q(x_1|x)q(x_2|x_1)...q(x_T|x_{T-1})}dx_1...dx_T \\\\\n\\log p(x) &\\ge E_{q(x_{1:T}|x_0)}\\left[\\log \\frac{p_\\theta(x_{0:T})}{q(x_{1:T}|x_0)}\\right]\n\\end{align}\n\\] Where the last inequality came into play due to Jensen‚Äôs inequality. Note in the second equation that \\(p_\\theta(x_1|x_2)\\) is in the reverse direction while \\(q(x_2|x_1)\\) is in the forward direction. We solve the reverse process by maximising the lower bound with respect to \\(\\theta\\). This lower bound is commonly known as the Evidence Lower BOund (ELBO).\nIn order for us to make the lower bound tractable we need a few more identities. \\[\nq(x_t|x_{t-1}, x_0) = \\frac{q(x_{t-1}|x_t, x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}\n\\] In order to get \\(q(x_t|x_0)\\) given the equation \\(q(x_t|x_{t-1})=\\mathcal{N}(\\sqrt{\\alpha_t}x_{t-1}, (1-\\alpha_t)\\mathbf{I})\\), we could iteratively integrate out \\(x_{t-1}...x_0\\) which leads us to the following identity. \\[\nq(x_t|x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha_t})\\mathbf{I})\n\\] where \\(\\bar{\\alpha_t}\\equiv \\prod_{i=1}^t \\alpha_i\\). These values can be precomputed. Finally, we get this identity. \\[\n\\begin{align}\nq(x_{t-1}|x_t, x_0) &= \\mathcal{N}(\\mu_q(x_t, x_0), \\sigma_q(t) \\mathbf{I})\\\\\n\\mu_q(x_t, x_0) &= \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})x_t + \\sqrt{\\bar{\\alpha}_{t-1}}(1-\\alpha_t)x_0}{(1-\\bar{\\alpha}_{t-1})} \\\\\n\\sigma_q(t) &= \\frac{(1-\\alpha_t)(1-\\bar{\\alpha}_{t-1})}{(1-\\bar{\\alpha}_t)}\n\\end{align}\n\\] It is worth noting that \\(q(x_{t-1}|x_t, x_0)\\) is not tractable without the \\(x_0\\). If it were, computing \\(p_\\theta(x_{t-1}|x_t)\\) would have been trivial.\nReturning back to the lower bound, we can now rewrite it as, \\[\n\\begin{align}\n\\log p(x) \\ge & E_{q(x_{1:T}|x_0)}\\left[\\log\\frac{p(x_T)p_\\theta(x_0|x_1)}{q(x_1|x_0)} + \\log \\prod_{t=2}^T \\frac{p_\\theta(x_{t-1}|x_0)}{\\frac{q(x_{t-1}|x_t, x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}}\\right]\\\\\n\\ge & E_{q(x_{1}|x_0)}(\\log p_\\theta(x_0|x_1)) - \\mathcal{D}_{KL}(q(x_T|x_0)|| p(x_T)) - \\sum_{t=2}^TE_{q(x_{t}|x_0)}\\left(\\mathcal{D}_{KL}(q(x_{t-1}|x_t, x_0)||p_\\theta(x_{t-1}|x_t))\\right)\n\\end{align}\n\\]\nI understand that I might have skipped quite a few steps in deriving the above. If you wish to see the full expansion you can see that in page 9 of this paper. The middle term of the above has no relation to \\(\\theta\\) and therefore can be ignored."
  },
  {
    "objectID": "blog/2023-04-18-ddpm.html#practical-considerations-of-solving-elbo",
    "href": "blog/2023-04-18-ddpm.html#practical-considerations-of-solving-elbo",
    "title": "Annotated DDPM",
    "section": "Practical Considerations of Solving ELBO",
    "text": "Practical Considerations of Solving ELBO\nFirstly, we set \\(p_\\theta\\) to be Gaussian so that \\[\np_\\theta(x_{t-1}|x_t) = \\mathcal{N}(\\mu_\\theta(x_t, t), \\sigma_q(t)\\mathbf{I})\n\\] where \\(\\mu_\\theta\\) is a neural network that transforms \\(x_t\\) and we set the variance to be equal to that of \\(q(x_{t-1}|x_t, x_0)\\). Note that even if \\(x_t\\) was Gaussian \\(p_\\theta(x_{t-1})\\) is not Gaussian. This is because \\(\\mu_\\theta\\) transforms the distribution.\nIn the ELBO term above, for the expectation terms we simply take a monte-carlo estimate (one sample of the distribution) since the expectations are intractable. This does not detract from estimating \\(p_\\theta\\) since we are doing stochastic gradient descent, and also due to the fact that these single sample estimates are unbiased.\nFor the second term, \\(q(x_T|x_0)\\) is far enough from the original distribution that it is safe to assume that it is a standard normal distribution, and \\(p(x_T)\\) is a standard normal by definition. Regardless, this term does not depend on \\(\\theta\\) therefore can be discarded.\nThe final term is the most important term and works out to optimising \\(\\theta\\) over the following: \\[\n\\argmin_\\theta\\frac{1}{2\\sigma_q^2(t)}||\\mu_q(x_t, x_0) - \\mu_\\theta ||_2^2\n\\] While we could use this loss to optimise, we will refactor further to achieve a similar yet emperically more powerful term. We can reuse \\(q(x_t|x_0)\\) to state that, \\[\nx_0 = \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_0}{\\sqrt{\\bar{\\alpha}_t}}\n\\] Substituting this into \\(\\mu_q\\) we arrive at \\[\n\\mu_q(x_t, x_0)=\\frac{1}{\\sqrt{\\alpha_t}}x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}\\sqrt{\\alpha_t}}\\epsilon_0\n\\] Therefore if we use \\[\n\\mu_\\theta(x_t, x_0)=\\frac{1}{\\sqrt{\\alpha_t}}x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}\\sqrt{\\alpha_t}}\\epsilon_\\theta(x_t, x_0)\n\\] we arrive at our final loss function: \\[\n\\mathcal{L} = \\argmin_\\theta\\frac{1}{2\\sigma_q^2(t)}\\frac{(1-\\alpha_t)^2}{(1-\\bar{\\alpha}_t)\\alpha_t}||\\epsilon_0 - \\epsilon_\\theta(x_t, t) ||_2^2\n\\] It has been emperically found that we can drop \\(\\frac{1}{2\\sigma_q^2(t)}\\frac{(1-\\alpha_t)^2}{(1-\\bar{\\alpha}_t)\\alpha_t}\\) term. This term can be thought of as a weighting term over the time steps which has been deemed unnecessary. Finally, do note that the loss is with respect to \\(\\epsilon_0\\), and not simply the scaled noise which may be much smaller in magnitude.\nFinally, since we have the ability to sample \\(q(x_t|x_0)\\) directly without having to sample intermediate steps, we can take just a single sample per \\(x_0\\) without summing the KL divergence over all time-steps as suggested.\nThe final algorithm for DDPM can be summarised as follows. Note how for inference we have no option but to sample over all time steps.  Source: Page 4 DDPM paper"
  },
  {
    "objectID": "blog/2023-04-18-ddpm.html#code",
    "href": "blog/2023-04-18-ddpm.html#code",
    "title": "Annotated DDPM",
    "section": "Code",
    "text": "Code\nThere are four aspects to (as far as I know) all diffusion models. These are: 1. Noise Scheduler 2. Noise Estimation Model 3. Training Process 4. Inference Process We will go into depth into each component.\n\nNoise Scheduler\nThe noise scheduler enables us to add noise to the image. While we could have a constant level of noise, the model learns better when it is varied. Below, we vary it linearly, however, another common scheduler is to use a cosine scheduler which performs even better.\nNote how \\(\\bar{\\alpha}_t\\) is precomputed as alphas_cumprod.\n\n\nCode\nclass DDPMScheduler:\n    def __init__(self, beta_start, beta_end, num_train_timesteps):\n        # for the forward process q(x_t|x_0)\n        self.timesteps = torch.arange(num_train_timesteps)\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.num_train_steps = num_train_timesteps\n        self.beta = torch.linspace(self.beta_start, self.beta_end, self.num_train_steps)\n        self.alphas = 1. - self.beta\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        # for the reverse process q(x_{t-1}|x_t,x_0)\n        self.sigmas = (1 - self.alphas[1:]) * (1 - self.alphas_cumprod[:-1]) / (1 - self.alphas_cumprod[1:])\n        self.sigmas = self.sigmas.sqrt()\n        \n        \n    def add_noise(self, x0, noise, t):\n        alphas_cumprod_t = self.alphas_cumprod.to(x0.device)[t].reshape(-1, 1, 1, 1)\n        return alphas_cumprod_t.sqrt() * x0 + (1 - alphas_cumprod_t).sqrt() * noise"
  },
  {
    "objectID": "blog/2023-04-18-ddpm.html#denoising-model",
    "href": "blog/2023-04-18-ddpm.html#denoising-model",
    "title": "Annotated DDPM",
    "section": "Denoising Model",
    "text": "Denoising Model\nWhen speaking of the denoising model, the term UNet gets thrown around alot. However, it is worth noting that there are only two requirements of this model, 1. The model takes in the inputs, \\(x_t\\), the noised image as well as time step \\(t\\). 2. The size of the output has to be the same as \\(x_t\\). It is because of the latter requirement that UNets are commonly used. However, as long as you can project the final dimension back to the same as the input dimension, there is no definite requirement for UNets alone.\nIn the following we will focus on 1. How to add time information to a ConvNet via ResNetWithTimeEmbed. 2. The UNet architecture. Especially focusing on hooks.\n\nInjecting time Information to a Convolutional Network\nAs we denoise our images (or rather estimate the original noise to be more specific), we require an input of the time-step. This allows the network to know the scale of the noise, while also knowing how far it is from the original image. The fact that we vary noise via the Noise Scheduler makes this information even more valuable.\nIn the following I have use a linear layer to project the time step to a higher dimension. The logic being that there ought to be a relationship between time step t and t+1. However, it is also common to simply use an embedding layer here. The dimensionality is the same as the final channel in the last convolutional layer.\nFinally, we repeat this over the width and height dimensions before adding onto x. x in this case could be the original image or one of the intermediate steps through the UNet.\nFor brevity, we will skip the explanation of ResNetBlock in the following as it could simply be a convolutional network.\n\nclass ResNetWithTimeEmbed(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.resnet1 = ResNetBlock(in_channels, out_channels)\n        self.resnet2 = ResNetBlock(out_channels, out_channels, stride)\n        self.time_embedding = nn.Linear(1, out_channels)\n        \n    def forward(self, x: torch.FloatTensor, t: torch.LongTensor) -&gt; torch.FloatTensor:\n        x = self.resnet2(self.resnet1(x))\n        \n        time_embed = self.time_embedding(t)\n        emb = time_embed[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n        \n        return x + emb\n\n\n\nThe UNet architecture\nUNet contains a downscaling architecture followed by upscaling. Both architectures contain ResNetWithTimeEmbed components as dicussed above.\nDown uses nn.MaxPool2d(2) to get the maximum value in a 2x2 region to downscale while, Up uses nn.Upsample to expand the width and height by a factor of 2. The latter takes a linear interpolation method to quadruple the number of pixels. Both methods are preceded by a ResNetWithTimeEmbed which does not change the height or width, but does increase/ decrese the number of channels.\nWhile it was possible to simply do self.up(self.down(x, t), t), it made a significant difference to the loss function (which was previously struggling) to include cross connections. Cross connections are depicted by the grey horizontal arrows below. The loss runs where it was &gt;0.6 were all when the model did not have those cross connections.\n\n \n\nIn order to get these cross connections we use this nifty feature called forward_hooks. Any submodule within a model can register_forward_hooks. It has three inputs into it, 1. The module itself, 2. The current input(s) into the model 3. The output(s).\nFirstly, we save the outputs of the Down modules into the buffer named self.down_outputs. Note how we only do this to the conv_layers of the Down class and does not include the down-sampling maxpool operation.\nThe next step is to add these values in the buffer to the layers of the Up module. This is done again by the forward hook using this function: lambda module, input, output: output + self.down_outputs.pop(-1). This function pops out the last layer of the buffer, but more importantly, it modifies the output. Note how this hook is registered to the self.up.up module. Despite not having submodules like the above self.conv_layers, this hook fires every time self.up.up is called.\nIt is also worth noting that there was a bit of trial and error for me to figure out where to place the hooks so that the shapes match up. I also had to resize the inputs to be of size 32x32 so that the down/up-scaling did not affect the width and height required for this addition operation.\n\nclass Down(nn.Module):\n    def __init__(self, layers: List[int]):\n        super().__init__()\n        self.conv_layers = nn.ModuleList([ResNetWithTimeEmbed(dim_in, dim_out) for dim_in, dim_out in zip(layers[:-1], layers[1:])])\n        self.bns = nn.ModuleList([nn.BatchNorm2d(feature_len) for feature_len in layers[1:-1]])\n        self.down = nn.MaxPool2d(2)\n        \n    def forward(self, x, t):\n        for layer, batch_norm in zip(self.conv_layers[:-1], self.bns):\n            x = self.down(batch_norm(F.gelu(layer(x, t))))\n        return self.down(self.conv_layers[-1](x, t))\n    \nclass Up(nn.Module):\n    def __init__(self, layers: List[int]):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n        self.conv_layers = nn.ModuleList([ResNetWithTimeEmbed(dim_in, dim_out) for dim_in, dim_out in zip(layers[:-1], layers[1:])])\n        self.bns = nn.ModuleList([nn.BatchNorm2d(feature_len) for feature_len in layers[1:-1]])\n        \n    def forward(self, x, t):\n        for layer, batch_norm in zip(self.conv_layers[:-1], self.bns):\n            x = F.gelu(batch_norm(layer(self.up(x), t)))\n        return self.conv_layers[-1](self.up(x), t)\n    \n    \nclass UNet(nn.Module):\n    def __init__(self, layers: List[int]):\n        super().__init__()\n        self.up = Up(layers[::-1])\n        self.down = Down(layers)\n        \n        self.down_outputs = []\n        \n        self.up.up.register_forward_hook(lambda module, input, output: output + self.down_outputs.pop(-1))\n            \n        for module in self.down.conv_layers.children():\n            module.register_forward_hook(lambda module, input, output: self.down_outputs.append(output))\n        \n    def forward(self, x, t):\n        return self.up(self.down(x, t), t)"
  },
  {
    "objectID": "blog/2023-04-18-ddpm.html#training",
    "href": "blog/2023-04-18-ddpm.html#training",
    "title": "Annotated DDPM",
    "section": "Training",
    "text": "Training\nThe training loop is as shown below. The most important thing to note here is how the loss is estimated. Firstly, note that we only take a batch size of time-steps instead of the full possible 1000 steps. This is depite the original loss function requiring you to sum over all timesteps. However, we can think of this is as a noisy estimate which is scaled down by a factor of \\(\\frac{bs}{T}\\). Furthermore, the KL-divergence term is also over the expectation under \\(q(x_t|x_0)\\). This is also ignored and only a single sample of \\(x_t\\) is taken for each \\(x_0\\). This is called taking a monte-carlo estimate in literature, and gives us a noisy estimate of the expectation.\nDespite these approximations, our model manages to learn a good denoiser as shown in the results section. This is due to the fact that we are optimising over many iterations to optimise over \\(\\theta\\). The noisy estimates ends up being of little to no consequence.\nI do also wish to point out that I used gradient clipping. I am fairly convinced that everyone should use this no matter what problem you are tackling using Deep Learning. It made my training significantly smoother.\n\nfor epoch in tqdm(range(EPOCHS)):\n    for i, (x, _) in enumerate(train_dl):\n        x = x.to(DEVICE)\n        noise = torch.randn(x.shape).to(DEVICE)\n        timesteps = torch.randint(0, NUM_DIFFUSION_STEPS, (len(x),)).long().to(DEVICE)\n        noised_images = noise_scheduler.add_noise(x, noise, timesteps)\n        noise_pred = model(noised_images, timesteps[:, None] / NUM_DIFFUSION_STEPS)\n        \n        loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean()        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        \n        if (i + 1) % LOG_FREQUENCY == 0:\n            loss_detached = loss.detach().item()\n            wandb.log({\"loss\": loss_detached})"
  },
  {
    "objectID": "blog/2023-04-18-ddpm.html#inference",
    "href": "blog/2023-04-18-ddpm.html#inference",
    "title": "Annotated DDPM",
    "section": "Inference",
    "text": "Inference\nUnfortunately, inference is costly under DDPM taking a 1000 iterations of the model denoising to reach the final state. The number of steps are continuously becoming less and less with some of the latest papers requiring just 4 iterations.\nIn this case we repeatedly use the distribution \\(p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(\\mu_\\theta(x_t, t), \\sigma_q(t)\\mathbf{I})\\) until we get to \\(x_0\\). Note also how we actually add more noise during the denoising process. \\(\\sigma_t\\) does however get smaller the closer we are to \\(x_0\\).\n\ndef generate_denoised_images_ddpm(\n    model: nn.Module,\n    diffusion_steps: int,\n    x_t: torch.FloatTensor,\n    noise_scheduler\n) -&gt; List[torch.FloatTensor]:        \n    with torch.inference_mode():\n        for t in range(diffusion_steps - 1, 0, -1):\n            time = torch.FloatTensor([t] * len(x_t))[:, None] / diffusion_steps\n            noise_pred = model(x_t.to(DEVICE), time.to(DEVICE))\n            alpha_bar_t = noise_scheduler.alphas_cumprod[t]\n            alpha_t = noise_scheduler.alphas[t]\n            sigma_t = noise_scheduler.sigmas[t - 1]\n            \n            mu_t = (x_t.to(DEVICE) - ((1 - alpha_t) / (1 - alpha_bar_t).sqrt()) * noise_pred) / alpha_t.sqrt()\n            x_t = mu_t + torch.randn_like(mu_t) * sigma_t\n            \n    return x_t"
  },
  {
    "objectID": "blog/2023-04-18-ddpm.html#results",
    "href": "blog/2023-04-18-ddpm.html#results",
    "title": "Annotated DDPM",
    "section": "Results",
    "text": "Results\nThe following shows results of where some of the intermediate steps were also saved."
  },
  {
    "objectID": "blog/2022-09-25-grammar-correction-via-gpt2.html",
    "href": "blog/2022-09-25-grammar-correction-via-gpt2.html",
    "title": "Fine Tuning GPT2 for Grammar Correction",
    "section": "",
    "text": "jimi henfrix fine tuning guitar"
  },
  {
    "objectID": "blog/2022-09-25-grammar-correction-via-gpt2.html#introduction",
    "href": "blog/2022-09-25-grammar-correction-via-gpt2.html#introduction",
    "title": "Fine Tuning GPT2 for Grammar Correction",
    "section": "Introduction",
    "text": "Introduction\nGPT2 is well known for it‚Äôs capabilities to generate text. While we could always use the existing model from huggingface in the hopes that it generates a sensible answer, it is far more profitable to tune it to our own task. In this example I show how to correct grammar using GPT2. While results aren‚Äôt perfect, had I been given enough time and (compute) resources we could have a possible replacement to chrome‚Äôs default grammar correction. If you wish to run this yourself, a working example can be found in this kaggle kernel."
  },
  {
    "objectID": "blog/2022-09-25-grammar-correction-via-gpt2.html#gpt2-model-architecture",
    "href": "blog/2022-09-25-grammar-correction-via-gpt2.html#gpt2-model-architecture",
    "title": "Fine Tuning GPT2 for Grammar Correction",
    "section": "GPT2 Model Architecture",
    "text": "GPT2 Model Architecture\nAs a quick primer on GPT2, note that GPT2 is a decoder only transformer. What this means is that GPT2 is only allowed to pay attention to the current token and the previous tokens. This is in contrast to encoder only transformers like BERT.\nThe reason that this architecture is important is that when it comes to generation time, the only tokens that ought to be visible are the previous tokens. During training, this effect is achieved by making the Attention matrix triangular."
  },
  {
    "objectID": "blog/2022-09-25-grammar-correction-via-gpt2.html#tokenizer",
    "href": "blog/2022-09-25-grammar-correction-via-gpt2.html#tokenizer",
    "title": "Fine Tuning GPT2 for Grammar Correction",
    "section": "Tokenizer",
    "text": "Tokenizer\nFor some odd reason GPT2 does not ship with beginning of sentence or end of sentence tokens. It only contains the padding token natively. Therefore, we need to add these to our tokenizer. As a result of this change, we also need to change the number of embeddings in GPT2 model and hence, language_model.resize_token_embeddings(len(tokenizer)). This will randomly initialise the embeddings for just the new embeddings while we maintain the previously trained embeddings for all other tokens.\nThere are two cases for tokenizing. 1. During training we have both input_sentence and corrected output_sentence. We add a bos token, seperate with a sep token and append a eos token. 2. In the inference stage, we only have access to input_sentence. Therefore, we end those sentences with bos. This logic is captured in the __call__ method below.\n\n\nCode\nclass Tokenizer:\n    def __init__(self, tokenizer, max_len: int):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.bos = tokenizer.bos_token\n        self.eos = tokenizer.eos_token\n        self.sep = tokenizer.sep_token\n        self.num_special_tokens = len(self.tokenizer.all_special_tokens)\n        \n    def __getattr__(self, attribute: str):\n        if hasattr(self.tokenizer, attribute):\n            return getattr(self.tokenizer, attribute)\n        else:\n            raise AttributeError(f\"{attribute} not found\")\n\n    def __call__(self, input_sentences: List[str], output_sentences: Optional[List[str]]=None, device:torch.device=None) -&gt; AutoTokenizer:\n        if output_sentences is None:\n            sentences = [self.bos + x + self.sep for x in input_sentences]\n        else:\n            sentences = [self.bos + x + self.sep + y + self.eos for x, y in zip(input_sentences, output_sentences)]\n        \n        tokenized = self.tokenizer(\n            sentences, \n            truncation=True,\n            padding=True,\n            return_tensors=\"pt\",\n            max_length=self.max_len,\n        )\n        if device is not None:\n            return {key: tensor.to(device) for key, tensor in tokenized.items()}\n        return tokenized\n\n    def decode(self, x: Dict[str, torch.LongTensor]):\n        return [self.tokenizer.decode(sentence[:sentence_len]) for sentence, sentence_len in \n                zip(x[\"input_ids\"], target[\"attention_mask\"].sum(axis=-1))]\n    \n    def batch_decode(self, encoded_outputs: torch.LongTensor) -&gt; List[str]:\n        return self.tokenizer.batch_decode(encoded_outputs.cpu(), skip_special_tokens=True)\n    \n    def __len__(self):\n        return len(self.tokenizer)\n\n\n# get text base and transform\nlanguage_model = AutoModelForCausalLM.from_pretrained(LANGUAGE_MODEL)\ntokenizer = Tokenizer(\n    AutoTokenizer.from_pretrained(\n        LANGUAGE_MODEL, \n        bos_token=\"&lt;|startoftext|&gt;\",\n        eos_token=\"&lt;|endoftext|&gt;\", \n        pad_token=\"&lt;|pad|&gt;\", \n        sep_token=\"&lt;|sep|&gt;\"\n    ),\n    MAX_LEN,\n)\nlanguage_model.resize_token_embeddings(len(tokenizer))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained."
  },
  {
    "objectID": "blog/2022-09-25-grammar-correction-via-gpt2.html#huggingface-datasets",
    "href": "blog/2022-09-25-grammar-correction-via-gpt2.html#huggingface-datasets",
    "title": "Fine Tuning GPT2 for Grammar Correction",
    "section": "(Huggingface) Datasets",
    "text": "(Huggingface) Datasets\nDue to huge kudos to HF‚Äôs new dataset API we can train large (streaming) datasets. In the following block we use the c4 dataset which contains grammar correction paris. We keep the first 100,000 as a valid dataset and the rest for training. I‚Äôm unsure what the group_batch was for. Just copied it from a tutorial.\n\ndata = datasets.load_dataset(\"liweili/c4_200m\", cache_dir=\"/kaggle/working/\", streaming=True, split=\"train\")\\\n        .shuffle(seed=42, buffer_size=10_000)\nc4_valid = data.take(100000)\nc4_train = data.skip(100000)\ndef group_batch(batch):\n    return {k: [v] for k, v in batch.items()}\ntrain_dl = c4_train.map(group_batch, batched=True, batch_size=BATCH_SIZE)\nvalid_dl = c4_valid.map(group_batch, batched=True, batch_size=BATCH_SIZE)"
  },
  {
    "objectID": "blog/2022-09-25-grammar-correction-via-gpt2.html#training",
    "href": "blog/2022-09-25-grammar-correction-via-gpt2.html#training",
    "title": "Fine Tuning GPT2 for Grammar Correction",
    "section": "Training",
    "text": "Training\nLet‚Äôs breakdown the following LightningModule.\n\nFreezing parameters\nI am personally not a fan of training the embeddings. Reason being that during training, we only see a fraction of all possible tokens. Some tokens appearing more frequently than others. So it seems unfair that we update some embeddings, while others do not get a chance to be updates. Therefore, it seems in terms of making the model resillient to unseen tokens, we should freeze the embeddings.\nHowever, given that we have 3 new tokens (bos, eos, sep), what we do instead is every few batches, we reset the embeddings of existing tokens to what we started with.\nif (batch_idx + 1) % 100 == 0:\n    self.model.transformer.wte.weight[:-self.tokenizer.num_special_tokens].data = self.original_embed_weights         \nIn the same thought process I believe that it is beneficial freeze the bottom 2 layers (out of 12) of the transformer. This again is a step to avoid overfitting to our training data.\n\n\nHow we use the data\nThe dataset defined above returns batch which is a dictionary with keys input and output. The input contains the incorrect grammar sentences, while the other contains the corrected setences. While we can match input to output, it is also important for the model to understand when not to do anything. i.e.¬†return the input when it sees a good sentence. Therefore, in common_step you will see input matched with output while also matching output with output.\n\n\nCalculating Loss\nHF transformers luckily takes care of calculating most of the loss for us. The loss is simply given the current token, what is the cross entropy loss over all possible tokens.\nHowever, there are two cases that we need to ignore. In order to ignore a token you simply set the label to -100. This is a special label outlined in the torch cross-entropy docs. 1. When some sentences are shorter than others in the batch. This is given to us by the tokenizer‚Äôs attention_mask. 2. The second case which is not entirely necessary is that we do not need to calculate loss before the sep token. This is due to the fact that the model will always be given the input sentence. We do not need to burden the model further to learn the structure of the incoming sentence. This is why we generate a mask defined by mask = (good_grammar_labels == self.tokenizer.sep_token_id).roll(shifts=1, dims=-1).cumsum(dim=-1) == 0.\n\n\nCode\nclass LightningModule(pl.LightningModule):\n    def __init__(\n        self,\n        model: nn.Module,\n        tokenizer: Tokenizer,\n        generation_kwargs: Dict[str, Any],\n        lr: float = 1e-3,\n    ) -&gt; None:\n        super().__init__()\n        self.model = model\n        self.tokenizer = tokenizer\n        self.lr = lr\n        self.generation_kwargs = generation_kwargs\n        self.original_embed_weights = self.model.transformer.wte.weight[:-self.tokenizer.num_special_tokens].clone()\n        \n        for layer in self.model.transformer.h[:FREEZE_LAYERS]:\n            layer.eval()\n            for p in layer.parameters():\n                p.requires_grad = False\n        \n        self.table_logging = 0\n        \n    def common_step(self, batch: Dict[str, torch.LongTensor]) -&gt; torch.Tensor:\n        good_grammar_batch = self.tokenizer(batch[\"output\"], batch[\"output\"], self.device)\n        good_grammar_labels = good_grammar_batch[\"input_ids\"].clone()\n        good_grammar_labels[good_grammar_batch[\"attention_mask\"] == 0] = LABEL_MASK\n        mask = (good_grammar_labels == self.tokenizer.sep_token_id).roll(shifts=1, dims=-1).cumsum(dim=-1) == 0\n        good_grammar_labels[mask] = LABEL_MASK\n        \n        bad_grammar_batch = self.tokenizer(batch[\"input\"], batch[\"output\"], self.device)\n        bad_grammar_labels = bad_grammar_batch[\"input_ids\"].clone()\n        bad_grammar_labels[bad_grammar_batch[\"attention_mask\"] == 0] = LABEL_MASK\n        mask = (bad_grammar_labels == self.tokenizer.sep_token_id).roll(shifts=1, dims=-1).cumsum(dim=-1) == 0\n        bad_grammar_labels[mask] = LABEL_MASK\n\n        good_grammar_out = self.model(\n            **good_grammar_batch,\n            labels=good_grammar_labels,\n        )\n        bad_grammar_out = self.model(\n            **bad_grammar_batch,\n            labels=bad_grammar_labels,\n        )\n        return good_grammar_out.loss + bad_grammar_out.loss\n        \n    def training_step(\n        self, batch: Dict[str, torch.LongTensor], batch_idx: int,\n    ) -&gt; torch.Tensor:\n        if (batch_idx + 1) % 100 == 0:\n            self.model.transformer.wte.weight[:-self.tokenizer.num_special_tokens].data = self.original_embed_weights\n            \n        loss = self.common_step(batch)     \n        self.log(\"training_loss\", loss, on_step=True, on_epoch=True, batch_size=len(batch[\"input\"]))\n             \n        return loss\n\n    def validation_step(\n        self, batch: Tuple[torch.Tensor, List[str]], batch_idx: int,\n    ) -&gt; torch.Tensor:\n        loss = self.common_step(batch)\n        self.log(\"validation_loss\", loss, on_step=False, on_epoch=True, batch_size=len(batch[\"input\"]))\n        \n        if batch_idx == 0:\n            self.log_examples(batch)\n            \n    def log_examples(self, batch):\n        good_grammar_batch = self.tokenizer(batch[\"output\"], device=self.device)\n        bad_grammar_batch = self.tokenizer(batch[\"input\"], device=self.device)\n        encoded_good_outputs = self.model.generate(**good_grammar_batch, **self.generation_kwargs)\n        encoded_bad_outputs = self.model.generate(**bad_grammar_batch, **self.generation_kwargs)\n        generated_good_sentences = self.tokenizer.batch_decode(encoded_good_outputs)\n        generated_bad_sentences = self.tokenizer.batch_decode(encoded_bad_outputs)\n        \n        data = list(map(list, zip(batch[\"output\"] + batch[\"input\"], generated_good_sentences + generated_bad_sentences)))\n        columns = [\"Actual Sentence\", \"Generated Sentence\"]\n        data = [[x, y.split(x)[1]] for x, y in data]\n        table = wandb.Table(data=data, columns=columns)\n        if self.logger is not None:\n            self.table_logging += 1\n            self.logger.experiment.log({f\"epoch {self.table_logging} results\": table})\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        caption_params = [\n            {\"params\": self.model.transformer.ln_f.parameters() , \"lr\": self.lr},\n            {\"params\": self.model.transformer.h[FREEZE_LAYERS:].parameters() , \"lr\": self.lr},\n            {\"params\": self.model.transformer.wte.parameters() , \"lr\": self.lr},\n        ]\n        return adam.FusedAdam(caption_params)"
  },
  {
    "objectID": "blog/2022-09-25-grammar-correction-via-gpt2.html#results",
    "href": "blog/2022-09-25-grammar-correction-via-gpt2.html#results",
    "title": "Fine Tuning GPT2 for Grammar Correction",
    "section": "Results",
    "text": "Results\nIn order to prove that the model is learning, the following results show the generated text at the outset of training (which is just jibberish). This is to be expected since the model does not understand what a sep token is or what to do with it. \nThe following are the results after 10 epochs. Which are clearly showing great improvement, but still not perfect. For instance, it doesn‚Äôt seem to understand you only capitalize only at the beginning of a sentence. However, as seen in row 23 it seems to be intelligent enough to copy across names such as Conor and nouns such as British."
  },
  {
    "objectID": "blog/2022-09-25-grammar-correction-via-gpt2.html#summary",
    "href": "blog/2022-09-25-grammar-correction-via-gpt2.html#summary",
    "title": "Fine Tuning GPT2 for Grammar Correction",
    "section": "Summary",
    "text": "Summary\nIn summarising the main points made in this article, 1. Freeze the lower layers, and only train the new token embeddings. 2. Calculate loss for only what is necessary."
  },
  {
    "objectID": "blog/2022-09-25-grammar-correction-via-gpt2.html#shameless-self-promotion",
    "href": "blog/2022-09-25-grammar-correction-via-gpt2.html#shameless-self-promotion",
    "title": "Fine Tuning GPT2 for Grammar Correction",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy my course (30 days moneyback)."
  },
  {
    "objectID": "blog/2016-10-20-Keras-LSTM.html",
    "href": "blog/2016-10-20-Keras-LSTM.html",
    "title": "Keras LSTMs",
    "section": "",
    "text": "Keras has been one of the really powerful Deep Learning libraries that allow you to have a Deep Net running in a few lines of codes. Best part, don‚Äôt worry about the math. In the following videos you will find how to implement a popular Recursive Neural Net (RNN) called Long Short Term Memory RNNs (LSTM).\nNote: You could easily replace the LSTM units with Gated Recurrent Units (GRU) with the same function call.\nSource code: https://github.com/sachinruk/PyData_Keras_Talk/blob/master/cosine_LSTM.ipynb\n\n\n\n\n\nFAQ:\n\nWhy do we need a Dense Layer? The output is still one dimensional (y) and therefore the 32 hidden layers need to be projected down to one. Hence the dense layer is used.\nHow do you decide number of layers and number of nodes in each layer? Personally for me this is trial and error. Generally larger number of layers (deeper) is better than going wide (more nodes). But I usually limit myself to 5 at most unless there is a truly large dataset (100MB+)\n\n\n\nReferences\n\nTo understand the maths behind LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\nFor another guide to Keras LSTMs: http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\nIf you are still confused (try my stackoverflow post): http://stackoverflow.com/questions/38714959/understanding-keras-lstms"
  },
  {
    "objectID": "blog/2021-08-28-weighted-cross-entropy-loss.html",
    "href": "blog/2021-08-28-weighted-cross-entropy-loss.html",
    "title": "An Intuitive Loss for Imbalanced Classification",
    "section": "",
    "text": "## Introduction Getting an intuitive loss function when there are large class imbalances remains a hot topic. Some of the common techniques involve, re-weighting classes and lately focal loss. This paper is a good overview of re-weighting methods. The following idea is not mine and is something I saw on kaggle (but could not find again üò¢).\nThe basic crux of the following loss is simple. We will use the cross entropy loss as usual, however we will have a dynamic weighting scheme. We start off as having all classes being equally important. However, as training evolves for each batch we calculate the false negative rate. That is for a given class, what proportion of that class was mis-labelled. During training the model will get better at some classes (especially the over represented), and have a small false negative rate. We use an exponentially smoothed version of this false negative rate as the importance of that class during training."
  },
  {
    "objectID": "blog/2021-08-28-weighted-cross-entropy-loss.html#code",
    "href": "blog/2021-08-28-weighted-cross-entropy-loss.html#code",
    "title": "An Intuitive Loss for Imbalanced Classification",
    "section": "Code",
    "text": "Code\n\nclass WeightedLoss(nn.Module):\n    def __init__(self, num_classes: int, alpha: float=1e-2) -&gt; None:\n        super().__init__()\n        self.num_classes = num_classes\n        self.loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n        self.register_buffer(\"importance\", torch.ones(num_classes).float())\n        self.alpha = alpha\n        \n    def compute_false_negative_rate(self, y, pred_class) -&gt; None:\n        wrong_preds = y != pred_class\n        wrong_categories, false_negatives = y[wrong_preds].unique(return_counts=True)\n        categories, actual_counts = y.unique(return_counts=True)\n        \n        false_negative_rate = torch.zeros_like(categories).float()\n        \n        idx = (categories[:, None] == wrong_categories[None, :]).nonzero(as_tuple=True)[0]\n        false_negative_rate[idx] = false_negatives / actual_counts[idx]\n        \n        self.importance[categories] = self.alpha * false_negative_rate + (1 - self.alpha) * self.importance[categories]\n        \n    def forward(self, logits: torch.FloatTensor, y: torch.LongTensor) -&gt; torch.FloatTensor:\n        pred_class = logits.argmax(dim=1)\n        self.compute_false_negative_rate(y, pred_class)\n        \n        return (self.loss_fn(logits, y) * self.importance[y]).mean()"
  },
  {
    "objectID": "blog/2021-08-28-weighted-cross-entropy-loss.html#results",
    "href": "blog/2021-08-28-weighted-cross-entropy-loss.html#results",
    "title": "An Intuitive Loss for Imbalanced Classification",
    "section": "Results",
    "text": "Results\nThe top image below is with normal cross entropy loss without any re-weighting, and the bottom image is with re-weighting. \n\n\n\ngood result with weighted loss"
  },
  {
    "objectID": "blog/2021-08-28-weighted-cross-entropy-loss.html#shameless-self-promotion",
    "href": "blog/2021-08-28-weighted-cross-entropy-loss.html#shameless-self-promotion",
    "title": "An Intuitive Loss for Imbalanced Classification",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy me a coffee, or better yet buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2016-08-08-XgBoost.html",
    "href": "blog/2016-08-08-XgBoost.html",
    "title": "XgBoost - Machine Learning made EASY!",
    "section": "",
    "text": "One of the machine learning frameworks that has been exploding on the Kaggle scene has been Xgboost. In my personal experience it has been an extremely powerful machine learning algorithm, beating random forests on most problems I‚Äôve played around with.\nThe following video is a quick introduction to XgBoost."
  },
  {
    "objectID": "blog/2021-11-17-collate-fn-vs-normal.html",
    "href": "blog/2021-11-17-collate-fn-vs-normal.html",
    "title": "HuggingFace Tokenizers as Collate Functions Timing ü§ó ü§ñ",
    "section": "",
    "text": "Since I have been trying to use collate functions alot I wanted to see what the speed was with. TLDR: It‚Äôs quicker to use tokenizer after normal batching than it is through a collate function. Not sure why.\n\n\nCode\nBATCH_SIZE = 64\nLANGUAGE_MODEL = \"bert-base-uncased\"\nMAX_TEXT_LENGTH = 256\nNUM_WORKERS = mp.cpu_count()\nN = 100000\n\n\nWe will be using the SNLI dataset sentences (and throwing away labels) for this experiment.\n\n\nCode\nsnli = datasets.load_dataset('snli', split='train')\n\nclass Sentences(Dataset):\n    def __init__(self, data: Dataset, limit: int) -&gt; None:\n        sentences = [[pair[\"hypothesis\"], pair[\"premise\"]] for pair in data]\n        sentences = [sentence for pair in sentences for sentence in pair]\n        self.sentences = sentences[:limit]\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, i):\n        return self.sentences[i]\n\n\nsentence_ds = Sentences(snli, N)\n\n\n\n\n\n\n\n\nDownloading and preparing dataset snli/plain_text (download: 90.17 MiB, generated: 65.51 MiB, post-processed: Unknown size, total: 155.68 MiB) to /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b...\nDataset snli downloaded and prepared to /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n\n\nLet‚Äôs define a collate function, which is just your usual HuggingFace tokenizer, but with some defaults.\n\n\nCode\ntokenizer = AutoTokenizer.from_pretrained(LANGUAGE_MODEL)\n\nclass CollateFn:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        \n    def __call__(self, x):\n        return self.tokenizer(\n            x, \n            max_length=MAX_TEXT_LENGTH, \n            truncation=True, \n            padding=\"max_length\", \n            return_tensors=\"pt\"\n        )\n    \ncollate_fn = CollateFn(tokenizer)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen in the following two experiments, the inline collate_fn is twice as slow. Would be great to hear your opinions as to why. My only guess is that considering the DataLoader multiprocessor is clashing with the tokenizer multiprocessor. However, changing workers to 1 in second cell below did nothing to help.\n\n%%time\nsentence_dl = DataLoader(\n    sentence_ds,\n    BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    shuffle=False,\n    drop_last=False,\n    pin_memory=True,\n)\n\nfor batch in tqdm(sentence_dl):\n    x = collate_fn(batch)\n\n\n\n\nCPU times: user 15.5 s, sys: 743 ms, total: 16.3 s\nWall time: 13.8 s\n\n\n\n%%time\nsentence_dl = DataLoader(\n    sentence_ds,\n    BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    shuffle=False,\n    drop_last=False,\n    pin_memory=True,\n    collate_fn=collate_fn,\n)\n\nfor batch in tqdm(sentence_dl):\n    continue\n\n\n\n\nCPU times: user 13.4 s, sys: 1.66 s, total: 15.1 s\nWall time: 28.1 s"
  },
  {
    "objectID": "blog/2015-08-10-von-Mises-Fisher.html",
    "href": "blog/2015-08-10-von-Mises-Fisher.html",
    "title": "von Mises-Fisher Distribution",
    "section": "",
    "text": "The von Mises Fisher Distribution is a multivariate distribution on a hyper sphere. I have decided to share the expectation and covariance of the vMF distribution. The Wikipedia page doesn‚Äôt give much info of this distribution."
  },
  {
    "objectID": "blog/2015-08-10-von-Mises-Fisher.html#expectation-of-vmf-distribution",
    "href": "blog/2015-08-10-von-Mises-Fisher.html#expectation-of-vmf-distribution",
    "title": "von Mises-Fisher Distribution",
    "section": "Expectation of vMF distribution",
    "text": "Expectation of vMF distribution\nLet \\(C\\) be the normalising constant.\n\\[\n\\int_{||\\mathbf{x}||_2=1}\\exp(\\kappa\\mathbf{\\mu}^T\\mathbf{x})\\,d\\mathbf{x} = \\frac{(2\\pi)^{d/2-1} I_{d/2-1}(\\kappa)}{\\kappa^{d/2-1}}=C\n\\]\nLet \\(=\\). Therefore \\(=\\).\n\\[\n\\begin{align}\n\\frac{d\\kappa}{d\\mathbf{y}}=\\frac{1}{2}\\frac{\\mathbf{y}}{\\sqrt{\\mathbf{y}^T\\mathbf{y}}}=\\frac{\\kappa\\mathbf{\\mu}}{\\kappa}=\\mathbf{\\mu}\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\int \\mathbf{x} \\exp(\\mathbf{y}^T \\mathbf{x}) d\\mathbf{x} =& \\frac{d}{d\\mathbf{y}} \\int \\exp(\\mathbf{y}^T \\mathbf{x}) d\\mathbf{x}\\\\\n=& \\frac{d\\kappa}{d\\mathbf{y}} \\frac{d}{d\\kappa} \\int \\exp(\\mathbf{y}^T \\mathbf{x}) d\\mathbf{x} \\\\\n=& \\mathbf{\\mu} \\frac{d}{d\\kappa} \\frac{(2\\pi)^{d/2-1} I_{d/2-1}(\\kappa)}{\\kappa^{d/2-1}} \\\\\n=& \\mathbf{\\mu} \\left(\\frac{I'_{d/2-1}(\\kappa)}{I_{d/2-1}(\\kappa)} - \\frac{d/2-1}{\\kappa}\\right) \\frac{(2\\pi)^{d/2-1} I_{d/2-1}(\\kappa)}{\\kappa^{d/2-1}}\\\\\nE(\\mathbf{x}) =& \\frac{\\int \\mathbf{x} \\exp(\\mathbf{y}^T \\mathbf{x}) d\\mathbf{x}}{\\int \\exp(\\mathbf{y}^T \\mathbf{x}) d\\mathbf{x}} = \\mathbf{\\mu} \\left(\\frac{I'_{d/2-1}(\\kappa)}{I_{d/2-1}(\\kappa)} - \\frac{d/2-1}{\\kappa}\\right)\\\\\nE(\\mathbf{x}) =& \\frac{I_{d/2}(\\kappa)}{I_{d/2-1}(\\kappa)}\\mathbf{\\mu}\n\\end{align}\n\\]\nThis is an interesting result because its saying that the mean of a von Mises-Fisher distribution is NOT \\(\\). It is infact multiplied a constant \\[ \\frac{I_{d/2}(\\kappa)}{I_{d/2-1}(\\kappa)} \\] which is between \\((0,1)\\). If you think about a uniformly distributed vMF this makes sense (\\(\\)). If we average all those vectors pointing in different directions it averages very close to 0. This whole ‚Äòaveraging‚Äô of unit vectors is what makes the expected value not equal \\(\\) but a vector pointing in the same direction but smaller in length.\n##Covariance of von Mises-Fisher Distribution\nUsing the same differential approach we can find \\[E(\\mathbf{xx}^T)\\] and hence the covariance by using the identity \\[cov(\\mathbf{x},\\mathbf{x})=E(\\mathbf{xx}^T)-E(\\mathbf{x})E(\\mathbf{x})^T\\]. Hence the covariance is,\n\\[\n\\begin{align}\n\\frac{h(\\kappa)}{\\kappa}\\mathbf{I}+\\left(1-2\\frac{\\nu+1}{\\kappa}h(\\kappa)-h(\\kappa)^2\\right)\\mathbf{\\mu}\\mathbf{\\mu}^T\n\\end{align}\n\\]\nwhere \\[h(\\kappa)=\\frac{I_{\\nu+1}(\\kappa)}{I_{\\nu}(\\kappa)}\\] and \\[\\nu=d/2-1\\]."
  },
  {
    "objectID": "blog/2021-04-05-tabnet_from_scratch.html",
    "href": "blog/2021-04-05-tabnet_from_scratch.html",
    "title": "The Annotated TabNet",
    "section": "",
    "text": "## Introduction We are talking about TabNet today which is a network designed for Tabular data. One aspect that tree based models such as Random Forest (RF) and XgBoost can claim over Neural Nets is the explainability of the model.\nPersonally, one of the coolest features of this network is the ability for the network to point out which features were used in it‚Äôs final decision. And these features change from input to input. We have a visual demo using MNIST below. RF does not have this kind of flexibility and is only one static graph for any kind of input."
  },
  {
    "objectID": "blog/2021-04-05-tabnet_from_scratch.html#acknowledgement",
    "href": "blog/2021-04-05-tabnet_from_scratch.html#acknowledgement",
    "title": "The Annotated TabNet",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nKudos to Paul Tune for pointing out how to configure the loss function in keras. In terms of the actual implementation it was done very much thanks to this YouTube talk done by Sebastien Fischman and hosted by Abhishek Thakur. Much of the TensorFlow code was very much a reimplimentation of the PyTorch code."
  },
  {
    "objectID": "blog/2021-04-05-tabnet_from_scratch.html#model",
    "href": "blog/2021-04-05-tabnet_from_scratch.html#model",
    "title": "The Annotated TabNet",
    "section": "Model",
    "text": "Model\nThis section will outline the blocks that we have used in creating the overall model. See the diagram above to refer to the sections mentioned below. We will not be talking about part (b) of the diagram as that from my understanding is only used for completing missing values.\n\nFully-Connected Block\nLet‚Äôs look at section (c) of the diagram and especially one set of the blue-green-red boxes and call that one Fully-Connected layer. Instead of ReLU the authors have used a GLU activation (the calculation of which is shown in the first two lines). The FCBlock will form as one of the building blocks of the entire architecture.\n\ndef GLU(x):\n    return x * tf.sigmoid(x)\n\nclass FCBlock(layers.Layer):\n    def __init__(self, units):\n        super().__init__()\n        self.layer = layers.Dense(units)\n        self.bn = layers.BatchNormalization()\n\n    def call(self, x):\n        return GLU(self.bn(self.layer(x)))\n\n\n\nFeature Transformer\nNext bit of the feature transformer in section (c) is the Shared Block and the Decision Block which look extrememly similar. This is why I sub-classed DecisionBlock to have parent SharedBlock. Also worth mentioning that this is all possible thanks to the new features of TensorFlow 2.0+.\nThe shared block is simply stacking two FCBlocks with a residual connection which is multiplied by sqrt(0.5). I‚Äôm not sure if this multiplication is necessary but I will leave it in.\nThe decision block is almost the same except that there are two residual connections.\nNow in terms of functionality, refer to section (a) of the diagram. The user needs to define the number of steps as one of the hyper-parameters. The feature transformer is repeated number of steps + 1 many times. In this, the SharedBlock weights are shared across all of that, whereas the DecisionBlock weights are different for each step.\n\nclass SharedBlock(layers.Layer):\n    def __init__(self, units, mult=tf.sqrt(0.5)):\n        super().__init__()\n        self.layer1 = FCBlock(units)\n        self.layer2 = FCBlock(units)\n        self.mult = mult\n\n    def call(self, x):\n        out1 = self.layer1(x)\n        out2 = self.layer2(out1)\n        return out2 + self.mult * out1\n\nclass DecisionBlock(SharedBlock):\n    def __init__(self, units, mult=tf.sqrt(0.5)):\n        super().__init__(units, mult)\n\n    def call(self, x):\n        out1 = x * self.mult + self.layer1(x)\n        out2 = out1 * self.mult + self.layer2(out1)\n        return out2\n\n\n\nAttentive Transformer\nThis is the part where the magic happens. The attentive transformer decides which bits of the input features (x) it needs to pay attention (mask) at each step.\nBefore, talking about the Attentive transformer we need to talk about the split module and the prior layers. The split module simply splits the output of the feature transformer into two portions. One portion which feeds into the attentive transformer, and the other which goes into the output of our overall network. The portions (n_d, n_a) are hyper-parameters that the user needs to specify and it would sum to the number of output nodes of the decision layer.\nThe attentive layer takes the n_a output nodes from the decision block, runs it through a dense layer and batch norm layer before passing through a sparsemax layer. Sparsemax is similar to softmax in that the output sums to one. However, it drives some of the smaller outputs to exactly zero unlike softmax (which can only get close to zero). It is important to note that the dense layer used here projects n_a nodes to the same dimensionality as the input features, as we will use this as a mask to select features.\nThe prior layer is used as a tool to suppress some of the inputs that were used before in a previous step. In the first step none of the input had been used before, and therefore the output of the attentive transformer is unaffected by the prior. However, in the second step (and onwards), the prior is updated to be prior = previous_prior * (constant - previous_mask). The constant mentioned here is a number close to one. Therefore, if the previous_mask output by the attentive mask had numbers close to one, the prior is pushed closer to zero, whereas the unused inputs having a mask close to zero, would have a prior closer to one. The priors closer to zero would effectively act as a suppressant before passing into the sparsemax activation in the attentive layer.\nThe reason that we attempt to use this kind of prior is so that each step attempts to find unique features which adds to the output of the network.\n\nclass Prior(layers.Layer):\n    def __init__(self, gamma=1.1):\n        super().__init__()\n        self.gamma = gamma\n\n    def reset(self):\n        self.P = 1.0\n\n    def call(self, mask):\n        self.P = self.P * (self.gamma - mask)\n        return self.P\n\n\nclass AttentiveTransformer(layers.Layer):\n    def __init__(self, units):\n        super().__init__()\n        self.layer = layers.Dense(units)\n        self.bn = layers.BatchNormalization()\n\n    def call(self, x, prior):\n        return sparsemax(prior * self.bn(self.layer(x)))\n\n\n\nPutting it All Together\nMuch of the architecture has already been discussed in the two previous sections. However, there is two more missing pieces.\nFirstly, the loss function. In additional to the loss function that is used for the task (CrossEntropy in the case of MNIST classification), there is an additional loss on the mask values to drive the values towards either 0 or 1. The (entropy) loss is defined as follows: \\[\n\\begin{align}\nL_{mask} = - M \\log (M + \\epsilon)\n\\end{align}\n\\] The plot below shows entropy as a function of one mask value. We average out all available mask entropy values to get final loss. Note how they are minimised at either extreme.\n\n\n\n\n\nThe second part that we have not discussed so far is the actual output of the model. The n_d number of inputs that do not go through the attentive layer gets passed through a ReLU activation at each step before being added up for the final output. This is compared against a target by using a task specific loss function (cross-entropy in our case).\nOne other thing I‚Äôd like to bring your attention to (see what I did there?) is the fact that I used a for loop inside the call function in the Module below. It might seem like a minor thing, but this kind of flexibility in building a model was not available before to my knowledge.\n\n\nMask Importance\nThe whole point of building this model was to be able to select features and to be able to explain the model. The way that we calculate feature importance is by calculating the importance of each step, and by multiplying that by the mask.\nStep importance is calculated as the sum of the (n_d) outputs of the feature transformer.\n\n\nCode\nclass TabNet(keras.Model):\n    def __init__(self, input_dim, output_dim, steps, n_d, n_a, gamma=1.3):\n        super().__init__()\n        # hyper-parameters\n        self.n_d, self.n_a, self.steps = n_d, n_a, steps\n        # input-normalisation\n        self.bn = layers.BatchNormalization()\n        # Feature Transformer\n        self.shared = SharedBlock(n_d+n_a)\n        self.first_block = DecisionBlock(n_d+n_a)\n        self.decision_blocks = [DecisionBlock(n_d+n_a)] * steps\n        # Attentive Transformer\n        self.attention = [AttentiveTransformer(input_dim)] * steps\n        self.prior_scale = Prior(gamma)\n        # final layer\n        self.final = layers.Dense(output_dim)\n\n        self.eps = 1e-8\n        self.add_layer = layers.Add()\n\n    @tf.function\n    def call(self, x):\n        self.prior_scale.reset()\n        final_outs = []\n        mask_losses = []\n\n        x = self.bn(x)\n        attention = self.first_block(self.shared(x))[:,:self.n_a]\n        for i in range(self.steps):\n            mask = self.attention[i](attention, self.prior_scale.P)\n            entropy = mask * tf.math.log(mask + self.eps)\n            mask_losses.append(\n                -tf.reduce_sum(entropy, axis=-1) / self.steps\n            )\n\n            prior = self.prior_scale(mask)\n            out = self.decision_blocks[i](self.shared(x * prior))\n            attention, output = out[:,:self.n_a], out[:,self.n_a:]\n            final_outs.append(tf.nn.relu(output))\n\n        final_out = self.add_layer(final_outs)\n        mask_loss = self.add_layer(mask_losses)\n\n        return self.final(final_out), mask_loss\n\n    def mask_importance(self, x):\n        self.prior_scale.reset()\n        feature_importance = 0\n\n        x = self.bn(x)\n        attention = self.first_block(self.shared(x))[:,:self.n_a]\n        for i in range(self.steps):\n            mask = self.attention[i](attention, self.prior_scale.P)\n\n            prior = self.prior_scale(mask)\n            out = self.decision_blocks[i](self.shared(x * prior))\n            attention, output = out[:,:self.n_a], out[:,self.n_a:]\n            step_importance = tf.reduce_sum(tf.nn.relu(output), axis=1, keepdims=True)\n            feature_importance += mask * step_importance\n\n        return feature_importance"
  },
  {
    "objectID": "blog/2021-04-05-tabnet_from_scratch.html#loss-and-fitting",
    "href": "blog/2021-04-05-tabnet_from_scratch.html#loss-and-fitting",
    "title": "The Annotated TabNet",
    "section": "Loss and Fitting",
    "text": "Loss and Fitting\nNow this is the part that I thought was strange in the keras API, unless I‚Äôm doing something wrong. Looking at the TabNet class above, it returns the actual prediction, and the mask_loss. From my understanding of loss functions in keras, it is always the ‚Äútrue y‚Äù and the ‚Äúpredictions‚Äù over which we need to define a loss.\nAs shown below my initial attempt was to unpack the predictions into logits and mask losses, and then in the next line to take the cross entropy and average out the mask loss. This however resulted in an error.\nWhat did work however, was to define mask_loss function defined below which ignores the y_true input, and to add the loss_weights during model.compile. It seems like attempting to add a breakpoint and debug does not seem to work after you run model.compile.\n\n\nCode\nfrom keras.losses import SparseCategoricalCrossentropy\n\nsce = SparseCategoricalCrossentropy(from_logits=True)\nreg_sparse = 0.01\ndef full_loss(y_true, y_pred):\n    logits, mask_loss = y_pred\n    return sce(y_true, logits) + reg_sparse * mask_loss.mean()\n\n\n\n\nCode\ndef mask_loss(y_true, mask_losses):\n    return tf.reduce_mean(mask_losses)\n\nmodel = TabNet(784, 10, 2, 10, 10, 1.3)\nmodel.compile(\n    'Adam', \n    loss=[sce, mask_loss],\n    loss_weights=[1, 0.01]\n)\nmodel.fit(\n    x_train, \n    y_train, \n    epochs=5, \n    batch_size=256, \n    validation_split=0.15,\n    workers=mp.cpu_count()\n)\n\n\nEpoch 1/5\n200/200 [==============================] - 24s 96ms/step - loss: 1.7993 - output_1_loss: 1.7842 - output_2_loss: 1.5173 - val_loss: 0.6537 - val_output_1_loss: 0.6461 - val_output_2_loss: 0.7665\nEpoch 2/5\n200/200 [==============================] - 19s 94ms/step - loss: 0.5893 - output_1_loss: 0.5828 - output_2_loss: 0.6446 - val_loss: 0.2964 - val_output_1_loss: 0.2923 - val_output_2_loss: 0.4080\nEpoch 3/5\n200/200 [==============================] - 19s 94ms/step - loss: 0.2887 - output_1_loss: 0.2849 - output_2_loss: 0.3869 - val_loss: 0.2182 - val_output_1_loss: 0.2151 - val_output_2_loss: 0.3119\nEpoch 4/5\n200/200 [==============================] - 19s 95ms/step - loss: 0.2148 - output_1_loss: 0.2118 - output_2_loss: 0.3055 - val_loss: 0.1908 - val_output_1_loss: 0.1882 - val_output_2_loss: 0.2625\nEpoch 5/5\n200/200 [==============================] - 19s 94ms/step - loss: 0.1764 - output_1_loss: 0.1737 - output_2_loss: 0.2686 - val_loss: 0.1775 - val_output_1_loss: 0.1751 - val_output_2_loss: 0.2383\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7f6402c12510&gt;"
  },
  {
    "objectID": "blog/2021-04-05-tabnet_from_scratch.html#results",
    "href": "blog/2021-04-05-tabnet_from_scratch.html#results",
    "title": "The Annotated TabNet",
    "section": "Results",
    "text": "Results\nModel has 91.8% accuracy on test set. See below for calculation.\n\n\nCode\ny_pred = model.predict(x_test)[0].argmax(axis=-1)\n(y_pred == y_test).mean()\n\n\n0.918\n\n\nLet‚Äôs see what features got picked up for the first ten images:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen in most images the important features on the actual digit itself. However, with the first two images above there are no important features in the actual digit.\nIt is worth noting that the MNIST digit is not a binary image and even in the far corners the pixel values may not be exactly zero. Alternatively, it is also fathomable to think that the net is making sure that the pixels are zero in certain parts of the image to ensure that the digit is of a certain shape."
  },
  {
    "objectID": "blog/2021-04-05-tabnet_from_scratch.html#conclusion",
    "href": "blog/2021-04-05-tabnet_from_scratch.html#conclusion",
    "title": "The Annotated TabNet",
    "section": "Conclusion",
    "text": "Conclusion\nHopefully that demystified TabNets. I personally feel like the sparsemax activation is not explored enough in other areas of DL and probably has more to contribute in the coming years.\nIn terms of TabNet, it‚Äôs great to see explainability being the pure focus of a paper. The experiments conducted in the paper claims that it has beaten XgBoost in some benchmarks and hopefully TabNet will be the gold standard in this space.\nFinally, it was really good to play with Tensorflow 2 in a familiar environment to pytorch users."
  },
  {
    "objectID": "blog/2021-04-05-tabnet_from_scratch.html#shameless-self-promotion",
    "href": "blog/2021-04-05-tabnet_from_scratch.html#shameless-self-promotion",
    "title": "The Annotated TabNet",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nSee here for my course on Machine Learning and Deep Learning (Use code DEEPSCHOOL-MARCH to 85% off)."
  },
  {
    "objectID": "blog/2022-03-22-ann-part2.html",
    "href": "blog/2022-03-22-ann-part2.html",
    "title": "Vector Database from Scratch",
    "section": "",
    "text": "Now that embeddings are becoming a vital part of search algorithms, the next question is how do we do that at scale. There are a lot of vendor Vector Databases popping up, and here we will explore one of those algorithms, ANNOY. We will be implement Approximate Nearest Neighbours Oh Yeah from scratch. We will use a synthetic dataset of a million points (N) and 768 dimensions (D). If we have K query points the run time of brute force search is \\(O(KND)\\). The ANNOY algorithm aims to bring that down to \\(O(K \\log N D)\\).\nDisclaimer: More often than not you will find that brute force search is fast enough. Especially if the number of vectors you have is &lt;1M. If you have a GPU you can stretch this even further due to the embarassingly parallel nature of matrix multiplication.\n &gt; A vector database as generated by Stable Diffusion"
  },
  {
    "objectID": "blog/2022-03-22-ann-part2.html#introduction",
    "href": "blog/2022-03-22-ann-part2.html#introduction",
    "title": "Vector Database from Scratch",
    "section": "",
    "text": "Now that embeddings are becoming a vital part of search algorithms, the next question is how do we do that at scale. There are a lot of vendor Vector Databases popping up, and here we will explore one of those algorithms, ANNOY. We will be implement Approximate Nearest Neighbours Oh Yeah from scratch. We will use a synthetic dataset of a million points (N) and 768 dimensions (D). If we have K query points the run time of brute force search is \\(O(KND)\\). The ANNOY algorithm aims to bring that down to \\(O(K \\log N D)\\).\nDisclaimer: More often than not you will find that brute force search is fast enough. Especially if the number of vectors you have is &lt;1M. If you have a GPU you can stretch this even further due to the embarassingly parallel nature of matrix multiplication.\n &gt; A vector database as generated by Stable Diffusion"
  },
  {
    "objectID": "blog/2022-03-22-ann-part2.html#annoy-algorithm",
    "href": "blog/2022-03-22-ann-part2.html#annoy-algorithm",
    "title": "Vector Database from Scratch",
    "section": "ANNOY algorithm",
    "text": "ANNOY algorithm\nThe premise of the algorithm lies in recursively partitioning the space of data points until min_leaf number of data points are left in that sub-space. The following high level steps shows how to construct a tree. 1. Initialise the list of indices to include all data points. 2. Choose a subset of x based on indices. 3. If the number of data points in subset is less that min_leaf stop. 4. Choose two data points, complete randomly. Store these two. 5. Choose data points close to first data point, and set indices to that. Go to 2. 6. Choose data points close to second data point, and set indices to that. Go to 2.\n\n\nCode\nclass AnnoyTree:\n    def __init__(self, max_depth, min_leaf, dim):\n        self.max_depth = max_depth\n        self.min_leaf = min_leaf\n        self.labels = None\n        self.max_level = 2 ** max_depth\n        # self.centers = torch.zeros(self.max_level + 1, dim)\n        self.centers = {}\n        self.leaf = {}\n        \n    def fit(self, x, idx=None, current_label=0):\n        if self.labels is None:\n            self.labels = np.zeros(len(x), dtype=np.int32)\n            idx = self.labels == 0\n            \n        next_label = 2 * current_label + 1\n        \n        x_subset = x[idx]\n        if len(x_subset) &lt;= self.min_leaf or current_label &gt;= self.max_level:\n            self.leaf[next_label] = x_subset\n            return\n        \n        # choose 2 points at random\n        center_idx = np.random.choice(len(x_subset), 2, replace=False)\n        x_centers = x_subset[center_idx]\n        self.centers[next_label] = x_centers # save centers\n        self.labels[idx] = next_label + (x_subset @ x_centers.T).argmax(dim=-1) # trick of 2n + 1, 2n + 2\n        \n        # assign left to cluster l\n        self.fit(x, self.labels == next_label, next_label)\n        self.fit(x, self.labels == next_label + 1, next_label + 1)\n        \n    def predict(self, x: torch.FloatTensor) -&gt; torch.FloatTensor:\n        vecs, similarities = zip(*[self.get_closest(row, 0) for row in x])\n        return torch.stack(vecs), similarities\n    \n    def get_closest(self, x: torch.FloatTensor, idx: int) -&gt; torch.FloatTensor:\n        current_index = 2 * idx + 1\n        if current_index in self.leaf:\n            val, idx = (x @ self.leaf[current_index].T).topk(1)\n            return self.leaf[current_index][idx.item()], val.item()\n        # closest_index = 2 * idx + 1 + (x @ self.centers[[2 * idx + 1, 2 * idx + 2]].T).argmax(dim=-1)\n        closest_index = current_index + (x @ self.centers[current_index].T).argmax(dim=-1)\n        return self.get_closest(x, closest_index.item())"
  },
  {
    "objectID": "blog/2022-03-22-ann-part2.html#fit-method",
    "href": "blog/2022-03-22-ann-part2.html#fit-method",
    "title": "Vector Database from Scratch",
    "section": "fit method",
    "text": "fit method\nLet‚Äôs go through the fit method in the above class.\nx_subset = x[idx]\nif len(x_subset) &lt;= self.min_leaf or current_label &gt;= self.max_level:\n        self.leaf[2 * current_label + 1] = x_subset\n        return\nThe above code saves the subset of datapoints if the conditions for a leaf are met. This is done so that we can compare a query datapoints against our data at a leaf level. Note that this does mean we have \\(O(ND)\\) storage costs.\nNote that we can use the \\(2n+1, 2n+2\\) trick to make sure that we don‚Äôt overlap labels. This also ensures that if we need a parent label we can simply do current_label // 2 to get to the parent label. This avoids us needing to have left and right nodes.\ncenter_idx = np.random.choice(len(x_subset), 2, replace=False)\nx_centers = x_subset[center_idx]\nself.centers[next_label] = x_centers # save centers\nself.labels[idx] = next_label + (x_subset @ x_centers.T).argmax(dim=-1)\nThis code block chooses 2 datapoints randomly and stores them. It also uses this line to assign what the level ought to be from 2n+1, 2n+2, next_label + (x_subset @ x_centers.T).argmax(dim=-1). This is done since argmax will return 0 or 1 and we simply add that to 2n+1 to get the child label.\nThe final two lines simply recursively calls the fit method until a stop condition is met."
  },
  {
    "objectID": "blog/2022-03-22-ann-part2.html#predict-method",
    "href": "blog/2022-03-22-ann-part2.html#predict-method",
    "title": "Vector Database from Scratch",
    "section": "predict method",
    "text": "predict method\nThe predict method takes the tree constructed in above step and compares them against the stored branches until a leaf node is reached. Once at a leaf node it does a brute force search to get the closest element in that block.\n\n\nCode\ntree = AnnoyTree(15, 1000, x.shape[1])\ntree.fit(x)"
  },
  {
    "objectID": "blog/2022-03-22-ann-part2.html#results",
    "href": "blog/2022-03-22-ann-part2.html#results",
    "title": "Vector Database from Scratch",
    "section": "Results",
    "text": "Results\nAs can be seen below, to predict closest vector on 1000 vectors takes 344ms while a full brute force search takes 17 seconds. That‚Äôs a 50x scale up in speed.\n\n%%time\nvecs, similarities = tree.predict(x_new)\n\nCPU times: user 342 ms, sys: 3.23 ms, total: 345 ms\nWall time: 344 ms\n\n\n\n%%time\nmax_similarity, max_idx = (x_new @ x.T).topk(1, dim=-1)\n\nCPU times: user 33.1 s, sys: 1.39 s, total: 34.5 s\nWall time: 16.5 s\n\n\nGiven the actual maximum similarity below, we can see that approximate method captures a close vector, but not the best. If you are wondering why the numbers are relatively small (~0.18) keep in mind that two random vectors are highly likely to be orthogonal and very close to zero the higher the number of dimensions\n\nmax_similarity.squeeze()[:10]\n\ntensor([0.1821, 0.1769, 0.1651, 0.1692, 0.1798, 0.1711, 0.1764, 0.1796, 0.1751,\n        0.1716])\n\n\n\n\nCode\nsimilarities[:10]\n\n\n(0.14211197197437286,\n 0.10861558467149734,\n 0.12875324487686157,\n 0.13691002130508423,\n 0.12784186005592346,\n 0.13487347960472107,\n 0.11535458266735077,\n 0.11262157559394836,\n 0.11334729194641113,\n 0.11169558018445969)"
  },
  {
    "objectID": "blog/2022-03-22-ann-part2.html#random-forest-approach",
    "href": "blog/2022-03-22-ann-part2.html#random-forest-approach",
    "title": "Vector Database from Scratch",
    "section": "Random Forest Approach",
    "text": "Random Forest Approach\nIn similar spirit to random forests, we can easily extend a single tree into multiple trees. Below we construct 10 trees. Unlike random forest where we average results across trees, here we take the data point with maximum similarity across the best datapoints chosen by each tree.\n\nN_TREE = 10\ntrees = [AnnoyTree(15, 1000, x.shape[1]) for _ in range(N_TREE)]\nfor tree in tqdm(trees):\n    tree.fit(x)\n\n\n\n\n\nvecs, similarities = zip(*[tree.predict(x_new) for tree in tqdm(trees)])\n\n\n\n\nThe similarity scores shown below are better than the original numbers obtained.\n\ntorch.stack([torch.Tensor(similarity) for similarity in similarities]).amax(dim=0)[:10]\n\ntensor([0.1430, 0.1509, 0.1453, 0.1444, 0.1554, 0.1476, 0.1374, 0.1420, 0.1582,\n        0.1419])"
  },
  {
    "objectID": "blog/2017-07-04-DeepSchool.io.html",
    "href": "blog/2017-07-04-DeepSchool.io.html",
    "title": "DeepSchool.io",
    "section": "",
    "text": "NodeSchool is one of the most inclusive software communities that I have come across. What I liked about it the most is its emphasis on writing code. There are so many meetups that I have been to where I simply listen to talks and go home without much of a takehome message.\nwww.DeepSchool.io is an open-source, community based project to teach the A-Z of Deep Learning (DL). This project came out of a weekly class that I did at Arbor Networks where I work as a Data Scientist.\nPersonally I come from a background where I did a PhD in Machine Learning. However, with the development of tools such as Keras, DL has become a lot more accessible to the general community.\nEven with these available tools teaching Deep Learning can be quite difficult. The first lesson I did was a complete train wreck. I had forgotten where I started and jumped straight into a multi layered Deep Net. I Took for granted that people would understand what a loss function is, and what regression vs logisitic regression is. \nConversely I did not want to spend too much time on the mathematics either. I wanted to create something that would get people tackling DL problems fast instead of diving too deep into the theory. I spent 6 months or so on Andrew Ngs DL course that did go through the theory. This unfortunately did not equip me with the tools necessary towards actually being comfortable with using DL in any meaningful way. The goal is to focus on the bigger picture of what you can do with DL.\n\n\n\nMake Deep Learning easier (minimal code).\nMinimise required mathematics.\nMake it practical (runs on laptops).\nOpen Source Deep Learning Learning.\nGrow a collaborating practical community around DL.\n\nThe assumed knowledge is that you are able to code in Python. I make all code available in Jupyter Notebooks for the sole reason being that you can interact with it. Running on a single python script decreases this interactivity.\nI also use Docker containers along with Docker-compose so that I don‚Äôt have to deal with installation issues. This tends to take up upwards of half an hour at some workshops. Mind you, the current container that I have put up uses 3GB of space.\n\n\n\nThere is still much to do with Deep School. These are some of the most important requirements in order of importance: 1. Use the tutorials! 2. Help with documenting tutorials (there are parts I could have explained better). 3. Contribute tutorials. At the time of writing I am yet to do a LSTM tutorial. Furthermore I am yet to provide the more advanced tutorials such as Attention Networks, Generative Adversarial Networks, Reinforcement Learning etc. 4. Help me setup a website/ forum. I have limited experience with websites. It would be good to provide a NodeSchool.io style webpage so that we could spread the message."
  },
  {
    "objectID": "blog/2017-07-04-DeepSchool.io.html#goals",
    "href": "blog/2017-07-04-DeepSchool.io.html#goals",
    "title": "DeepSchool.io",
    "section": "",
    "text": "Make Deep Learning easier (minimal code).\nMinimise required mathematics.\nMake it practical (runs on laptops).\nOpen Source Deep Learning Learning.\nGrow a collaborating practical community around DL.\n\nThe assumed knowledge is that you are able to code in Python. I make all code available in Jupyter Notebooks for the sole reason being that you can interact with it. Running on a single python script decreases this interactivity.\nI also use Docker containers along with Docker-compose so that I don‚Äôt have to deal with installation issues. This tends to take up upwards of half an hour at some workshops. Mind you, the current container that I have put up uses 3GB of space."
  },
  {
    "objectID": "blog/2017-07-04-DeepSchool.io.html#call-for-contributions",
    "href": "blog/2017-07-04-DeepSchool.io.html#call-for-contributions",
    "title": "DeepSchool.io",
    "section": "",
    "text": "There is still much to do with Deep School. These are some of the most important requirements in order of importance: 1. Use the tutorials! 2. Help with documenting tutorials (there are parts I could have explained better). 3. Contribute tutorials. At the time of writing I am yet to do a LSTM tutorial. Furthermore I am yet to provide the more advanced tutorials such as Attention Networks, Generative Adversarial Networks, Reinforcement Learning etc. 4. Help me setup a website/ forum. I have limited experience with websites. It would be good to provide a NodeSchool.io style webpage so that we could spread the message."
  },
  {
    "objectID": "blog/2021-02-15-tensorflow-learning-rate-finder.html",
    "href": "blog/2021-02-15-tensorflow-learning-rate-finder.html",
    "title": "Tensorflow Learning Rate Finder",
    "section": "",
    "text": "The following tutorial shows how to implement a learning rate finder from scratch, using Keras callbacks.\nBut first a quick refresher on how we would do model fitting on a simple network:"
  },
  {
    "objectID": "blog/2021-02-15-tensorflow-learning-rate-finder.html#imports-and-data",
    "href": "blog/2021-02-15-tensorflow-learning-rate-finder.html#imports-and-data",
    "title": "Tensorflow Learning Rate Finder",
    "section": "Imports and Data",
    "text": "Imports and Data\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\n%matplotlib inline\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(len(x_train), -1)\nx_test = x_test.reshape(len(x_test), -1)\n# Rescale the images from [0,255] to the [0.0,1.0] range.\nx_train, x_test = x_train/255.0, x_test/255.0\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step"
  },
  {
    "objectID": "blog/2021-02-15-tensorflow-learning-rate-finder.html#model",
    "href": "blog/2021-02-15-tensorflow-learning-rate-finder.html#model",
    "title": "Tensorflow Learning Rate Finder",
    "section": "Model",
    "text": "Model\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Input(x_train.shape[-1]))\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n\nmodel.fit(x_train, y_train, batch_size=64, epochs=5)\n\nEpoch 1/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.5480 - accuracy: 0.8500\nEpoch 2/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1601 - accuracy: 0.9546\nEpoch 3/5\n938/938 [==============================] - 3s 3ms/step - loss: 0.1106 - accuracy: 0.9681\nEpoch 4/5\n938/938 [==============================] - 2s 3ms/step - loss: 0.0817 - accuracy: 0.9773\nEpoch 5/5\n938/938 [==============================] - 2s 3ms/step - loss: 0.0632 - accuracy: 0.9811\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7f60660ca0f0&gt;"
  },
  {
    "objectID": "blog/2021-02-15-tensorflow-learning-rate-finder.html#lr-finder",
    "href": "blog/2021-02-15-tensorflow-learning-rate-finder.html#lr-finder",
    "title": "Tensorflow Learning Rate Finder",
    "section": "LR Finder",
    "text": "LR Finder\nLet me outline the logic behind LR finder before we dive into the code. The basic idea is to vary the learning rate and note down the loss. At a certain point when the learning rate is too high the loss will start increasing again.\nTherefore the tasks that we have to do in order are: 1. Get the minimum and maximum learning rate we are willing to look at. 2. Initialise buffers to hold the learning rate and losses. 3. Before we begin this process, get the current model weights so we can restore it later. 4. Get a batch, and get the loss for that batch, and increase the learning rate. 5. Repeat the above step until maximum learning rate is reached. 6. Reset old weights to model. 7. Plot the model.\nThe above 7 steps can be seen in the LRFind class below. on_train_begin, on_train_batch_end, on_train_end are simply callback functions provided by the keras API. Hopefully, they are self explanatory.\n\nclass LRFind(tf.keras.callbacks.Callback): \n    def __init__(self, min_lr, max_lr, n_rounds): \n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.step_up = (max_lr / min_lr) ** (1 / n_rounds)\n        self.lrs = []\n        self.losses = []\n     \n    def on_train_begin(self, logs=None):\n        self.weights = self.model.get_weights()\n        self.model.optimizer.lr = self.min_lr\n\n    def on_train_batch_end(self, batch, logs=None):\n        self.lrs.append(self.model.optimizer.lr.numpy())\n        self.losses.append(logs[\"loss\"])\n        self.model.optimizer.lr = self.model.optimizer.lr * self.step_up\n        if self.model.optimizer.lr &gt; self.max_lr:\n            self.model.stop_training = True\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.weights)\n\nWe want to reset the model since it already learnt something decent above, but feel free to skip the next cell to see if results differ.\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Input(x_train.shape[-1]))\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nBefore we go ahead and run learning rate finder, a few things we should define. - First, we need to use tf.data.Dataset.from_tensor_slices incase there aren‚Äôt enough batches per epoch for learning rate to go from min_lr to max_lr. - We use EPOCHS=1 but, this is a repeating dataset forever as seen in line 6 below. It is lr_finder_steps that force this repetition to stop at 400 batches. - Instead of model.fit(x_train, y_train,...), we use model.fit(train_dataset). - When plotting we use the log scale since we increase learning rate multiplicatively.\n\nEPOCHS = 1\nBATCH_SIZE = 64\nlr_finder_steps = 400\nlr_find = LRFind(1e-6, 1e1, lr_finder_steps)\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\\\n                    .repeat()\\\n                    .shuffle(2048)\\\n                    .batch(BATCH_SIZE)\nmodel.fit(\n    train_dataset,\n    steps_per_epoch=lr_finder_steps,\n    epochs=EPOCHS,\n    callbacks=[lr_find]\n)\n\nplt.plot(lr_find.lrs, lr_find.losses)\nplt.xscale('log')\nplt.show()\n\n400/400 [==============================] - 2s 4ms/step - loss: 1.7651 - accuracy: 0.4492\n\n\n\n\n\nSo looking at the plot above, the minimum occurs at 0.1, however this is most likely going to be unstable. So a good learning rate to use would be 0.01."
  },
  {
    "objectID": "blog/2021-02-15-tensorflow-learning-rate-finder.html#shameless-self-promotion",
    "href": "blog/2021-02-15-tensorflow-learning-rate-finder.html#shameless-self-promotion",
    "title": "Tensorflow Learning Rate Finder",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nI have a Machine Learning (and Deep Learning) course on Udemy. If you use the code DEEPSCHOOL2021 you can get the course for $15 instead of the usual $99."
  },
  {
    "objectID": "blog/2022-02-13-pytorch-prefetch-experiment.html",
    "href": "blog/2022-02-13-pytorch-prefetch-experiment.html",
    "title": "PyTorch prefetch or rather the lack of it",
    "section": "",
    "text": "I had an issue at work where the questions was if I should stream the data from an S3 bucket via the Dataset class, or if I should download first and simply read it in. I was hoping that increasing prefetch_factor in dataloaders would increase the speed when streaming it via S3, and possibly even be an alternative to downloading. For anyone who has not come across this flag, it is meant to get prefetch_factor many batches while the GPU is busy. The purpose being that there is little to no downtime when taking up the next batch."
  },
  {
    "objectID": "blog/2022-02-13-pytorch-prefetch-experiment.html#introduction",
    "href": "blog/2022-02-13-pytorch-prefetch-experiment.html#introduction",
    "title": "PyTorch prefetch or rather the lack of it",
    "section": "",
    "text": "I had an issue at work where the questions was if I should stream the data from an S3 bucket via the Dataset class, or if I should download first and simply read it in. I was hoping that increasing prefetch_factor in dataloaders would increase the speed when streaming it via S3, and possibly even be an alternative to downloading. For anyone who has not come across this flag, it is meant to get prefetch_factor many batches while the GPU is busy. The purpose being that there is little to no downtime when taking up the next batch."
  },
  {
    "objectID": "blog/2022-02-13-pytorch-prefetch-experiment.html#streaming-via-dataloaders",
    "href": "blog/2022-02-13-pytorch-prefetch-experiment.html#streaming-via-dataloaders",
    "title": "PyTorch prefetch or rather the lack of it",
    "section": "Streaming via Dataloaders",
    "text": "Streaming via Dataloaders\nIn order to stream data instead of opening from disk (as is done in many tutorials) the dataset class was setup as the following:\nclass Data(Dataset):\n    def __init__(self, prefix, transform):\n        self.prefix = \"https://aft-vbi-pds.s3.amazonaws.com/bin-images\"\n        self.transform = transform\n        \n    def __len__(self):\n        return 999\n    \n    def __getitem__(self, i):\n        response = requests.get(self.prefix + f\"/{i+1}.jpg\")\n        img = Image.open(BytesIO(response.content))\n        return self.transform(img)\nAs shown in the experiments done in this kaggle kernel, prefetch_factor flag did not speed things in a meaningful manner. The results are summarisd below. For each iteration the following code snippet was run, where model is simply resnet18.\nwith torch.inference_mode():\n    for img_batch in tqdm(dl):\n        out = model(img_batch.to(device))\n\n\n\nSettings\nTime Elapsed\n\n\n\n\nnum_workers = 2\n04:02\n\n\nnum_workers = 2, prefetch_factor=8\n03:57\n\n\nnum_workers = 8\n1:01\n\n\nnum_workers = 8, prefetch_factor=8\n1:01\n\n\n\nAll other parameters such as batch_size=32, pin_memory=True was held constant across all iterations.\nNote that the reason we had 2 workers was due to the fact that this was the number given by multiprocessing.cpu_count(). However, going past that number in the last iteration and setting it at 8 gave the following ugly (repeated) warnings: Exception ignored in: Exception ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__"
  },
  {
    "objectID": "blog/2022-02-13-pytorch-prefetch-experiment.html#downloading-then-reading-in",
    "href": "blog/2022-02-13-pytorch-prefetch-experiment.html#downloading-then-reading-in",
    "title": "PyTorch prefetch or rather the lack of it",
    "section": "Downloading then reading in",
    "text": "Downloading then reading in\nAs the title suggests, needless to say this was the fastest way to conduct this. However, downloading on itself can take a long time which would negate the lack of speed in pytorch dataloaders. The trick here is to use multiprocessing to download data as well. In the following case it took 55 seconds to download the data.\ndef download_file(i):\n    image_url = PREFIX + f\"/{i+1}.jpg\"\n    img_data = requests.get(image_url).content\n    with open(DATA + f\"{i+1}.jpg\", \"wb\") as handler:\n        handler.write(img_data)\n        \n    return DATA + f\"{i+1}.jpg\"\n    \nwith mp.Pool(8) as pool:\n    file_paths = list(tqdm(pool.imap(download_file, range(999)), total=999))\nNote how I set the number of workers / threads (I confess I don‚Äôt know the difference) to 8 which is 4x greater than mp.cpu_count()\nUsing a simple Dataset class where we do Image.open to get the image, and setting num_workers=mp.cpu_count() (2 cores) we were able to run through the data in 6 seconds. Setting prefetch_factor=4 in this scenario actually slowed down the dataloader slightly to 7 seconds."
  },
  {
    "objectID": "blog/2022-02-13-pytorch-prefetch-experiment.html#conclusion",
    "href": "blog/2022-02-13-pytorch-prefetch-experiment.html#conclusion",
    "title": "PyTorch prefetch or rather the lack of it",
    "section": "Conclusion",
    "text": "Conclusion\nSimply due to the ugly warnings, I would say that downloading and reading in is the safest and fastest way to go. In a scenario where you do not have access to that much disk space, you would need to design a download - evaluate - delete - repeat cycle.\nThe disclaimer here is that in either case I had to go beyond the available cores to make this go fast. I‚Äôm not sure if this is safe and would be great if someone who understands threads/ processes comments on the safety of doing this."
  },
  {
    "objectID": "blog/2022-02-13-pytorch-prefetch-experiment.html#shameless-self-promotion",
    "href": "blog/2022-02-13-pytorch-prefetch-experiment.html#shameless-self-promotion",
    "title": "PyTorch prefetch or rather the lack of it",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy me a coffee, or better yet buy my course (usually 90% off)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DeepSchool.ai",
    "section": "",
    "text": "Welcome to my website! I‚Äôm Dr.¬†Sachin Abeywardana, an experienced Deep Learning Engineer specialized in NLP and Computer Vision. I offer my expertise as a consultant, having successfully delivered transformative solutions across various industries. Notably, as Lead ML Engineer at Canva, I innovated customer service using NLP, employing BERT for feedback classification and fine-tuning GPT2 for content titles. At Remi AI, I predicted demand and optimized prices using advanced DL models, whilst also mentoring other data scientists. In my role at Data Processors, I improved sports betting models and video analysis using Computer Vision. Holding a Ph.D.¬†in Bayesian ML, my research includes quantile regression and market segmentation. I‚Äôm also a Udemy Instructor, contributing to the ML community. As a consultant, I aim to drive innovation and collaboration, offering custom AI solutions to push boundaries and achieve exceptional results. Feel free to get in touch with me on LinkedIn."
  },
  {
    "objectID": "blog/2015-08-02-Normal-Distribution.html",
    "href": "blog/2015-08-02-Normal-Distribution.html",
    "title": "Normal Distribution",
    "section": "",
    "text": "No stats blog would be complete without a discussion of the Gaussian distribution. In the following video I discuss how to obtain the mean and variance of a Gaussian. You do need some knowledge of integration.\n\n\nOh, and this is a statisticians ‚Äúhello world‚Äù."
  },
  {
    "objectID": "blog/2022-08-07-transformer_compression.html",
    "href": "blog/2022-08-07-transformer_compression.html",
    "title": "Transformer Model Compression (Attempt)",
    "section": "",
    "text": "image of transformer being crushed"
  },
  {
    "objectID": "blog/2022-08-07-transformer_compression.html#introduction",
    "href": "blog/2022-08-07-transformer_compression.html#introduction",
    "title": "Transformer Model Compression (Attempt)",
    "section": "Introduction",
    "text": "Introduction\nSo straight off the bat let me warn you that this is a failed experiment. However, I do think that the method that I have used here should be interesting enough to warrant a read.\nAs deep learning architectures keep spitting out more and more amazing results whether it be GPT-3 or DALL-E-2 they all rely on gigantic scale which remains unaffordable for most small scale startups.\nDespite the original model remaining out of reach, compressions methods have also been getting popular. These advances come from both the hardware side (eg. float16, quantization) as well as software side. Our focus is on the latter."
  },
  {
    "objectID": "blog/2022-08-07-transformer_compression.html#current-distillation-method",
    "href": "blog/2022-08-07-transformer_compression.html#current-distillation-method",
    "title": "Transformer Model Compression (Attempt)",
    "section": "Current Distillation Method",
    "text": "Current Distillation Method\nDistilbert is one of the popular models in huggingface model hub which is a distilled version of the larger BERT model. If I understand the distillation training method correctly this is the training loss: \\[\n\\mathcal{L}_{ce} = \\sum_i t_i \\log(s_i)\n\\] where \\(t_i\\) is the teacher model (BERT) logits, and \\(s_i\\) are the predicted logits of the student model (DistilBERT).\n\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
  },
  {
    "objectID": "blog/2022-08-07-transformer_compression.html#hypothesis",
    "href": "blog/2022-08-07-transformer_compression.html#hypothesis",
    "title": "Transformer Model Compression (Attempt)",
    "section": "Hypothesis",
    "text": "Hypothesis\nMy hypothesis was that in order to make a model smaller, we don‚Äôt need to simply imitate the entire network, but instead to focus on sub layers (or a group of such). Therefore as you will see below in the Teacher network, I‚Äôve chosen two encoder layers stacked on each other to be the teacher network. Each layer is a combination of standard attention layer upon which we have a few linear layers.\n\nclass Teacher(nn.Module):\n    def __init__(self, bert_model: nn.Module, layers: int):\n        super().__init__()\n        self.layers = bert_model.encoder.layer[:layers]\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)[0] # 0 because HF returns as a tuple\n\n        return x\n\nIn the student network (ApproximateBertAttention) there‚Äôs a few twists (which may or may not have helped). 1. I have a similar network to the teacher, except that it is only one attention like layer upon which we have a single dense layer with a residual connection. 2. I squeeze the number of dimensions to be less than 768 (dimensionality of BERT) in most layers except for the final output layer. 3. The query, key and value layers are actually stacked linear layers, instead of the single linear layer as done in standard Attention layers. 4. The dimensionality of the query and value layers are not necessarily the same. The only actual requirement is that the query and key dimensionality must be the same to calculate softmax step. Despite being the same in the code below, I attempted with slighly lower attention_hidden_size and the drop in performance was slim.\n\n\nCode\ndef linear_projector(input_dim: int, output_dim: int, layers: int = 1) -&gt; nn.Module:\n    layers = sum([[nn.Linear(input_dim, input_dim), nn.GELU()] for _ in range(layers)], []) + [nn.Linear(input_dim, output_dim)]\n    return nn.Sequential(*layers)\n\nclass ApproxBertAttention(nn.Module):\n    def __init__(\n        self,\n        num_attention_heads=12,\n        attention_hidden_size=32,\n        value_dim=384,\n        input_dim=BERT_DIM,\n        output_dim=BERT_DIM,\n        qkv_layers=2,\n        p=0.1,\n        eps=1e-12,\n    ):\n        super().__init__()\n        self.num_attention_heads = num_attention_heads\n        self.attention_head_size = attention_hidden_size\n        self.query_head_size = (num_attention_heads, attention_hidden_size)\n        self.key_head_size = (num_attention_heads, attention_hidden_size)\n        self.value_head_size = (num_attention_heads, value_dim // num_attention_heads)\n        self.value_dim = value_dim\n\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.attention_denominator = self.attention_head_size ** 0.5\n\n        self.query = linear_projector(input_dim, self.all_head_size, qkv_layers)\n        self.key = linear_projector(input_dim, self.all_head_size, qkv_layers)\n        self.value = linear_projector(input_dim, value_dim, qkv_layers)\n        self.dropout_1 = nn.Dropout(p)\n\n        self.dense = linear_projector(value_dim, output_dim, qkv_layers)\n        self.LayerNorm = nn.LayerNorm(output_dim, eps=eps)\n        self.dropout_2 = nn.Dropout(p)\n\n    def transpose_for_scores(self, x, reshape_size):\n        new_x_shape = x.size()[:-1] +  reshape_size\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n    ):\n        query_layer = self.transpose_for_scores(self.query(hidden_states), self.query_head_size)\n        key_layer = self.transpose_for_scores(self.key(hidden_states), self.key_head_size)\n        value_layer = self.transpose_for_scores(self.value(hidden_states), self.value_head_size)\n        \n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        attention_scores = attention_scores / self.attention_denominator\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout_1(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.value_dim,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        context_projection_layer = self.dense(context_layer)\n        context_projection_layer = self.dropout_2(context_projection_layer)\n        # skip layer\n        return self.LayerNorm(context_projection_layer + hidden_states)\n\n\nBelow you can see that the number of parameters in the student network is almost a third\n\ndef get_num_elements(module) -&gt; int:\n    return sum(\n        [torch.prod(torch.tensor(p.shape)) for p in module.parameters()]\n    )\n    \nteacher = Teacher(bert_model, 2)\nstudent = ApproxBertAttention()\n\nprint(get_num_elements(teacher))\nprint(get_num_elements(student))\n\ntensor(14175744)\ntensor(5022336)"
  },
  {
    "objectID": "blog/2022-08-07-transformer_compression.html#data",
    "href": "blog/2022-08-07-transformer_compression.html#data",
    "title": "Transformer Model Compression (Attempt)",
    "section": "Data",
    "text": "Data\nI believe that most distillation methods require you to pass in the correct format of data in order to train. For example a CNN would require images and BERT would require tokenized text. In our case I take quite a different approach and push random numbers through.\nThe intuition is that 1. It will be faster, 2. Considering that the student network is still a universal approximator, we should be able to simply treat the outputs of the teacher model (via random inputs) as the true value that we are trying to approximate.\n\nclass DummyData(Dataset):\n    def __init__(self, seq_length:int, dim: int, batches_per_epoch: int):\n        self.seq_length = seq_length\n        self.dim = dim\n        self.batches_per_epoch = batches_per_epoch\n\n    def __len__(self) -&gt; int:\n        return self.batches_per_epoch\n\n    def __getitem__(self, idx):\n        return torch.randn(self.seq_length, self.dim)\n\ntrain_ds = DummyData(SEQ_LEN, BERT_DIM, 1000 * BATCH_SIZE)\nvalid_ds = DummyData(SEQ_LEN, BERT_DIM, 100 * BATCH_SIZE)\n\ntrain_dl = DataLoader(train_ds, BATCH_SIZE, num_workers=mp.cpu_count(), pin_memory=True)\nvalid_dl = DataLoader(train_ds, BATCH_SIZE, num_workers=mp.cpu_count(), pin_memory=True)"
  },
  {
    "objectID": "blog/2022-08-07-transformer_compression.html#training",
    "href": "blog/2022-08-07-transformer_compression.html#training",
    "title": "Transformer Model Compression (Attempt)",
    "section": "Training",
    "text": "Training\nFinally we use pytorch-lightning along with l1_loss to train our model. We get a loss of 0.22. For comparison if we use a smoothed exponential average of the output of the teacher model as an estimate, the error that we see 0.59, an almost 3x improvement. The code snipped for comparison is shown in the cell before next.\n\n\nCode\nclass LightningModule(pl.LightningModule):\n    def __init__(self, teacher: nn.Module, student: nn.Module, learning_rate: float, loss_fn: nn.Module):\n        super().__init__()\n        self.teacher = teacher.eval()\n        self.student = student\n        self.learning_rate = learning_rate\n        self.loss_fn = loss_fn\n\n    def common_step(self, x: torch.FloatTensor) -&gt; torch.FloatTensor:\n        y = self.teacher(x).detach()\n        y_est = self.student(x)\n        return self.loss_fn(y, y_est) #, basic_error\n\n    def training_step(self, x: torch.FloatTensor, batch_idx: int) -&gt; torch.FloatTensor:\n        loss = self.common_step(x)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, x: torch.FloatTensor, batch_idx: int) -&gt; torch.FloatTensor:\n        loss = self.common_step(x)\n        self.log(\"valid_loss\", loss, on_step=True, on_epoch=True)\n    \n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        return torch.optim.AdamW(self.student.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n    \nlightning_module = LightningModule(teacher, student, 1e-3, loss_fn)\nnum_gpus = torch.cuda.device_count()\ntrainer = pl.Trainer(\n    fast_dev_run=False,\n    max_epochs=2,\n    gpus=num_gpus,\n    gradient_clip_val=1.0,\n    num_sanity_val_steps=0,\n    precision=16 if num_gpus &gt; 0 else 32,\n)\ntrainer.fit(lightning_module, train_dl, valid_dl)\n\n\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\nUsing 16bit native Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name    | Type                | Params\n------------------------------------------------\n0 | teacher | Teacher             | 14.2 M\n1 | student | ApproxBertAttention | 5.0 M \n2 | loss_fn | L1Loss              | 0     \n------------------------------------------------\n19.2 M    Trainable params\n0         Non-trainable params\n19.2 M    Total params\n38.396    Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=2` reached.\n\n\n\n\n\n\n\n\n\n\n\n\nwith torch.inference_mode():\n    mean = torch.zeros(128, 768).to(DEVICE)\n    alpha = 0.01\n    progress_bar = tqdm(train_dl)\n    teacher = teacher.eval().to(DEVICE)\n    for x in progress_bar:\n        y = teacher(x.to(DEVICE))[0]\n        # batch_mean = torch.cat([tensor for tensor in y]).mean(dim=0)\n        batch_mean = y.mean(dim=0)\n        mean = alpha * batch_mean + (1- alpha) * mean\n        error = loss_fn(y, mean)\n        progress_bar.set_description(f\"Current error {error:.4f}\")"
  },
  {
    "objectID": "blog/2022-08-07-transformer_compression.html#conclusion",
    "href": "blog/2022-08-07-transformer_compression.html#conclusion",
    "title": "Transformer Model Compression (Attempt)",
    "section": "Conclusion",
    "text": "Conclusion\nIn concluding, these is clearly more to be done. However, I do believe that we could potentially use this for pretraining distilled networks. This is due to the simple fact that it is faster to generate synthetic random data, than it would be to preprocess raw text or images.\nFurthermore, in this experiment I only stacked 2 layers for for the query, key and value modules. Increasing this, along with the dimensionality of attention_hidden_size could lead to further gains."
  },
  {
    "objectID": "blog/2022-08-07-transformer_compression.html#shameless-self-promotion",
    "href": "blog/2022-08-07-transformer_compression.html#shameless-self-promotion",
    "title": "Transformer Model Compression (Attempt)",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2021-07-03-image-patches.html",
    "href": "blog/2021-07-03-image-patches.html",
    "title": "PyTorch Image Patches",
    "section": "",
    "text": "Getting the 16x16 patches required for the Visual Transformer (ViT) is not that straight forward. This tutorial demonstrates how to use the unfold function in combination with reshape to get the required shape of data.\n\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nfrom torchvision import io, transforms\nfrom torchvision.utils import Image, ImageDraw\nfrom torchvision.transforms.functional import to_pil_image\n\n%matplotlib inline\n\nLet‚Äôs break up our image of size 256 x 256 into 64 x 64 patches. We should end up with 4 rows and 4 columns of these patches.\n\nIMG_SIZE = 256\nPATCH_SIZE = 64\n\nresize = transforms.Resize((IMG_SIZE, IMG_SIZE))\nimg = resize(io.read_image(\"../images/autobot.jpg\"))\n\nThe actual image looks like so:\n\nto_pil_image(img)\n\n\n\n\nThe unfold function can be used to grab a patch of certain size and stride. Unfortunately, you need to use it twice along relevant dimension to get what we are after.\n\npatches = img.unfold(1, PATCH_SIZE, PATCH_SIZE).unfold(2, PATCH_SIZE, PATCH_SIZE)\n\nfig, ax = plt.subplots(4, 4)\nfor i in range(4):\n    for j in range(4):\n        sub_img = patches[:, i, j]\n        ax[i][j].imshow(to_pil_image(sub_img))\n        ax[i][j].axis('off')\n\n\n\n\nAnd finally we can line up the patches and plot them using reshape.\n\npatches = patches.reshape(3, -1, PATCH_SIZE, PATCH_SIZE)\npatches.transpose_(0, 1)\n\nfig, ax = plt.subplots(1, 16, figsize=(12, 12))\nfor i in range(16):\n    ax[i].imshow(to_pil_image(patches[i]))\n    ax[i].axis('off')"
  },
  {
    "objectID": "blog/2021-07-03-image-patches.html#introduction",
    "href": "blog/2021-07-03-image-patches.html#introduction",
    "title": "PyTorch Image Patches",
    "section": "",
    "text": "Getting the 16x16 patches required for the Visual Transformer (ViT) is not that straight forward. This tutorial demonstrates how to use the unfold function in combination with reshape to get the required shape of data.\n\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nfrom torchvision import io, transforms\nfrom torchvision.utils import Image, ImageDraw\nfrom torchvision.transforms.functional import to_pil_image\n\n%matplotlib inline\n\nLet‚Äôs break up our image of size 256 x 256 into 64 x 64 patches. We should end up with 4 rows and 4 columns of these patches.\n\nIMG_SIZE = 256\nPATCH_SIZE = 64\n\nresize = transforms.Resize((IMG_SIZE, IMG_SIZE))\nimg = resize(io.read_image(\"../images/autobot.jpg\"))\n\nThe actual image looks like so:\n\nto_pil_image(img)\n\n\n\n\nThe unfold function can be used to grab a patch of certain size and stride. Unfortunately, you need to use it twice along relevant dimension to get what we are after.\n\npatches = img.unfold(1, PATCH_SIZE, PATCH_SIZE).unfold(2, PATCH_SIZE, PATCH_SIZE)\n\nfig, ax = plt.subplots(4, 4)\nfor i in range(4):\n    for j in range(4):\n        sub_img = patches[:, i, j]\n        ax[i][j].imshow(to_pil_image(sub_img))\n        ax[i][j].axis('off')\n\n\n\n\nAnd finally we can line up the patches and plot them using reshape.\n\npatches = patches.reshape(3, -1, PATCH_SIZE, PATCH_SIZE)\npatches.transpose_(0, 1)\n\nfig, ax = plt.subplots(1, 16, figsize=(12, 12))\nfor i in range(16):\n    ax[i].imshow(to_pil_image(patches[i]))\n    ax[i].axis('off')"
  },
  {
    "objectID": "blog/2021-07-03-image-patches.html#putting-it-all-together",
    "href": "blog/2021-07-03-image-patches.html#putting-it-all-together",
    "title": "PyTorch Image Patches",
    "section": "Putting it all together",
    "text": "Putting it all together\nBefore sending it through to a transformer, we need to reshape our images from being (batch_size, channels, img_height, img_width) to (batch_size, number_patches, pixels) where pixels in the above example would be 64 x 64 x 3 = 12288 pixels.\nTherefore, an example Dataset to read in the images would look like:\n\nfrom torch.utils.data import Dataset\n\nclass ImageData(Dataset):\n    def __init__(self, files: List[str]):\n        self.files = files\n        self.resize = transforms.Resize((IMG_SIZE, IMG_SIZE))\n        self.num_patches = PATCH_SIZE * PATCH_SIZE\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, i):\n        img = self.resize(io.read_image(self.files[i]))\n        patches = img\\\n                    .unfold(1, PATCH_SIZE, PATCH_SIZE)\\\n                    .unfold(2, PATCH_SIZE, PATCH_SIZE)\n        \n        patches = patches.reshape(3, -1, PATCH_SIZE, PATCH_SIZE)\n        patches.transpose_(0, 1)\n        \n        return patches.reshape(self.num_patches, -1)"
  },
  {
    "objectID": "blog/2021-07-03-image-patches.html#shameless-self-promotion",
    "href": "blog/2021-07-03-image-patches.html#shameless-self-promotion",
    "title": "PyTorch Image Patches",
    "section": "Shameless self promotion",
    "text": "Shameless self promotion\nIf you enjoyed the tutorial buy me a coffee, or better yet buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2021-03-07-clip.html",
    "href": "blog/2021-03-07-clip.html",
    "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
    "section": "",
    "text": "CLIP\nThis is a walkthrough of training CLIP by OpenAI. CLIP was designed to put both images and text into a new projected space such that they can map to each other by simply looking at dot products.\nTraditionally training sets like imagenet only allowed you to map images to a single class (and hence one word). This method allows you to map text to images, but can also be used to map images to text if the need arises.\nThis particular blog however is specifically how we managed to train this on colab GPUs using huggingface transformers and pytorch lightning.\nA Working version of this code can be found on kaggle."
  },
  {
    "objectID": "blog/2021-03-07-clip.html#acknowledgement",
    "href": "blog/2021-03-07-clip.html#acknowledgement",
    "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nKudos to the following CLIP tutorial in the keras documentation.\nThe important thing to notice about the constants is the embedding dim. We will project the output of a resnet and transformers into 512 dimensional space.\n\nEMBED_DIM = 512\nTRANSFORMER_EMBED_DIM = 768\nMAX_LEN = 32 # Maximum length of text\nTEXT_MODEL = \"distilbert-base-multilingual-cased\"\n\nEPOCHS = 5\nBATCH_SIZE = 64"
  },
  {
    "objectID": "blog/2021-03-07-clip.html#data",
    "href": "blog/2021-03-07-clip.html#data",
    "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
    "section": "Data",
    "text": "Data\nWe download the coco dataset which contains 5 captions per image and has roughly 82k images. We take 20% of it to be our validation set.\nConsidering that the image backbone is trained using imagenet, we normalise it using the imagenet stats as shown in the transforms normalize step. We also resize the image to 128x128 to make sure it trains in reasonable time.\nWarning: Downloading the files will take a while (~5-10 minutes).\n\nclass Tokenizer:\n    def __init__(self, tokenizer: BertTokenizer) -&gt; None:\n        self.tokenizer = tokenizer\n\n    def __call__(self, x: str) -&gt; AutoTokenizer:\n        return self.tokenizer(\n            x, max_length=MAX_LEN, truncation=True, padding=True, return_tensors=\"pt\"\n        )\n\n    def decode(self, x: Dict[str, torch.LongTensor]):\n        return [self.tokenizer.decode(sentence[:sentence_len]) for sentence, sentence_len in \n                zip(x[\"input_ids\"], x[\"attention_mask\"].sum(axis=-1))]\n\ntokenizer = Tokenizer(AutoTokenizer.from_pretrained(TEXT_MODEL))\n\n\nimg = inv_tfm(img)\nplt.imshow(np.rot90(img.transpose(0, 2), 3))\nplt.title(tokenizer.decode(target)[0])\nplt.show()\n\n\n\n\n\ntrain_len = int(0.8*len(cap))\ntrain_data, valid_data = random_split(cap, [train_len, len(cap) - train_len])\ntrain_dl = DataLoader(train_data, BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=4, drop_last=True)\nvalid_dl = DataLoader(valid_data, BATCH_SIZE, pin_memory=True, shuffle=False, num_workers=4, drop_last=False)\n\n/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))"
  },
  {
    "objectID": "blog/2021-03-07-clip.html#model",
    "href": "blog/2021-03-07-clip.html#model",
    "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
    "section": "Model",
    "text": "Model\nThere are two main models, the VisionEncoder and the TextEncoder which have resnet18 and distilbert as backbones. In order to make it multi-lingual, we simply choose the distilbert-multilingual model and that‚Äôs it! No need to specifically train on non-english words as you will soon see.\nThe Projection module, takes the embeddings from vision and text encoders and projects them into 512 dimensional space.\nTwo things to note: 1. We have frozen both the text and vision encoder backbones and do not retrain their weights at all. 2. For both encoders the final output is normalised to be of unit length.\n\nclass Projection(nn.Module):\n    def __init__(self, d_in: int, d_out: int, p: float=0.5) -&gt; None:\n        super().__init__()\n        self.linear1 = nn.Linear(d_in, d_out, bias=False)\n        self.linear2 = nn.Linear(d_out, d_out, bias=False)\n        self.layer_norm = nn.LayerNorm(d_out)\n        self.drop = nn.Dropout(p)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        embed1 = self.linear1(x)\n        embed2 = self.drop(self.linear2(F.gelu(embed1)))\n        embeds = self.layer_norm(embed1 + embed2)\n        return embeds\n\n\nclass VisionEncoder(nn.Module):\n    def __init__(self, d_out: int) -&gt; None:\n        super().__init__()\n        base = models.resnet34(pretrained=True)\n        d_in = base.fc.in_features\n        base.fc = nn.Identity()\n        self.base = base\n        self.projection = Projection(d_in, d_out)\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        projected_vec = self.projection(self.base(x))\n        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n        return projected_vec / projection_len\n\n\nclass TextEncoder(nn.Module):\n    def __init__(self, d_out: int) -&gt; None:\n        super().__init__()\n        self.base = AutoModel.from_pretrained(TEXT_MODEL)\n        self.projection = Projection(TRANSFORMER_EMBED_DIM, d_out)\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        out = self.base(**x)[0]\n        out = out[:, 0, :]  # get CLS token output\n        projected_vec = self.projection(out)\n        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n        return projected_vec / projection_len"
  },
  {
    "objectID": "blog/2021-03-07-clip.html#clip-loss-function",
    "href": "blog/2021-03-07-clip.html#clip-loss-function",
    "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
    "section": "CLIP loss function",
    "text": "CLIP loss function\nFor someone like me who hasn‚Äôt played around with contrastive loss, this was the most interesting part.\nWe know that we want the vectors of the corresponding image and the text to line up. Which means that the dot product has to be as close to one as possible. For everything else we need to push it towards 0.\nTherfore for a given caption, we take the softmax of the dot products across all images, and then take cross entropy loss. Similarly for a given image, we repeat the process across all captions. We average these two losses.\nIn terms of which element is the true positive within a batch, remember that we are sending image, caption pairs already lined up. Therefore we want all the diagonal elements to line up while all off-diagonal elements we want to push towards zero.\n\ndef contrastive_loss(logits, dim):\n    neg_ce = torch.diag(F.log_softmax(logits, dim=dim))\n    return -neg_ce.mean()\n    \ndef clip_loss(similarity: torch.Tensor) -&gt; torch.Tensor:\n    caption_loss = contrastive_loss(similarity, dim=0)\n    image_loss = contrastive_loss(similarity, dim=1)\n    return (caption_loss + image_loss) / 2.0\n\ndef metrics(similarity: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    y = torch.arange(len(similarity)).to(similarity.device)\n    img2cap_match_idx = similarity.argmax(dim=1)\n    cap2img_match_idx = similarity.argmax(dim=0)\n\n    img_acc = (img2cap_match_idx == y).float().mean()\n    cap_acc = (cap2img_match_idx == y).float().mean()\n\n    return img_acc, cap_acc"
  },
  {
    "objectID": "blog/2021-03-07-clip.html#model-1",
    "href": "blog/2021-03-07-clip.html#model-1",
    "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
    "section": "Model",
    "text": "Model\nIf you haven‚Äôt used pytorch lightning before, the benefit is that you do not need to stress about which device to put it in, remembering to zero the optimizer etc. All of that is taken care of. Just simply specify the training and validation steps, along with the optimizer and you are good to go.\nThe other benefit that I really like is logging. You just need to write self.log(\"name\", metric_to_track) and it will log to tensorboard by default, or any other kind of logger for that matter.\n\nclass Model(pl.LightningModule):\n    def __init__(self, \n                 lr: float = 1e-3\n        ) -&gt; None:\n        super().__init__()\n        self.vision_encoder = VisionEncoder(EMBED_DIM)\n        self.caption_encoder = TextEncoder(EMBED_DIM)\n        self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(TEXT_MODEL))\n        self.lr = lr\n       \n    def common_step(self, batch: Tuple[torch.Tensor, List[str]]) -&gt; torch.Tensor:\n        images, text = batch\n        text_dev = {k: v.to(self.device) for k, v in self.tokenizer(text).items()}\n\n        image_embed = self.vision_encoder(images)\n        caption_embed = self.caption_encoder(text)\n        similarity = caption_embed @ image_embed.T\n\n        loss = clip_loss(similarity)\n        img_acc, cap_acc = metrics(similarity)\n        return loss, img_acc, cap_acc\n\n    def training_step(\n        self, batch: Tuple[torch.Tensor, List[str]], *args: list\n    ) -&gt; torch.Tensor:\n        loss, img_acc, cap_acc = self.common_step(batch)     \n        self.log(\"training_loss\", loss, on_step=True)\n        self.log(\"training_img_acc\", img_acc, on_step=True, prog_bar=True)\n        self.log(\"training_cap_acc\", cap_acc, on_step=True, prog_bar=True)\n        return loss\n\n    def validation_step(\n        self, batch: Tuple[torch.Tensor, List[str]], *args: list\n    ) -&gt; torch.Tensor:\n        loss, img_acc, cap_acc = self.common_step(batch)\n        self.log(\"validation_loss\", loss, on_step=True)\n        self.log(\"validation_img_acc\", img_acc, on_step=True, prog_bar=True)\n        self.log(\"validation_cap_acc\", cap_acc, on_step=True, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        vision_params = {\"params\": self.vision_encoder.projection.parameters(), \"lr\": self.lr}\n        caption_params = {\"params\": self.caption_encoder.projection.parameters() , \"lr\": self.lr}\n        return torch.optim.Adam([vision_params, caption_params])"
  },
  {
    "objectID": "blog/2021-03-07-clip.html#train",
    "href": "blog/2021-03-07-clip.html#train",
    "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
    "section": "Train",
    "text": "Train\nTraining is straight forward as show in the five lines below. Using 16 bit precision almost halved the training time from 16 minutes to 9 minutes per epoch. Notice how easy it was to add half precision training and gradient clipping.\nAlso one thing to note is that I could not get this working on TPUs so if anyone knows what I need to adjust, please let me know. Setting tpu_cores=8 just did not work.\n\nmodel = Model(1e-3)\ntrainer = pl.Trainer(\n    max_epochs= 1,\n    gpus=torch.cuda.device_count(),\n    # tpu_cores=1,\n    gradient_clip_val=1.0,\n    precision=16\n)\ntrainer.fit(model, train_dl, valid_dl) # \n\nRun the following cell if you wish to see the logs in tensorboard. But here‚Äôs a screenshot I took: \n\n# Load the TensorBoard notebook extension\n%reload_ext tensorboard\n%tensorboard --logdir ./lightning_logs/"
  },
  {
    "objectID": "blog/2021-03-07-clip.html#results",
    "href": "blog/2021-03-07-clip.html#results",
    "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
    "section": "Results",
    "text": "Results\nI will compare the text embeddings of the first batch (in the validation set) to all the images of the validation set by taking the dot product between them.\n\nsimilarity = caption_embed @ image_embed.T\nval, closest = similarity.topk(5, dim=-1)\nsimilarity.shape\n\ntorch.Size([64, 16557])\n\n\ndraw_result(i, similarity_matrix) is a convenience function that takes the i-th caption and the similarity matrix, and plots the five closest images, along with the true image. The similarity between the caption and the image is shown in the title. The caption is printed first.\nThe histogram show the similarity of the caption to all images as a histogram.\n\ndraw_result(2, similarity)\n\nA baseball player in the outfield with his hands up, standing next to a team mascot.\n\n\n\n\n\n\n\n\n\ndraw_result(1, similarity)\n\nA watch and clock repair shop window with clocks on display.\n\n\n\n\n\n\n\n\n\ndraw_result(10, similarity)\n\nA person on a skateboard on the ground.\n\n\n\n\n\n\n\n\nBelow is the google translted version of one of the captions.\nEnglish caption: ‚ÄúA zebra standing up with it‚Äôs head down and eating grass on the dirt ground.‚Äù, tranlated into Spanish:\n\ntext = \"Una cebra de pie con la cabeza gacha y comiendo hierba en el suelo de tierra.\"\ntext_dev = {k: v.to(device) for k, v in tokenizer(text).items()}\nwith torch.no_grad():\n    caption_embed_text = caption_encoder(text_dev)\n\nsimilarity_text = caption_embed_text @ image_embed.T\n\n\ndraw_result_single_query(10, similarity_text)\n\n\n\n\nSkateboarder conducting a trick with bicycles in the background.\n\n\n\n\n\nAgain a translated version, this time to french. English caption: ‚ÄúA laptop is displayed on a small wooden platform.‚Äù\n\n# Guy and woman in glasses shake hands while exchanging gifts.\ntext = \"Un ordinateur portable est affich√© sur une petite plate-forme en bois.\"\ntext_dev = {k: v.to(device) for k, v in tokenizer(text).items()}\nwith torch.no_grad():\n    caption_embed_text = caption_encoder(text_dev)\n\nsimilarity_text = caption_embed_text @ image_embed.T\n\ndraw_result_single_query(3, similarity_text)\n\n\n\n\nLaptop computer on a small table on the side of a bed\n\n\n\n\n\nThe russian translation below is doing terrible though, so its clearly not bullet proof. Or perhaps I need to train for a bit longer. English caption: ‚ÄúA shop filled with different kinds of clocks.\n\ntext = \"–ú–∞–≥–∞–∑–∏–Ω —Å —Ä–∞–∑–Ω—ã–º–∏ —á–∞—Å–∞–º–∏\"\ntext_dev = {k: v.to(device) for k, v in tokenizer(text).items()}\nwith torch.no_grad():\n    caption_embed_text = caption_encoder(text_dev)\n\nsimilarity_text = caption_embed_text @ image_embed.T\n\ndraw_result_single_query(1, similarity_text)\n\n\n\n\nA room filled with clocks through a window.\n\n\n\n\n\nAnd lastly I check a single word version. Notice how the dog does kind of look like a bear. Maybe it‚Äôs name is bear?\n\ntext = \"bear\"\ntext_dev = {k: v.to(device) for k, v in tokenizer(text).items()}\nwith torch.no_grad():\n    caption_embed_text = caption_encoder(text_dev)\n\nsimilarity_text = caption_embed_text @ image_embed.T\n\ndraw_result_single_query(1, similarity_text)\n\n\n\n\nLarge collection of digital and analog clocks on display. \n\n\n\n\n\nWould love to hear any thoughts and comments on the above."
  },
  {
    "objectID": "blog/2021-03-07-clip.html#shameless-self-promotion",
    "href": "blog/2021-03-07-clip.html#shameless-self-promotion",
    "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nSee here for my course on Machine Learning and Deep Learning (Use code DEEPSCHOOL-MARCH to 85% off)."
  },
  {
    "objectID": "blog/2022-11-07-t5-for-grammar-correction.html#introduction",
    "href": "blog/2022-11-07-t5-for-grammar-correction.html#introduction",
    "title": "Fine Tuning T5 for Grammar Correction",
    "section": "Introduction",
    "text": "Introduction\nYou may have read my previous post on fine tuning GPT-2 for grammar correction. Well, I am here to tell you I have made a terrible mistake. While it was a fun exercise to understand the intricacies of GPT-2, I butchered it into correcting grammar. Let me explain why.\nFirstly, GPT-2 is a decoder only model. Meaning the current token can only attend previous tokens. While this is fine for our task by adding a seperator token, this also means that the decoder model needs to understand AND reconstruct the sentence. Therefore performing two tasks. T5 on the other has an encoder-decoder architecture. The encoder only contains an input task and the decoder only has a output/ generative task. Therefore dividing the responsibilities.\nT5 is also trained to as a multi task model which does things like summarization, translation etc. \nCode for this blog."
  },
  {
    "objectID": "blog/2022-11-07-t5-for-grammar-correction.html#data",
    "href": "blog/2022-11-07-t5-for-grammar-correction.html#data",
    "title": "Fine Tuning T5 for Grammar Correction",
    "section": "Data",
    "text": "Data\nSetting up the data is no different to what we did during GPT-2. We will still use the c4_200m dataset from huggingface datasets, and we will still try to match input to a corrected output, while also trying to teach the model when to leave it alone when it sees a good sentence."
  },
  {
    "objectID": "blog/2022-11-07-t5-for-grammar-correction.html#loss-function",
    "href": "blog/2022-11-07-t5-for-grammar-correction.html#loss-function",
    "title": "Fine Tuning T5 for Grammar Correction",
    "section": "Loss function",
    "text": "Loss function\nHowever, we now come to the first gotcha. The loss function. While in GPT-2 we can use outputs.loss we cannot do so here. That is because HF does not interally shift the tokens for a autoregressive task like generation. Instead, it is expecting a missing token prediction task by default. The following function simply shifts the labels 1 across, so that we can predict one token ahead. The loss function is your standard cross entropy loss.\nTo dive into this deeper, the model predicts what the next token ought to be. Therefore, the shape of the output is [batch_size, sequence_length, all_possible_tokens]. If you are wondering why it‚Äôs not [batch_size, sequence_length, hidden_dim_size], that‚Äôs because in this class of HF models (generative, specifically in this case, transformers.T5ForConditionalGeneration) have one more layer of shape [hidden_dim_size, all_possible_tokens] over which a softmax layer is used to get a probability over the tokens. This last layer is often simply the input embeddings transposed.\n\ndef calculate_loss_fn(loss_fn, logits: torch.Tensor, labels: torch.Tensor) -&gt; torch.Tensor:\n    shift_logits = logits[:, :-1, :].contiguous()\n    shift_labels = labels[:, 1:].contiguous()\n    return loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\nThe naive method to use the training data here would be to simply use the bad sentence as an input and the good sentence as the output. However, the model should also recognise when to leave a good sentence as is. Therefore, we calculate loss once for each scenario described and sum them up.\nThis brings us to the second gotcha. When I trained the first time around I simply used the tokenized sentence of the input/ output sentence. It fortunately trained well and masked a serious mathematical error. That was during inference, when we call the model.generate(...) function, it always starts off with a special token in the decoder. This could be accessed via model.config.decoder_start_token_id. Therefore, during training we need to prepend this token to the output sentence. We can see this in the LightningModule below.\n\n\nCode\nclass LightningModule(pl.LightningModule):\n    def __init__(\n        self,\n        model: nn.Module,\n        tokenizer: Tokenizer,\n        generation_kwargs: Dict[str, Any],\n        lr: float,\n        loss_fn: Callable = nn.CrossEntropyLoss(),\n    ) -&gt; None:\n        super().__init__()\n        self.model = model\n        self.tokenizer = tokenizer\n        self.lr = lr\n        self.generation_kwargs = generation_kwargs\n        self.loss_fn = loss_fn\n        self.prepend_sentence = PREPEND_SENTENCE\n        \n        decoder_start_token_id = model.config.decoder_start_token_id\n        self.prepend_input_ids = torch.LongTensor([decoder_start_token_id] * BATCH_SIZE)[:, None]\n        self.prepend_attention_masks = torch.LongTensor([1] * BATCH_SIZE)[:, None]\n        \n        self.model.train()\n        if IS_FREEZE_LAYERS:\n            for layer in self.model.encoder.block[:FREEZE_LAYERS]:\n                layer.eval()\n                for p in layer.parameters():\n                    p.requires_grad = False\n            for layer in self.model.decoder.block[:FREEZE_LAYERS]:\n                layer.eval()\n                for p in layer.parameters():\n                    p.requires_grad = False\n        \n        self.table_logging = 0\n        \n    def prepend_tokens(self, tokenized_batch: Dict[str, torch.LongTensor], len_batch: int) -&gt; Dict[str, torch.LongTensor]:\n        input_ids = torch.cat(\n            [\n                self.prepend_input_ids[:len_batch, :].to(self.device),\n                tokenized_batch[\"input_ids\"],\n            ],\n            dim=-1,\n        )\n        attention_mask = torch.cat(\n            [\n                self.prepend_attention_masks[:len_batch, :].to(self.device),\n                tokenized_batch[\"attention_mask\"],\n            ],\n            dim=-1,\n        )\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n        }\n        \n    def get_loss(self, input_sentences: List[str], output_sentences: List[str]) -&gt; torch.FloatTensor:\n        tokenized_input = self.tokenizer(\n            [self.prepend_sentence + sentence for sentence in input_sentences], \n            self.device\n        )\n        tokenized_output = self.prepend_tokens(self.tokenizer(output_sentences, self.device), len(output_sentences))\n        labels = tokenized_output[\"input_ids\"].clone()\n        labels[tokenized_output[\"attention_mask\"] == 0] == LABEL_MASK\n        \n        out = self.model(\n            input_ids=tokenized_input[\"input_ids\"],\n            attention_mask=tokenized_input[\"attention_mask\"],\n            decoder_input_ids=tokenized_output[\"input_ids\"],\n            decoder_attention_mask=tokenized_output[\"attention_mask\"],\n        )\n        return calculate_loss_fn(self.loss_fn, out.logits, labels)\n        \n        \n    def common_step(self, batch: Dict[str, str]) -&gt; torch.Tensor:\n        bad_grammar_loss = self.get_loss(batch[\"input\"], batch[\"output\"])\n        good_grammar_loss = self.get_loss(batch[\"output\"], batch[\"output\"])\n    \n        return good_grammar_loss + bad_grammar_loss\n        \n    def training_step(\n        self, batch: Dict[str, torch.LongTensor], batch_idx: int,\n    ) -&gt; torch.Tensor:\n        loss = self.common_step(batch)     \n        self.log(\"training_loss\", loss, on_step=True, on_epoch=True, batch_size=len(batch[\"input\"]))\n             \n        return loss\n\n    def validation_step(\n        self, batch: Tuple[torch.Tensor, List[str]], batch_idx: int,\n    ) -&gt; torch.Tensor:\n        loss = self.common_step(batch)\n        self.log(\"validation_loss\", loss, on_step=False, on_epoch=True, batch_size=len(batch[\"input\"]))\n        \n        if batch_idx == 0:\n            self.log_examples(batch)\n            \n    def log_examples(self, batch):\n        good_grammar_batch = self.tokenizer(batch[\"output\"], device=self.device)\n        bad_grammar_batch = self.tokenizer(batch[\"input\"], device=self.device)\n        encoded_good_outputs = self.model.generate(**good_grammar_batch, **self.generation_kwargs)\n        encoded_bad_outputs = self.model.generate(**bad_grammar_batch, **self.generation_kwargs)\n        generated_good_sentences = self.tokenizer.batch_decode(encoded_good_outputs)\n        generated_bad_sentences = self.tokenizer.batch_decode(encoded_bad_outputs)\n        \n        columns = [\"good input\", \"good output\", \"bad input\", \"bad output\"]\n        data = [\n            [good_input, good_output, bad_input, bad_output]\n            for good_input, good_output, bad_input, bad_output in zip(\n                batch[\"output\"], generated_good_sentences, batch[\"input\"], generated_bad_sentences\n            )\n        ]\n        table = wandb.Table(data=data, columns=columns)\n        if self.logger is not None:\n            self.table_logging += 1\n            self.logger.experiment.log({f\"epoch {self.table_logging} results\": table})\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        if IS_FREEZE_LAYERS:\n            return adam.FusedAdam(\n                [\n                    {\"params\": layer.parameters(), \"lr\": self.lr} for layer in language_model.encoder.block[FREEZE_LAYERS:]\n                ] + \\\n                [\n                    {\"params\": layer.parameters() , \"lr\": self.lr} for layer in language_model.decoder.block[FREEZE_LAYERS:]\n                ]\n            )\n        else:\n            return adam.FusedAdam(self.model.parameters(), self.lr)"
  },
  {
    "objectID": "blog/2022-11-07-t5-for-grammar-correction.html#training-the-model",
    "href": "blog/2022-11-07-t5-for-grammar-correction.html#training-the-model",
    "title": "Fine Tuning T5 for Grammar Correction",
    "section": "Training the model",
    "text": "Training the model\nThe last gotcha comes from during the training, it seems using 16 bit training is unstable. Therefore, I was forced to use 32 bit with a smaller batch size, but this can be remedied by increasing the accumulate_grad_batches.\nAlso as a side note, I do like to freeze certain layers, a trick I picked up in fast.ai. This is done in order to not overfit my training data. Conceptually, it makes sense not to train embeddings since some words will be seen (and therefore updated) more often than other.\n\n\nCode\nadam.FusedAdam(\n    [\n        {\"params\": layer.parameters(), \"lr\": self.lr} for layer in language_model.encoder.block[FREEZE_LAYERS:]\n    ] + \\\n    [\n        {\"params\": layer.parameters() , \"lr\": self.lr} for layer in language_model.decoder.block[FREEZE_LAYERS:]\n    ]\n)"
  },
  {
    "objectID": "blog/2022-11-07-t5-for-grammar-correction.html#results",
    "href": "blog/2022-11-07-t5-for-grammar-correction.html#results",
    "title": "Fine Tuning T5 for Grammar Correction",
    "section": "Results",
    "text": "Results\nAccoring to the limited experiments that I ran, it seems T5 does better than Flan-T5. Keep in mind that this was a drop in replacement during training. In the following examples it seems to be doing a decent job of leaving some text as is while redoing others. \nHowever, let me add that Flan-T5 is magical! If you read the paper it is supposed to generalise to unseen tasks. The original T5 paper was only tasked with 5 tasks, whereas there were many tasks than Flan-T5 was trained with the same footprint in terms of number of weights. As you can see in the following, simply by adding Correct grammar in following sentence: I was able to get a corrected sentence in Flan-T5-large model."
  },
  {
    "objectID": "blog/2022-11-07-t5-for-grammar-correction.html#summary",
    "href": "blog/2022-11-07-t5-for-grammar-correction.html#summary",
    "title": "Fine Tuning T5 for Grammar Correction",
    "section": "Summary",
    "text": "Summary\nIn closing the two main take away points are: 1. Redo the loss calculation. 2. Change to 32 bit training/ try bfloat16.\nIf you can afford to pay for training it is worth trying to train Flan-T5 for longer and see where it gets to. My wandb logs can be seen here and the kaggle kernel can be found here."
  },
  {
    "objectID": "blog/2022-11-07-t5-for-grammar-correction.html#shameless-self-promotion",
    "href": "blog/2022-11-07-t5-for-grammar-correction.html#shameless-self-promotion",
    "title": "Fine Tuning T5 for Grammar Correction",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy my course (30 days moneyback)."
  },
  {
    "objectID": "blog/2022-03-20-kmeans.html",
    "href": "blog/2022-03-20-kmeans.html",
    "title": "KMeans in PyTorch with Cosine Distanceü•ßüî¶",
    "section": "",
    "text": "Kmeans is one of the easiest and fastest clustering algorithms. Here we tweak the algorithm to cluster vectors with unit length."
  },
  {
    "objectID": "blog/2022-03-20-kmeans.html#introduction",
    "href": "blog/2022-03-20-kmeans.html#introduction",
    "title": "KMeans in PyTorch with Cosine Distanceü•ßüî¶",
    "section": "",
    "text": "Kmeans is one of the easiest and fastest clustering algorithms. Here we tweak the algorithm to cluster vectors with unit length."
  },
  {
    "objectID": "blog/2022-03-20-kmeans.html#data",
    "href": "blog/2022-03-20-kmeans.html#data",
    "title": "KMeans in PyTorch with Cosine Distanceü•ßüî¶",
    "section": "Data",
    "text": "Data\nWe randomly generate a million data points with 768 dimensions (usual size in transformer embeddings). And then we normalize all those data points to unit length.\n\nN = 1000000\nD = 768\nx = F.normalize(torch.randn(N, D), dim=-1)"
  },
  {
    "objectID": "blog/2022-03-20-kmeans.html#algorithm",
    "href": "blog/2022-03-20-kmeans.html#algorithm",
    "title": "KMeans in PyTorch with Cosine Distanceü•ßüî¶",
    "section": "Algorithm",
    "text": "Algorithm\nThe following shows our kmeans implementation. The steps are as follows: 1. Choose n_clusters points from our dataset randomly and set them as our initial centroids. 2. Iterate through all datapoints and assign each point to one of the centroids. 3. Recalculate centroids based by averaging datapoints assigned to each cluster. As an additional step to usual kmeans, normalize to unit length. 4. Repeat from step 2, for epochs iterations.\nIn order to preserve some memory usage we calculate similarity batch-wise and store simply the largest index (argmax).\n\nclass KMeans:\n    def __init__(self, n_clusters, batch_size, epochs):\n        self.n_clusters = n_clusters\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.centers = None\n        self.similarities = []\n        \n    def fit(self, x):\n        if self.centers is None:\n            idx = np.random.choice(len(x), self.n_clusters, replace=False)\n            centers = x[idx]\n        else:\n            centers = self.centers\n        \n        for _ in range(self.epochs):\n            batch_cluster = []\n            similarity_mean = 0\n            for i in tqdm(range(0, len(x), self.batch_size)):\n                similarity, idx = (x[i:i+self.batch_size] @ centers.T).max(dim=-1)\n                similarity_mean += similarity.sum()\n                batch_cluster.append(idx)\n            x_assigned_cluster = torch.cat(batch_cluster)    \n            centers = torch.stack(\n                [\n                    F.normalize(x[x_assigned_cluster==i].mean(dim=0), dim=-1)\n                    for i in range(self.n_clusters)\n                ]\n            )\n            self.similarities.append(similarity_mean / len(x))\n        \n        self.centers = centers\n                \n\n\nkmeans = KMeans(1000, 64, 10)\nkmeans.fit(x)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can see the similarities converging below:\n\nplt.plot(kmeans.similarities)"
  },
  {
    "objectID": "blog/2022-03-20-kmeans.html#shameless-self-promotion",
    "href": "blog/2022-03-20-kmeans.html#shameless-self-promotion",
    "title": "KMeans in PyTorch with Cosine Distanceü•ßüî¶",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2021-08-21-coco-semantic-segmentation-data.html",
    "href": "blog/2021-08-21-coco-semantic-segmentation-data.html",
    "title": "Coco Semantic Segmentation in PyTorch - Data Prep",
    "section": "",
    "text": "This post describes how to use the coco dataset for semantic segmentation. Kudos to this blog for giving me the necessary hints to create this.\n\ntrain_annotations = COCO(ROOT_PATH / \"annotations/instances_train2017.json\")\nvalid_annotations = COCO(ROOT_PATH / \"annotations/instances_val2017.json\")\n\ncat_ids = train_annotations.getCatIds(supNms=[\"person\", \"vehicle\"])\ntrain_img_ids = []\nfor cat in cat_ids:\n    train_img_ids.extend(train_annotations.getImgIds(catIds=cat))\n    \ntrain_img_ids = list(set(train_img_ids))\nprint(f\"Number of training images: {len(train_img_ids)}\")\n\nvalid_img_ids = []\nfor cat in cat_ids:\n    valid_img_ids.extend(valid_annotations.getImgIds(catIds=cat))\n    \nvalid_img_ids = list(set(valid_img_ids))\nprint(f\"Number of validation images: {len(valid_img_ids)}\")\n\nNumber of training images: 74152\nNumber of validation images: 3125\n\n\n\nclass ImageData(Dataset):\n    def __init__(\n        self, \n        annotations: COCO, \n        img_ids: List[int], \n        cat_ids: List[int], \n        root_path: Path, \n        transform: Optional[Callable]=None\n    ) -&gt; None:\n        super().__init__()\n        self.annotations = annotations\n        self.img_data = annotations.loadImgs(img_ids)\n        self.cat_ids = cat_ids\n        self.files = [str(root_path / img[\"file_name\"]) for img in self.img_data]\n        self.transform = transform\n        \n    def __len__(self) -&gt; int:\n        return len(self.files)\n    \n    def __getitem__(self, i: int) -&gt; Tuple[torch.Tensor, torch.LongTensor]:\n        ann_ids = self.annotations.getAnnIds(\n            imgIds=self.img_data[i]['id'], \n            catIds=self.cat_ids, \n            iscrowd=None\n        )\n        anns = self.annotations.loadAnns(ann_ids)\n        mask = torch.LongTensor(np.max(np.stack([self.annotations.annToMask(ann) * ann[\"category_id\"] \n                                                 for ann in anns]), axis=0)).unsqueeze(0)\n        \n        img = io.read_image(self.files[i])\n        if img.shape[0] == 1:\n            img = torch.cat([img]*3)\n        \n        if self.transform is not None:\n            return self.transform(img, mask)\n        \n        return img, mask"
  },
  {
    "objectID": "blog/2021-08-21-coco-semantic-segmentation-data.html#introduction",
    "href": "blog/2021-08-21-coco-semantic-segmentation-data.html#introduction",
    "title": "Coco Semantic Segmentation in PyTorch - Data Prep",
    "section": "",
    "text": "This post describes how to use the coco dataset for semantic segmentation. Kudos to this blog for giving me the necessary hints to create this.\n\ntrain_annotations = COCO(ROOT_PATH / \"annotations/instances_train2017.json\")\nvalid_annotations = COCO(ROOT_PATH / \"annotations/instances_val2017.json\")\n\ncat_ids = train_annotations.getCatIds(supNms=[\"person\", \"vehicle\"])\ntrain_img_ids = []\nfor cat in cat_ids:\n    train_img_ids.extend(train_annotations.getImgIds(catIds=cat))\n    \ntrain_img_ids = list(set(train_img_ids))\nprint(f\"Number of training images: {len(train_img_ids)}\")\n\nvalid_img_ids = []\nfor cat in cat_ids:\n    valid_img_ids.extend(valid_annotations.getImgIds(catIds=cat))\n    \nvalid_img_ids = list(set(valid_img_ids))\nprint(f\"Number of validation images: {len(valid_img_ids)}\")\n\nNumber of training images: 74152\nNumber of validation images: 3125\n\n\n\nclass ImageData(Dataset):\n    def __init__(\n        self, \n        annotations: COCO, \n        img_ids: List[int], \n        cat_ids: List[int], \n        root_path: Path, \n        transform: Optional[Callable]=None\n    ) -&gt; None:\n        super().__init__()\n        self.annotations = annotations\n        self.img_data = annotations.loadImgs(img_ids)\n        self.cat_ids = cat_ids\n        self.files = [str(root_path / img[\"file_name\"]) for img in self.img_data]\n        self.transform = transform\n        \n    def __len__(self) -&gt; int:\n        return len(self.files)\n    \n    def __getitem__(self, i: int) -&gt; Tuple[torch.Tensor, torch.LongTensor]:\n        ann_ids = self.annotations.getAnnIds(\n            imgIds=self.img_data[i]['id'], \n            catIds=self.cat_ids, \n            iscrowd=None\n        )\n        anns = self.annotations.loadAnns(ann_ids)\n        mask = torch.LongTensor(np.max(np.stack([self.annotations.annToMask(ann) * ann[\"category_id\"] \n                                                 for ann in anns]), axis=0)).unsqueeze(0)\n        \n        img = io.read_image(self.files[i])\n        if img.shape[0] == 1:\n            img = torch.cat([img]*3)\n        \n        if self.transform is not None:\n            return self.transform(img, mask)\n        \n        return img, mask"
  },
  {
    "objectID": "blog/2021-08-21-coco-semantic-segmentation-data.html#image-augmentations",
    "href": "blog/2021-08-21-coco-semantic-segmentation-data.html#image-augmentations",
    "title": "Coco Semantic Segmentation in PyTorch - Data Prep",
    "section": "Image Augmentations",
    "text": "Image Augmentations\nWhen using augmentations we need to be careful to apply the same transformation to image and the mask. So for example when doing a random crop as below, we need to make it somewhat deterministic. The way to do that in torch is by getting the transformation parameters and then using torchvision.transforms.functional which are deterministic transformations.\n\ndef train_transform(\n    img1: torch.LongTensor, \n    img2: torch.LongTensor\n) -&gt; Tuple[torch.LongTensor, torch.LongTensor]:\n    params = transforms.RandomResizedCrop.get_params(img1, scale=(0.5, 1.0), ratio=(0.75, 1.33))\n    \n    img1 = TF.resized_crop(img1, *params, size=IMAGE_SIZE)\n    img2 = TF.resized_crop(img2, *params, size=IMAGE_SIZE)\n    \n    # Random horizontal flipping\n    if random.random() &gt; 0.5:\n        img1 = TF.hflip(img1)\n        img2 = TF.hflip(img2)\n        \n    return img1, img2\n\n\ntrain_data = ImageData(train_annotations, train_img_ids, cat_ids, ROOT_PATH / \"train2017\", train_transform)\nvalid_data = ImageData(valid_annotations, valid_img_ids, cat_ids, ROOT_PATH / \"val2017\", train_transform)\n\ntrain_dl = DataLoader(\n    train_data,\n    BATCH_SIZE, \n    shuffle=True, \n    drop_last=True, \n    num_workers=4,\n    pin_memory=True,\n)\n\nvalid_dl = DataLoader(\n    valid_data,\n    BATCH_SIZE, \n    shuffle=False, \n    drop_last=False, \n    num_workers=4,\n    pin_memory=True,\n)\n\nThe following demos a single output.\n\nimg, mask = train_data[22]\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.imshow(TF.to_pil_image(img))\nplt.subplot(122)\nplt.imshow(mask.squeeze())\nplt.show()\n\n\n\n\nRun the following commented out section to see how long data loading takes. It takes approximately 10 minutes to run through an epoch without any modelling.\n\nx, y = next(iter(train_dl))\nprint(x.shape, y.shape)\n# for x, y in tqdm(train_dl):\n#     continue\n\ntorch.Size([64, 3, 128, 128]) torch.Size([64, 1, 128, 128])"
  },
  {
    "objectID": "blog/2015-10-09-Chinese-Restaurant-Process.html",
    "href": "blog/2015-10-09-Chinese-Restaurant-Process.html",
    "title": "Chinese Restuarant Process",
    "section": "",
    "text": "In this instance we generate the parameters \\[\\theta_k\\] from \\[\\mathcal{N}(\\mathbf{0},3\\mathbf{I})\\]. The data is generated from \\[\\mathcal{N}(\\theta_k,0.1\\mathbf{I})\\]. Where \\[k\\] is the table. Table allocation is the main part of the CRP which is determined by: \\[\\begin{align}\nk=\\begin{cases}\n\\text{new table } & \\text{with prob = } \\frac{\\alpha}{\\alpha+n-1}\\\\\n\\text{table k } & \\text{with prob = } \\frac{n_k}{\\alpha+n-1}\n\\end{cases}\n\\end{align}\n\\] where \\[n_k\\] is the number of customers at table \\[k\\].\nThe associated ipython notebook is located here."
  },
  {
    "objectID": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html",
    "href": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers",
    "section": "",
    "text": "There has been a lot of hype about generating images from text. However, I had not seen many things in the caption generation domain. This is obviously the easier of the two problems, and perhaps it has been mostly solved, but I thought I‚Äôd get my hands dirty trying to do this almost from scratch. Before we get going HF does have VisionEncoderDecoderModels which does exactly what we are doing today, but I wanted to try and build this from mostly scratch.\nVisual Transformers was used to classify images in the Imagenet problem and GPT2 is a language model than can be used to generate text. So the question is can we combine these two? And the answer is yes, thanks to EncoderDecoderModels from HF. In the original Attention Is All You Need paper, using attention was the game changer. Not many people are aware however, that there were two kinds of attention. 1. Self-attention which most people are familiar with, 2. Cross-attention which allows the decoder to retrieve information from the encoder. \nBy default GPT-2 does not have this cross attention layer pre-trained. This paper by Google Research demonstrated that you can simply randomly initialise these cross attention layers and train the system. And this is exactly what we will be doing in this blog using the COCO dataset. An executable version of this can be found here on kaggle."
  },
  {
    "objectID": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#introduction",
    "href": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#introduction",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers",
    "section": "",
    "text": "There has been a lot of hype about generating images from text. However, I had not seen many things in the caption generation domain. This is obviously the easier of the two problems, and perhaps it has been mostly solved, but I thought I‚Äôd get my hands dirty trying to do this almost from scratch. Before we get going HF does have VisionEncoderDecoderModels which does exactly what we are doing today, but I wanted to try and build this from mostly scratch.\nVisual Transformers was used to classify images in the Imagenet problem and GPT2 is a language model than can be used to generate text. So the question is can we combine these two? And the answer is yes, thanks to EncoderDecoderModels from HF. In the original Attention Is All You Need paper, using attention was the game changer. Not many people are aware however, that there were two kinds of attention. 1. Self-attention which most people are familiar with, 2. Cross-attention which allows the decoder to retrieve information from the encoder. \nBy default GPT-2 does not have this cross attention layer pre-trained. This paper by Google Research demonstrated that you can simply randomly initialise these cross attention layers and train the system. And this is exactly what we will be doing in this blog using the COCO dataset. An executable version of this can be found here on kaggle."
  },
  {
    "objectID": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#data",
    "href": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#data",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers",
    "section": "Data",
    "text": "Data\nThe coco dataset provides us with an image and 5 possible captions. We choose one at random during each epoch.\n\nprint(caption)\ntransforms.ToPILImage()(descale(img))\n\nA lone zebra grazing in some green grass."
  },
  {
    "objectID": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#gpt2-tokenizer-and-model",
    "href": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#gpt2-tokenizer-and-model",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers",
    "section": "GPT2 Tokenizer and Model",
    "text": "GPT2 Tokenizer and Model\nAs mentioned earlier, we will use the EncoderDecoderModel which will initialize the cross attention layers for us, and use pretrained weights from the Visual Transformer and (distil) GPT2. We only use the distil version for the sake of quick training, and as you will see soon, is good enough.\nThe tokenizer requires a bit more preprocessing than what you‚Äôd be used to compared to a BERT tokenizer. The following tokenizer code is something I copied (sorry don‚Äôt remember where), but the important bit is that a padding token was required to be introduced which I thought was strange. Mostly because how would GPT-2 have been trained without padding?\n\n# model\nvit2gpt2 = EncoderDecoderModel.from_encoder_decoder_pretrained(VIT_MODEL, DISTIL_GPT2)\n\n# tokenizer\n# make sure GPT2 appends EOS in begin and end\ndef build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n    return outputs\n    \nGPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\ngpt2_tokenizer = GPT2Tokenizer.from_pretrained(DISTIL_GPT2)\n# set pad_token_id to unk_token_id -&gt; be careful here as unk_token_id == eos_token_id == bos_token_id\ngpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token"
  },
  {
    "objectID": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#nucleus-sampling",
    "href": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#nucleus-sampling",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers",
    "section": "Nucleus Sampling",
    "text": "Nucleus Sampling\nAt the time of writing it seems that EncoderDecoderModel does not seem to have a generate method which is used by GPT-2 etc. to generate text. Hence the following code.\nSampling the next token/ word is not simply a matter of taking the highest likelihood of the next token. This is due to the fact that there is no guarantee that the (log) likelihood of the entire sequence is maximised by taking the maximum at each token. This will lead to a sub-optimal answer. Beam search is an alternate method where you keep the top k tokens and iterate to the end, and hopefully one of the k beams will contain the solution we are after.\nIn the code below we use a sampling based method named Nucleus Sampling which is shown to have superior results and minimises common pitfalls such as repetition when generating text. The algorithm is as follows:\n\nChoose all largest tokens that sums up to a given threshold.\nSet all other token probabilities to zero, and renormalize probability distribution.\nSample from above distribution.\nIn the code below, apart from a threshold on top probable tokens, we also have a limit on possible tokens which is defaulted to a large number (1000).\n\n\ndef top_k_top_p_filtering(\n    next_token_logits: torch.FloatTensor,\n    top_k: Optional[float]=None, \n    top_p: Optional[float]=None,\n    device: Union[str, torch.device]=\"cpu\",\n) -&gt; torch.FloatTensor:\n    if top_k is None:\n        top_k = next_token_logits.shape[-1]\n    if top_p is None:\n        top_p = 1.0\n        \n    p, largest_p_idx = F.softmax(next_token_logits, dim=-1).topk(top_k, dim=-1)\n    cumulative_p = p.cumsum(dim=-1)\n    threshold_repeated = top_p + torch.zeros((len(p),1)).to(device)\n    idx = torch.searchsorted(cumulative_p, threshold_repeated).clip(max=top_k-1).squeeze()\n    cutoffs = cumulative_p[torch.arange(len(cumulative_p)), idx]\n    censored_p = (cumulative_p &lt;= cutoffs[:, None]) * p\n    renormalized_p = censored_p / censored_p.sum(dim=-1, keepdims=True)\n    \n    final_p = torch.zeros_like(next_token_logits)\n    row_idx = torch.arange(len(p)).unsqueeze(1).repeat(1,top_k).to(device)\n    final_p[row_idx, largest_p_idx] = renormalized_p.to(final_p.dtype)\n\n    return final_p\n\nIn order to generate the actual sequence we need 1. The image representation according to the encoder (ViT) and 2. The generated tokens so far. Note that the first token is always going to be a beginning of sentence token (&lt;BOS&gt;). We pass the generated tokens iteratively for a predefined length or until end of sentence is reached. In the following since we are using a batch, we ignore the &lt;EOS&gt; token.\n\ndef generate_sentence_from_image(model, encoder_outputs, tokenizer, max_text_length: int, device)-&gt; List[str]:\n    generated_so_far = torch.LongTensor([[tokenizer.bos_token_id]]*len(encoder_outputs.last_hidden_state)).to(device)\n    with torch.no_grad():\n        for _ in tqdm(range(max_text_length)):\n            attention_mask = torch.ones_like(generated_so_far)\n            decoder_out = model(\n                decoder_input_ids=generated_so_far, \n                decoder_attention_mask=attention_mask,\n                encoder_outputs=encoder_outputs\n            )\n\n            next_token_logits = decoder_out[\"logits\"][:, -1, :]\n            filtered_p = top_k_top_p_filtering(next_token_logits, top_k=TOP_K, top_p=TOP_P, device=device)\n            next_token = torch.multinomial(filtered_p, num_samples=1)\n            generated_so_far = torch.cat((generated_so_far, next_token), dim=1)\n\n    return [tokenizer.decode(coded_sentence) for coded_sentence in generated_so_far]"
  },
  {
    "objectID": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#training-module-pytorch-lightning",
    "href": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#training-module-pytorch-lightning",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers",
    "section": "Training Module (PyTorch Lightning)",
    "text": "Training Module (PyTorch Lightning)\nExpand the button below to see the pytorch lightning code. There are a few things to note in the training step.\n\nTrain only the cross-attention weights. This was a design decision based on time available, and not a necessity. This however meant that I was able to train an epoch of 110000 image caption pairs in 30 minutes.\n\nfor name, param in self.model.named_parameters():\n    if \"crossattention\" not in name:\n        param.requires_grad = False\n\nLoss was calculated for you by HF. However, if you wish to understand how exactly HF calculates loss, that can be found in this discussion that I authored.\n\nencoder_outputs = self.model.encoder(pixel_values=images)\noutputs = self.model(\n    encoder_outputs=encoder_outputs,\n    decoder_input_ids=tokenized_captions[\"input_ids\"],\n    decoder_attention_mask=tokenized_captions[\"attention_mask\"],\n    labels=labels,\n    return_dict=True,\n)\n\nreturn outputs[\"loss\"]\n\nWeights and Biases are amazing and you should log all your experiments. The wandb.Table element especially was a godsend.\n\n\n\nCode\nclass LightningModule(pl.LightningModule):\n    def __init__(\n        self,\n        model: nn.Module,\n        tokenizer,\n        lr: float,\n    ):\n        super().__init__()\n        self.model = model\n        self.tokenizer = tokenizer\n        self.lr = lr\n        \n        for name, param in self.model.named_parameters():\n            if \"crossattention\" not in name:\n                param.requires_grad = False\n        \n    def common_step(self, batch: Tuple[torch.FloatTensor, List[str]]) -&gt; torch.FloatTensor:\n        images, captions = batch\n        tokenized_captions = {\n            k: v.to(self.device) for k, v in \n            self.tokenizer(\n                captions,\n                max_length=MAX_TEXT_LENGTH,\n                truncation=True,\n                padding=True,\n                return_tensors=\"pt\",\n            ).items()\n        }\n        labels = tokenized_captions[\"input_ids\"].clone()\n        labels[tokenized_captions[\"attention_mask\"]==0] = LABEL_MASK\n        encoder_outputs = self.model.encoder(pixel_values=images)\n        outputs = self.model(\n            encoder_outputs=encoder_outputs,\n            decoder_input_ids=tokenized_captions[\"input_ids\"],\n            decoder_attention_mask=tokenized_captions[\"attention_mask\"],\n            labels=labels,\n            return_dict=True,\n        )\n        \n        return outputs[\"loss\"]\n    \n    def training_step(self, batch: Tuple[torch.FloatTensor, List[str]], batch_idx: int) -&gt; torch.FloatTensor:\n        loss = self.common_step(batch)\n        self.log(name=\"Training loss\", value=loss, on_step=True, on_epoch=True)\n        \n        return loss\n        \n    def validation_step(self, batch: Tuple[torch.FloatTensor, List[str]], batch_idx: int):\n        loss = self.common_step(batch)\n        self.log(name=\"Validation loss\", value=loss, on_step=True, on_epoch=True)\n\n        images, actual_sentences = batch\n        \n        if batch_idx == 0:\n            encoder_outputs = self.model.encoder(pixel_values=images.to(self.device))\n            generated_sentences = generate_sentence_from_image(\n                self.model, \n                encoder_outputs, \n                self.tokenizer, \n                MAX_TEXT_LENGTH,\n                self.device\n            )\n            images = [wandb.Image(transforms.ToPILImage()(descale(image))) for image in images]\n            data = list(map(list, zip(images, actual_sentences, generated_sentences)))\n            columns = [\"Images\", \"Actual Sentence\", \"Generated Sentence\"]\n            table = wandb.Table(data=data, columns=columns)\n            self.logger.experiment.log({f\"epoch {self.current_epoch} results\": table})\n                        \n    def on_after_backward(self):\n        if self.trainer.global_step % 50 == 0:  # don't make the tf file huge\n            for name, param in self.model.named_parameters():\n                if \"weight\" in name and not \"norm\" in name and param.requires_grad:\n                    self.logger.experiment.log(\n                        {f\"{name}_grad\": wandb.Histogram(param.grad.detach().cpu())}\n                    )\n                    self.logger.experiment.log(\n                        {f\"{name}\": wandb.Histogram(param.detach().cpu())}\n                    )\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"
  },
  {
    "objectID": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#results",
    "href": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#results",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers",
    "section": "Results",
    "text": "Results\nThe full set of results can be seen here but here‚Äôs some highlights:  Actual caption: A teddy bear and a metronome sitting on a table top. Generated caption: &lt;|endoftext|&gt;A stuffed teddy bear is being laid down next to a table.&lt;|endoftext|&gt;  Actual caption: A white cat is sitting on a laptop. Generated caption: &lt;|endoftext|&gt;A cat sitting on a stool with an empty laptop.&lt;|endoftext|&gt;  Actual caption: The cows are laying down in a straight row. Generated caption: &lt;|endoftext|&gt;A horse is drinking milk and taking pictures of cattle inside.&lt;|endoftext|&gt;\nAtleast the last one captured cattle, not sure about the shifty horse ü§î."
  },
  {
    "objectID": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#gotchas-and-potential-improvements",
    "href": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#gotchas-and-potential-improvements",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers",
    "section": "Gotchas and Potential Improvements",
    "text": "Gotchas and Potential Improvements\nThe biggest problem I had training this model was gradient explosion. When I chose the learning rate too high (1e-3) the weights quickly hit infinity, too small 1e-5 and the generated text wasn‚Äôt so good. So it was possibly worth unfreezing the weights after one epoch and training with a smaller learning rate. The following is a cross section of the histograms of one set of weights. As you can see it goes into the 1000s even though I managed to train it succesfully which is concerning: \nPerhaps putting some sort of normalization would have fixed the gradient explosion, but that is something unfortunately I don‚Äôt have access to as this is done for me by HF EncoderDecoderModel API."
  },
  {
    "objectID": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#shameless-self-promotion",
    "href": "blog/2021-12-28-vit-to-gpt2-encoder-decoder-model.html#shameless-self-promotion",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2017-08-24-Docker-for-Data-Science.html",
    "href": "blog/2017-08-24-Docker-for-Data-Science.html",
    "title": "Docker for Data Science",
    "section": "",
    "text": "Docker is a tool that simplifies the installation process for software engineers. Coming from a statistics background I used to care very little about how to install software and would occasionally spend a few days trying to resolve system configuration issues. Enter the god-send Docker almighty.\nThink of Docker as a light virtual machine (I apologise to the Docker gurus for using that term). Its underlying philosophy is that if it works on my machine it will work on yours.\n\n\n\nTime: The amount of time that you save on not installing packages in itself makes this framework worth it.\nReproducible Research: I think of Docker as akin to setting the seed in a report. This makes sure that the analysis that you are generating will run on any other analysts machine.\n\n\n\n\nDocker employs the concept of (reusable) layers. So whatever line that you write inside the Dockerfile is considered a layer. For example you would usually start with:\nFROM ubuntu\nRUN apt-get install python3\nThis Dockerfile would install python3 (as a layer) on top of the Ubuntu layer.\nWhat you essentially do is for each project you write all the apt-get install, pip install etc. commands into your Dockerfile instead of executing it locally.\nI recommend reading the tutorial on https://docs.docker.com/get-started/ to get started on Docker. The learning curve is minimal (2 days work at most) and the gains are enormous.\n\n\n\nLastly Dockerhub deserves a special mention. Personally Dockerhub is what makes Docker truly powerful. It‚Äôs what github is to git, a open platform to share your Docker images.\nMy Docker image for Machine Learning and data science is availale here: https://hub.docker.com/r/sachinruk/ml_class/"
  },
  {
    "objectID": "blog/2017-08-24-Docker-for-Data-Science.html#whats-in-it-for-data-scientists",
    "href": "blog/2017-08-24-Docker-for-Data-Science.html#whats-in-it-for-data-scientists",
    "title": "Docker for Data Science",
    "section": "",
    "text": "Time: The amount of time that you save on not installing packages in itself makes this framework worth it.\nReproducible Research: I think of Docker as akin to setting the seed in a report. This makes sure that the analysis that you are generating will run on any other analysts machine."
  },
  {
    "objectID": "blog/2017-08-24-Docker-for-Data-Science.html#how-does-it-work",
    "href": "blog/2017-08-24-Docker-for-Data-Science.html#how-does-it-work",
    "title": "Docker for Data Science",
    "section": "",
    "text": "Docker employs the concept of (reusable) layers. So whatever line that you write inside the Dockerfile is considered a layer. For example you would usually start with:\nFROM ubuntu\nRUN apt-get install python3\nThis Dockerfile would install python3 (as a layer) on top of the Ubuntu layer.\nWhat you essentially do is for each project you write all the apt-get install, pip install etc. commands into your Dockerfile instead of executing it locally.\nI recommend reading the tutorial on https://docs.docker.com/get-started/ to get started on Docker. The learning curve is minimal (2 days work at most) and the gains are enormous."
  },
  {
    "objectID": "blog/2017-08-24-Docker-for-Data-Science.html#dockerhub",
    "href": "blog/2017-08-24-Docker-for-Data-Science.html#dockerhub",
    "title": "Docker for Data Science",
    "section": "",
    "text": "Lastly Dockerhub deserves a special mention. Personally Dockerhub is what makes Docker truly powerful. It‚Äôs what github is to git, a open platform to share your Docker images.\nMy Docker image for Machine Learning and data science is availale here: https://hub.docker.com/r/sachinruk/ml_class/"
  },
  {
    "objectID": "blog/2015-10-20-Reversible-Jump-MCMC.html",
    "href": "blog/2015-10-20-Reversible-Jump-MCMC.html",
    "title": "Reversible jump MCMC",
    "section": "",
    "text": "Reversible jump MCMC is a Bayesian algorithm to infer the number of components/ clusters from a set of data. For this illustration we shall consider a two component model at most.\n\n\nThe likelihoods can be represented as: \\[\n\\begin{align}\np(y_i|\\lambda_{11},k=1)=&\\lambda_{11}\\exp(-\\lambda_{11}y_i)\\\\\np(y_i|\\lambda_{12},\\lambda_{22},k=2,z_i)=&\\prod_j (\\lambda_{j2}\\exp(-\\lambda_{j2}y_i))^{1(z_i=j)}\n\\end{align}\n\\]\nThe priors on the latent variables are:\n\\[\n\\begin{align}\np(\\lambda_{jk})\\propto & \\frac{1}{\\lambda_{jk}}\\qquad \\lambda_{jk}\\in[a,b]\\\\\np(z_i=1)=&\\pi\\\\\np(\\pi) = & \\text{Dir}(\\alpha)\np(k=j)= & 1/K\n\\end{align}\n\\]\n\n\n\nWe need to consider a Metropolis-Hastings (MH) step to consider going from one component to two components. The MH step in general is as follows:\n\\[\n\\begin{align}\n\\alpha = & \\frac{p(y,\\theta_2^{t+1})}{p(y,\\theta_1^t)}\\frac{q(\\theta_1^t|\\theta_2^{t+1})}{q(\\theta_2^{t+1}|\\theta_1^{t})}\\\\\nA = & \\text{min}\\left(1,\\alpha\\right)\n\\end{align}\n\\]\nwhere,\n\\[\n\\begin{align}\np(y_i,\\theta_2)=& p(y|\\lambda_{12},\\lambda_{22},\\pi)p(\\lambda_{12})p(\\lambda_{22})p(\\pi)\\\\\n=&\\pi p(y_i|\\lambda_{12})+(1-\\pi) p(y_i|\\lambda_{22})\n\\end{align}\n\\]\n\n\nIn this case let the parameters \\[\\theta=\\{\\cup_j\\lambda_{jk},k,\\pi\\}\\] . As we can let the proposal distribution be anything, we let \\[q(\\theta_1\\to\\theta_2)\\] as follows: \\[\n\\begin{align}\nq(\\lambda_{j2},\\pi,k=2|k=1,\\lambda_{11})=q(\\lambda_{j2}|k=2,\\lambda_{11})q(\\pi|k=2)q(k=2|k=1)\n\\end{align}\n\\]\nWe let the proposal \\[q(k=2\\vert k=1)=1\\]. We also have the following dimensional jump:\n\\[\n\\begin{align}\n\\mu_1,\\mu_2\\sim & U(0,1)\\\\\n\\lambda_{12}=&\\lambda_{11}\\frac{\\mu_1}{1-\\mu_1}\\\\\n\\lambda_{22}=&\\lambda_{11}\\frac{1-\\mu_1}{\\mu_1}\\\\\n\\pi=&\\mu_2\n\\end{align}\n\\]\nThus, in order to find the distribution (q({j2}k=2,{11})) we use the change of variable identity that \\[q(\\lambda_{j2}\\vert k=2,\\lambda_{11})=q(\\mu_1)\\vert J\\vert\\] where, \\[J\\] is the jacobian \\[\\frac{\\partial(\\lambda_{11},\\mu_1)}{\\partial(\\lambda_{12},\\lambda_{22})}\\]. The Jacobian determinant is found to be \\[\\frac{\\mu_1(1-\\mu_1)}{2\\lambda_{11}}\\] while \\[q(\\mu_1)=q(\\mu_2)=1\\] since they are sampled from standard uniform distributions. Also (q(_2)=q(k=2)).\nSince we need the ratio of proposed states ( ) we are also required to find ( q({11},k=2{2j},,k=1) = q({11}{2j},k=2) q(k=1 k=2) ). We again take ( q(k=1k=2)=1 ). (q(_{11}=)=1)\n\n\n\nThe MH step is conducted using the reciprocal of \\(\\alpha\\) in the equation above."
  },
  {
    "objectID": "blog/2015-10-20-Reversible-Jump-MCMC.html#model",
    "href": "blog/2015-10-20-Reversible-Jump-MCMC.html#model",
    "title": "Reversible jump MCMC",
    "section": "",
    "text": "The likelihoods can be represented as: \\[\n\\begin{align}\np(y_i|\\lambda_{11},k=1)=&\\lambda_{11}\\exp(-\\lambda_{11}y_i)\\\\\np(y_i|\\lambda_{12},\\lambda_{22},k=2,z_i)=&\\prod_j (\\lambda_{j2}\\exp(-\\lambda_{j2}y_i))^{1(z_i=j)}\n\\end{align}\n\\]\nThe priors on the latent variables are:\n\\[\n\\begin{align}\np(\\lambda_{jk})\\propto & \\frac{1}{\\lambda_{jk}}\\qquad \\lambda_{jk}\\in[a,b]\\\\\np(z_i=1)=&\\pi\\\\\np(\\pi) = & \\text{Dir}(\\alpha)\np(k=j)= & 1/K\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/2015-10-20-Reversible-Jump-MCMC.html#jumping-dimensions",
    "href": "blog/2015-10-20-Reversible-Jump-MCMC.html#jumping-dimensions",
    "title": "Reversible jump MCMC",
    "section": "",
    "text": "We need to consider a Metropolis-Hastings (MH) step to consider going from one component to two components. The MH step in general is as follows:\n\\[\n\\begin{align}\n\\alpha = & \\frac{p(y,\\theta_2^{t+1})}{p(y,\\theta_1^t)}\\frac{q(\\theta_1^t|\\theta_2^{t+1})}{q(\\theta_2^{t+1}|\\theta_1^{t})}\\\\\nA = & \\text{min}\\left(1,\\alpha\\right)\n\\end{align}\n\\]\nwhere,\n\\[\n\\begin{align}\np(y_i,\\theta_2)=& p(y|\\lambda_{12},\\lambda_{22},\\pi)p(\\lambda_{12})p(\\lambda_{22})p(\\pi)\\\\\n=&\\pi p(y_i|\\lambda_{12})+(1-\\pi) p(y_i|\\lambda_{22})\n\\end{align}\n\\]\n\n\nIn this case let the parameters \\[\\theta=\\{\\cup_j\\lambda_{jk},k,\\pi\\}\\] . As we can let the proposal distribution be anything, we let \\[q(\\theta_1\\to\\theta_2)\\] as follows: \\[\n\\begin{align}\nq(\\lambda_{j2},\\pi,k=2|k=1,\\lambda_{11})=q(\\lambda_{j2}|k=2,\\lambda_{11})q(\\pi|k=2)q(k=2|k=1)\n\\end{align}\n\\]\nWe let the proposal \\[q(k=2\\vert k=1)=1\\]. We also have the following dimensional jump:\n\\[\n\\begin{align}\n\\mu_1,\\mu_2\\sim & U(0,1)\\\\\n\\lambda_{12}=&\\lambda_{11}\\frac{\\mu_1}{1-\\mu_1}\\\\\n\\lambda_{22}=&\\lambda_{11}\\frac{1-\\mu_1}{\\mu_1}\\\\\n\\pi=&\\mu_2\n\\end{align}\n\\]\nThus, in order to find the distribution (q({j2}k=2,{11})) we use the change of variable identity that \\[q(\\lambda_{j2}\\vert k=2,\\lambda_{11})=q(\\mu_1)\\vert J\\vert\\] where, \\[J\\] is the jacobian \\[\\frac{\\partial(\\lambda_{11},\\mu_1)}{\\partial(\\lambda_{12},\\lambda_{22})}\\]. The Jacobian determinant is found to be \\[\\frac{\\mu_1(1-\\mu_1)}{2\\lambda_{11}}\\] while \\[q(\\mu_1)=q(\\mu_2)=1\\] since they are sampled from standard uniform distributions. Also (q(_2)=q(k=2)).\nSince we need the ratio of proposed states ( ) we are also required to find ( q({11},k=2{2j},,k=1) = q({11}{2j},k=2) q(k=1 k=2) ). We again take ( q(k=1k=2)=1 ). (q(_{11}=)=1)\n\n\n\nThe MH step is conducted using the reciprocal of \\(\\alpha\\) in the equation above."
  },
  {
    "objectID": "blog/2015-08-06-Sample-Variance.html",
    "href": "blog/2015-08-06-Sample-Variance.html",
    "title": "Sample Variance",
    "section": "",
    "text": "People often question why is there a ‚Äún-1‚Äù term when I calculate the variance. Why not divide through by ‚Äún‚Äù. Most stats courses dismiss this question by saying, ‚Äúoh, that‚Äôs because you lose a degree of freedom‚Äù. What is a degree of freedom. In the video below we ignore this notion of degree of freedom and answer where the ‚Äún-1‚Äù came from when calculating sample variance."
  },
  {
    "objectID": "blog/2021-10-10-zero-shot-classification-with-hf.html",
    "href": "blog/2021-10-10-zero-shot-classification-with-hf.html",
    "title": "Zero Shot Classification with Huggingface + Sentence Transformers ü§ó ü§ñ",
    "section": "",
    "text": "Photo credit: Bert."
  },
  {
    "objectID": "blog/2021-10-10-zero-shot-classification-with-hf.html#introduction",
    "href": "blog/2021-10-10-zero-shot-classification-with-hf.html#introduction",
    "title": "Zero Shot Classification with Huggingface + Sentence Transformers ü§ó ü§ñ",
    "section": "Introduction",
    "text": "Introduction\nWhen it comes to text classification Bert/ Distilbert is our goto. However, quite often we lack labels to start off our classification process. Huggingface released a tool about a year ago to do exactly this but by using BART. The concept behind zero shot classification is to match the text to a topic word. The words used in a topic sentence contains information that describes the cluster as opposed to a one hot encoded vector."
  },
  {
    "objectID": "blog/2021-10-10-zero-shot-classification-with-hf.html#whats-wrong-with-bart",
    "href": "blog/2021-10-10-zero-shot-classification-with-hf.html#whats-wrong-with-bart",
    "title": "Zero Shot Classification with Huggingface + Sentence Transformers ü§ó ü§ñ",
    "section": "What‚Äôs wrong with BART?",
    "text": "What‚Äôs wrong with BART?\nI personally believe that BART is a heavy handed way of doing this as it‚Äôs complexity is O(NK) whereas, using a sentence transformer, the complexity is roughly O(N + K) (where N is the number of sentences and K is the number of topics).\nWhen using BART to check if a topic is similar to a word, we must concatenate the sentence along with the potential topic (seperated by a &lt;SEP&gt; token) and pass it through a BART transformer. This needs to be done against all potential topics. BART outputs a probability of the two sentences being neutral (nothing to do with each other), entailing and contradictions. In the HF repo the entailment probabilities are normalised across topics to choose the most likely topic."
  },
  {
    "objectID": "blog/2021-10-10-zero-shot-classification-with-hf.html#sentence-transformers",
    "href": "blog/2021-10-10-zero-shot-classification-with-hf.html#sentence-transformers",
    "title": "Zero Shot Classification with Huggingface + Sentence Transformers ü§ó ü§ñ",
    "section": "Sentence Transformers",
    "text": "Sentence Transformers\nSentence Transformers are used to summarise a sentence into a single vector. Therefore this is ideal to compare a sentence against and works reasonably well as shown below. One other benefit of using Sentence Transformers is that they offer a small model (127 MB!) compared to BART which is 500MB. One other benfit that is given for free is the fact that the sentence transformer is multilingual!"
  },
  {
    "objectID": "blog/2021-10-10-zero-shot-classification-with-hf.html#experiment",
    "href": "blog/2021-10-10-zero-shot-classification-with-hf.html#experiment",
    "title": "Zero Shot Classification with Huggingface + Sentence Transformers ü§ó ü§ñ",
    "section": "Experiment",
    "text": "Experiment\nTo demonstrate zero shot classification I have used the News 20 dataset to classify news articles into one of 20 topics such as politics, religion, baseball etc.\nIn order to calculate the sentence embedding the mean_pooling function takes all the token embedding transformed outputs and averages them. We go further and normalise these embedding vectors to be of unit length.\n\n\nCode\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\ndef get_embedding_batch(model, tokenizer, sentences: List[str]) -&gt; torch.FloatTensor:\n    x = tokenizer(\n        sentences, \n        max_length=MAX_TEXT_LENGTH, \n        truncation=True, \n        padding=\"max_length\", \n        return_tensors=\"pt\"\n    )\n    x_dev = {k: v.to(device) for k, v in x.items()}\n    out = model(**x_dev)\n    embeds = mean_pooling(out, x_dev[\"attention_mask\"]).cpu()\n    embed_lens = torch.norm(embeds, dim=-1, keepdim=True)\n    return embeds / embed_lens\n\ndef get_embeddings(model, tokenizer, sentences, batch_size):\n    with torch.no_grad():\n        embeds = []\n        for i in tqdm(range(0, len(sentences), batch_size)):\n            embeds.append(get_embedding_batch(model, tokenizer, sentences[i:i+batch_size]))\n    \n    return torch.cat(embeds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe pass the topics as well as the candidate sentences through the sentence transformer separately. By taking the product we are able to get a similarity metric. Below we add one and halve it to ensure the number lies between [0, 1]. Strictly speaking this rescaling is not necessary.\n\nsimilarity = 0.5 * (1 + sentence_embeds @ topic_embeds.T)\nconfidence, idx = similarity.topk(k=2)\n\nAs can be seen below, even when it does get it wrong, the predictions are close. If your topics are quite distinct you might observe better results than what is shown below.\n\n\n\n\n\n\n\n\n\nTrue Topic\nPredicted Topic\nConfidence\n\n\n\n\n0\nhockey\n[hockey, baseball]\n[0.64, 0.58]\n\n\n1\nsys ibm pc hardware\n[graphics, sys ibm pc hardware]\n[0.63, 0.62]\n\n\n2\nmiddle east\n[middle east, politics guns]\n[0.64, 0.60]\n\n\n3\nsys ibm pc hardware\n[sys mac hardware, sys ibm pc hardware]\n[0.69, 0.68]\n\n\n4\nsys mac hardware\n[sys ibm pc hardware, sys mac hardware]\n[0.64, 0.61]\n\n\n\n\n\n\n\nLooking at the top k accuracy we get the following result:\n\n\nTop 1 Accuracy is: 58.04%, Top 2 Accuracy is: 58.82%"
  },
  {
    "objectID": "blog/2021-10-10-zero-shot-classification-with-hf.html#shameless-self-promotion",
    "href": "blog/2021-10-10-zero-shot-classification-with-hf.html#shameless-self-promotion",
    "title": "Zero Shot Classification with Huggingface + Sentence Transformers ü§ó ü§ñ",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2022-01-26-visionencoderdecoder-model-training.html",
    "href": "blog/2022-01-26-visionencoderdecoder-model-training.html",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers - Part 2",
    "section": "",
    "text": "If you‚Äôve read my previous post, this post is slightly different. Here we will focus on how we read in data, some training tricks we used, along with logging, and finally how we pushed the model up into ü§ó. The training code can be found on kaggle."
  },
  {
    "objectID": "blog/2022-01-26-visionencoderdecoder-model-training.html#introduction",
    "href": "blog/2022-01-26-visionencoderdecoder-model-training.html#introduction",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers - Part 2",
    "section": "",
    "text": "If you‚Äôve read my previous post, this post is slightly different. Here we will focus on how we read in data, some training tricks we used, along with logging, and finally how we pushed the model up into ü§ó. The training code can be found on kaggle."
  },
  {
    "objectID": "blog/2022-01-26-visionencoderdecoder-model-training.html#data",
    "href": "blog/2022-01-26-visionencoderdecoder-model-training.html#data",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers - Part 2",
    "section": "Data",
    "text": "Data\nWhile we can use the VitFeatureExtractor directly from HF, this doesn‚Äôt allow you to do any augmentations. Digging into the VitFeatureExtractor all it does is 1. normalize the pixel values to be 0 and 1 (by dividing by 255) 2. Minusing a ‚Äòmean‚Äô value of 0.5 and dividing by a ‚Äòstandard deviation‚Äô value of 0.5. Given this we can do the following augmentations to our training data:\ntfms = transforms.Compose(\n    [\n        transforms.Resize(IMAGE_SIZE),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomApply([transforms.RandomRotation(degrees=20)], p=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=MEAN, std=STD)\n   ]\n)\nThis states that 50% of the time we flip the image horizontally, and 10% of the time we would then apply a random rotation of +/- 20 degrees."
  },
  {
    "objectID": "blog/2022-01-26-visionencoderdecoder-model-training.html#training-module",
    "href": "blog/2022-01-26-visionencoderdecoder-model-training.html#training-module",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers - Part 2",
    "section": "Training Module",
    "text": "Training Module\nMy previous post talked about the actual model and loss. The main difference here is that I used the VisionEncoderDecoder along with its generate function instead of EncoderDecoder model where I had to implement my own generate function. Instead I want to focus here on how we trained the model and logged results.\n\nFreezing/ Unfreezing model\nWhen initialising a encoder decoder model, the cross attention weights are initialised randomly and do not mean anything. We could simply use a low learning rate and train the model, however, I opted to freeze the already trained parts for the first epoch. Kudos to Jeremy Howard + Rachel Thomas‚Äô for this trick I learnt in their Fastai course.\nWhen freezing models it is not enough to simply set parameters requires_grad=False, you need to make sure every submodule is set to .eval(). This is because the parameters of the frozen modules will continue to update in for example the normalization layers, and any dropout will continue to drop out inputs. By inspecting the model we were able to see that we needed to unfreeze any layer that had crossattention in it and ln_cross_attn layers which were the layer normalizations associated with the former.\nmodel.eval()\n    for p in model.parameters():\n        p.requires_grad = False\n\n    # only allow training of cross attention parameters\n    for layer in model.decoder.transformer.h:\n        layer.crossattention.train()\n        for p in layer.crossattention.parameters():\n            p.requires_grad = True\n        layer.ln_cross_attn.train()\n        for p in layer.ln_cross_attn.parameters():\n            p.requires_grad = True\n\n\nLogging metrics/ results/ weights\nLogging loss was the simple part. In PytorchLightning you simply had to do self.log(name=\"Training loss\", value=loss, on_step=True, on_epoch=True) within one of the steps. However, I wanted to log the results of images and its generated text. I only did this for a single batch in my validation step. This allowed me to compare results across epochs:\nif batch_idx == 0:\n    images = [wandb.Image(transforms.ToPILImage()(descale(image))) for image in images]\n    data = list(map(list, zip(images, actual_sentences, generated_sentences)))\n    columns = [\"Images\", \"Actual Sentence\", \"Generated Sentence\"]\n    table = wandb.Table(data=data, columns=columns)\n    self.logger.experiment.log({f\"epoch {self.current_epoch} results\": table})\nHere are the results of the last epoch: \nWe can (and should) log the gradients too. In this case since there are many parameter groups we will only log the ones with cross attention. We can do this by using wandb.Histogram function.\ndef on_after_backward(self):\n    if self.trainer.global_step % 50 == 0:  # don't make the tf file huge\n        for name, param in self.model.named_parameters():\n            if \"crossattention\" in name and not \"norm\" in name and param.requires_grad:\n                self.logger.experiment.log(\n                    {f\"{name}_grad\": wandb.Histogram(param.grad.detach().cpu())}\n                )\n                self.logger.experiment.log(\n                    {f\"{name}\": wandb.Histogram(param.detach().cpu())}\n                )\nThe histogram over time can be seen below. Note how large the gradient distribution is until end of epoch 1 where we unfreeze everything. Just to be cautious though I reduced the learning rate of everything by a factor of 10 at the end of the first epoch. \nWhile I was training I was a bit worried that the kaggle kernel would die since there was a time limit. Therefore I decided to log the weights each epoch. This was achieved by doing the following in pytorch-lightning.\ndef on_train_epoch_end(self, *args):\n    model_name = f\"model-epoch-{self.current_epoch}.ckpt\"\n    model_path = f\"/kaggle/working/{model_name}\"\n    torch.save(self.model.state_dict(), model_path)\n    self.logger.experiment.log_artifact(\n        artifact_or_path=model_path,\n        name=model_name,\n        type=\"model\",\n    )"
  },
  {
    "objectID": "blog/2022-01-26-visionencoderdecoder-model-training.html#pushing-to-huggingface-library",
    "href": "blog/2022-01-26-visionencoderdecoder-model-training.html#pushing-to-huggingface-library",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers - Part 2",
    "section": "Pushing to HuggingFace library",
    "text": "Pushing to HuggingFace library\nThis was the easy part. Go to https://huggingface.co/ and register for and account, and then make a token with write access in https://huggingface.co/settings/token. Once you have this you can simply do model.push_to_hub(\"my-awesome-model\", access_token=token) where token is the string you generated."
  },
  {
    "objectID": "blog/2022-01-26-visionencoderdecoder-model-training.html#shameless-self-promotion",
    "href": "blog/2022-01-26-visionencoderdecoder-model-training.html#shameless-self-promotion",
    "title": "Generating captions with ViT and GPT2 using ü§ó Transformers - Part 2",
    "section": "Shameless Self Promotion",
    "text": "Shameless Self Promotion\nIf you enjoyed the tutorial buy my course (usually 90% off)."
  },
  {
    "objectID": "blog/2020-11-28-focal-loss.html",
    "href": "blog/2020-11-28-focal-loss.html",
    "title": "Focal Loss for Multi-class Classification",
    "section": "",
    "text": "class WeightedFocalLoss(nn.Module):\n    \"Non weighted version of Focal Loss\"\n    def __init__(self, weights, gamma=1.1):\n        super().__init__()\n        self.weights = weights\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        inputs = inputs.squeeze()\n        targets = targets.squeeze()\n\n        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.weights[targets]*(1-pt)**self.gamma * BCE_loss\n\n        return F_loss.mean()"
  },
  {
    "objectID": "blog/2016-10-16-Quantile-Regression.html",
    "href": "blog/2016-10-16-Quantile-Regression.html",
    "title": "Deep Learning Quantile Regression - Keras",
    "section": "",
    "text": "The loss function is simple as doing the following. Which is simply the pin-ball loss function.\ndef tilted_loss(q,y,f):\n    e = (y-f)\n    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)\nWhen it comes to compiling the neural network, just simply do:\nmodel.compile(loss=lambda y,f: tilted_loss(0.5,y,f), optimizer='adagrad')\nI chose 0.5 which is the median, but you can try whichever quantile that you are after. Word of caution, which applies to any quantile regression method; you may find that the quantile output might be extreme/ unexpected when you take extreme quantiles (eg. 0.001 or 0.999).\nA more complete working example can be found here."
  },
  {
    "objectID": "ML_consulting.html",
    "href": "ML_consulting.html",
    "title": "deepschool.ai",
    "section": "",
    "text": "Welcome to DeepSchool.ai!\nHarness the Power of Deep Learning for Your Business Transformation\nAre you ready to unlock the potential of artificial intelligence and revolutionize your business operations? Look no further. We specialize in providing cutting-edge deep learning solutions tailored to meet your unique needs and drive significant growth and innovation.\nWhy Choose Us?\n\nExpertise and Experience: Our team of seasoned deep learning professionals brings a wealth of expertise and experience in the field. We stay at the forefront of the latest advancements in artificial intelligence, ensuring that we deliver state-of-the-art solutions that give your business a competitive edge.\nCustomized Solutions: We understand that every business is unique, which is why we take a personalized approach to developing deep learning solutions. Our team will work closely with you to understand your goals, challenges, and requirements, creating customized solutions that address your specific needs and deliver tangible results.\nComprehensive Services: From initial consultation to implementation and ongoing support, we offer a comprehensive range of services to guide you through your deep learning journey. Whether you need assistance with data preparation, model development, or deployment strategies, we have you covered at every step of the way.\nCutting-Edge Technology: At [Your Deep Learning Consulting Business], we leverage the latest advancements in deep learning technology and tools to build robust and scalable solutions. We combine the power of neural networks, natural language processing, computer vision, and more to develop intelligent systems that can solve complex business challenges.\nROI-Driven Approach: We are committed to delivering measurable results and maximizing your return on investment. Our team focuses on developing solutions that not only enhance efficiency and accuracy but also drive revenue growth, reduce costs, and improve overall business performance.\nConfidentiality and Security: We prioritize the security and confidentiality of your data. Rest assured that your information is handled with the utmost care and protected through rigorous security measures, ensuring that your intellectual property remains safe and secure.\n\nUnlock the Potential of Deep Learning Today!\nTake the first step towards transforming your business with the power of deep learning. Contact [Your Deep Learning Consulting Business] today to schedule a consultation with our experts. Together, let‚Äôs unlock the unlimited possibilities that artificial intelligence has to offer and take your business to new heights."
  },
  {
    "objectID": "programming_tips/choosing-timm-model.html",
    "href": "programming_tips/choosing-timm-model.html",
    "title": "Timm choosing a model",
    "section": "",
    "text": "Choosing a backbone for transfer learning is easy thanks to Timm. However, which model is a mystery. I use the following to use a small model which has &gt;80% accuracy on imagenet and has less than 50M parameters.\nimport pandas as pd\n\nURL = \"https://raw.githubusercontent.com/rwightman/pytorch-image-models/master/results/results-imagenet.csv\"\nINFER_URL = \"https://raw.githubusercontent.com/rwightman/pytorch-image-models/main/results/benchmark-infer-amp-nchw-pt112-cu113-rtx3090.csv\"\n\ndf = pd.read_csv(URL)\ndf2 = pd.read_csv(INFER_URL)\n# df[\"param_count\"] = df[\"param_count\"].astype(str)\n# df2[\"param_count\"] = df2[\"param_count\"].astype(str)\ndf[\"model_base\"] = df[\"model\"].map(lambda x: x.split(\".\")[0])\ndf2.rename(columns={\"model\": \"model_base\"}, inplace=True)\n\ndf = df.join(df2.set_index(\"model_base\").drop(\"param_count\", axis=1), on=\"model_base\")\ndf[\"param_count\"] = df[\"param_count\"].str.replace(\",\",\"\")\ndf[\"param_count\"] = df[\"param_count\"].astype(float)\n\ntop1_cond = df[\"top1\"] &gt; 80\nparam_count_cond = df[\"param_count\"] &lt; 50\ndf[param_count_cond & top1_cond].sort_values(\"top1\", ascending=False)\nThis is the same result but ordered by increasing number of parameters.\ndf[param_count_cond & top1_cond].sort_values(\"param_count\").iloc[:10]"
  },
  {
    "objectID": "programming_tips/pl-learning-rate-finder.html",
    "href": "programming_tips/pl-learning-rate-finder.html",
    "title": "Pytorch Lightning Learning rate finder",
    "section": "",
    "text": "The following plots the loss against learning rate in order to discover an ‚Äúoptimal‚Äù learning rate.\nlr_finder = trainer.tuner.lr_find(lightning_module, train_dataloaders=train_dl)\nfig = lr_finder.plot(suggest=True)\nfig.show()\nWhen all else fails, 1e-3."
  },
  {
    "objectID": "DL_Course.html",
    "href": "DL_Course.html",
    "title": "Deep Learning Course",
    "section": "",
    "text": "I have created a DL Course on Udemy to take you from zero to hero. This course is designed to provide you with a comprehensive understanding of deep learning techniques and their applications. Throughout this course, you will learn various concepts, tools, and practical skills related to deep learning.\nThe following curriculum covers a wide range of topics and progresses from beginner to intermediate and advanced levels. Whether you are new to deep learning or have some experience, this course will provide you with the knowledge and skills to excel in the field.\n\n\n\n\n\nIntroduction\nHow to tackle this course\nInstallations and sign-ups\nJupyter Notebooks\nCourse Material\nGoogle Drive Link for All Course Material\n\n\n\n\n\nIntro\nBasic Data Structures\nDictionaries\nPython functions (methods)\nNumpy functions\nConditional statements\nFor loops\nDictionaries again\nPandas Intro\n\n\n\n\n\nPandas simple functions\nPandas: Subsetting\nPandas: loc and iloc\nPandas: loc and iloc 2\nPandas: map and apply\nPandas: groupby\n\n\n\n\n\nPlotting resources (notebooks)\nLine plot\nPlot multiple lines\nHistograms\nScatter Plots\nSubplots\nSeaborn + pair plots\n\n\n\n\n\n\n\n\nYour reviews are important to me!\nNumpy\nGradient Descent\nKmeans part 1\nKmeans part 2\nBroadcasting\n\n\n\n\n\nIntro\nLinear Regression Part 1\nLinear Regression Part 2\nClassification and Regression Trees\nCART part 2\nRandom Forest theory\nRandom Forest Code\nGradient Boosted Machines\n\n\n\n\n\nKaggle part 1\nKaggle part 2\nTheory part 1\nTheory part 2 + code\nTitanic dataset\nSklearn classification prelude\nSklearn classification\nDealing with missing values\n\n\n\n\n\nIntro\nLoss functions\nFB Prophet part 1\nFB Prophet part 2\nTheory behind FB Prophet\n\n\n\n\n\nOverfitting\nCross Validation\nStratified K Fold\nArea Under Curve (AUC) Part 1\nArea Under Curve (AUC) Part 2\n\n\n\n\n\nPrincipal Component Analysis (PCA) theory\nFashion MNIST PCA\nK-means\nOther clustering methods\nDBSCAN theory\nGaussian Mixture Models (GMM) theory\n\n\n\n\n\nIntro\nStop words and Term Frequency\nTerm Frequency - Inverse Document Frequency (Tf - Idf) theory\nFinancial News Sentiment Classifier\nNLTK + Stemming\nN-grams\nWord (feature) importance\nSpacy intro\nFeature Extraction with Spacy (using Pandas)\nClassification Example\nOver-sampling\n\n\n\n\n\nIntroduction\nMSE recap\nL2 Loss / Ridge Regression intro\nRidge regression (L2 penalized regression)\nS&P500 data preparation for L1 loss\nL1 Penalized Regression (Lasso)\nL1/ L2 Penalty theory: why it works\n\n\n\n\n\nIntro\nDL theory part 1\nDL theory part 2\nTensorflow + Keras demo problem 1\nActivation functions\nFirst example with Relu\nMNIST\n\nand Softmax - Deep Learning Input Normalization - Softmax theory - Batch Norm - Batch Norm Theory\n\n\n\n\nIntro\nFashion MNIST feed forward net for benchmarking\nKeras Conv2D layer\nModel fitting and discussion of results\nDropout theory and code\nMaxPool (and comparison to stride)\nCifar-10\nNose Tip detection with CNNs\n\n\n\n\n\nWord2vec and Embeddings\nKaggle + Word2Vec\nWord2Vec: Keras Model API\nRecurrent Neural Nets - Theory\nDeep Learning - Long Short Term Memory (LSTM) Nets\nDeep Learning - Stacking LSTMs + GRUs\nTransfer Learning - GLOVE vectors\nSequence to Sequence Introduction + Data Prep\nSequence to Sequence model + Keras Model API\nSequence to Sequence models: Prediction step\n\n\n\n\n\nNotebooks\nIntroduction\nPytorch: TensorDataset\nPytorch: Dataset and DataLoaders\nDeep Learning with PyTorch: nn.Sequential models\nDeep Learning with PyTorch: Loss functions\nDeep Learning with PyTorch: Stochastic Gradient Descent\nDeep Learning with PyTorch: Optimizers\nPytorch Model API\nPytorch in GPUs\nDeep Learning: Intro to Pytorch Lightning\n\n\n\n\n\nNotebooks\nTransfer Learning Introduction\nKaggle problem description\nPyTorch datasets + Torchvision\nPyTorch transfer learning with ResNet\nPyTorch Lightning Model\nPyTorch Lightning Trainer + Model evaluation\nDeep Learning for Cassava Leaf Classification\nCassava Leaf Dataset\nData Augmentation with Torchvision Transforms\nTrain vs Test Augmentations + DataLoader parameters\nDeep Learning: Transfer Learning Model with ResNet\nSetting up PyTorch Lightning for training\nCross Entropy Loss for Imbalanced Classes\nPyTorch Test dataset setup and evaluation\nWandB for logging experiments\n\n\n\n\n\n\n\n\nNotebooks\nIntroduction\nCoco Dataset + Augmentations for Segmentation with Torchvision\nUnet Architecture overview\nPyTorch Model Architecture\nPyTorch Hooks\nPyTorch Hooks: Step through with breakpoints\nPyTorch Weighted CrossEntropy Loss\nWeights and Biases: Logging images.\nSemantic Segmentation training with PyTorch Lightning\n\n\n\n\n\nResources\nIntroduction to Transformers\nThe illustrated Transformer (blog post by Jay Alammar)\nEncoder Transformer Models: The Maths\nBERT - The theory\nKaggle Multi-lingual Toxic Comment Classification Challenge\nTokenizers and data prep for BERT models\nDistilbert (Smaller BERT) model\nPyTorch Lightning + DistilBERT for classification"
  },
  {
    "objectID": "DL_Course.html#beginner-level",
    "href": "DL_Course.html#beginner-level",
    "title": "Deep Learning Course",
    "section": "",
    "text": "Introduction\nHow to tackle this course\nInstallations and sign-ups\nJupyter Notebooks\nCourse Material\nGoogle Drive Link for All Course Material\n\n\n\n\n\nIntro\nBasic Data Structures\nDictionaries\nPython functions (methods)\nNumpy functions\nConditional statements\nFor loops\nDictionaries again\nPandas Intro\n\n\n\n\n\nPandas simple functions\nPandas: Subsetting\nPandas: loc and iloc\nPandas: loc and iloc 2\nPandas: map and apply\nPandas: groupby\n\n\n\n\n\nPlotting resources (notebooks)\nLine plot\nPlot multiple lines\nHistograms\nScatter Plots\nSubplots\nSeaborn + pair plots"
  },
  {
    "objectID": "DL_Course.html#intermediate-level",
    "href": "DL_Course.html#intermediate-level",
    "title": "Deep Learning Course",
    "section": "",
    "text": "Your reviews are important to me!\nNumpy\nGradient Descent\nKmeans part 1\nKmeans part 2\nBroadcasting\n\n\n\n\n\nIntro\nLinear Regression Part 1\nLinear Regression Part 2\nClassification and Regression Trees\nCART part 2\nRandom Forest theory\nRandom Forest Code\nGradient Boosted Machines\n\n\n\n\n\nKaggle part 1\nKaggle part 2\nTheory part 1\nTheory part 2 + code\nTitanic dataset\nSklearn classification prelude\nSklearn classification\nDealing with missing values\n\n\n\n\n\nIntro\nLoss functions\nFB Prophet part 1\nFB Prophet part 2\nTheory behind FB Prophet\n\n\n\n\n\nOverfitting\nCross Validation\nStratified K Fold\nArea Under Curve (AUC) Part 1\nArea Under Curve (AUC) Part 2\n\n\n\n\n\nPrincipal Component Analysis (PCA) theory\nFashion MNIST PCA\nK-means\nOther clustering methods\nDBSCAN theory\nGaussian Mixture Models (GMM) theory\n\n\n\n\n\nIntro\nStop words and Term Frequency\nTerm Frequency - Inverse Document Frequency (Tf - Idf) theory\nFinancial News Sentiment Classifier\nNLTK + Stemming\nN-grams\nWord (feature) importance\nSpacy intro\nFeature Extraction with Spacy (using Pandas)\nClassification Example\nOver-sampling\n\n\n\n\n\nIntroduction\nMSE recap\nL2 Loss / Ridge Regression intro\nRidge regression (L2 penalized regression)\nS&P500 data preparation for L1 loss\nL1 Penalized Regression (Lasso)\nL1/ L2 Penalty theory: why it works\n\n\n\n\n\nIntro\nDL theory part 1\nDL theory part 2\nTensorflow + Keras demo problem 1\nActivation functions\nFirst example with Relu\nMNIST\n\nand Softmax - Deep Learning Input Normalization - Softmax theory - Batch Norm - Batch Norm Theory\n\n\n\n\nIntro\nFashion MNIST feed forward net for benchmarking\nKeras Conv2D layer\nModel fitting and discussion of results\nDropout theory and code\nMaxPool (and comparison to stride)\nCifar-10\nNose Tip detection with CNNs\n\n\n\n\n\nWord2vec and Embeddings\nKaggle + Word2Vec\nWord2Vec: Keras Model API\nRecurrent Neural Nets - Theory\nDeep Learning - Long Short Term Memory (LSTM) Nets\nDeep Learning - Stacking LSTMs + GRUs\nTransfer Learning - GLOVE vectors\nSequence to Sequence Introduction + Data Prep\nSequence to Sequence model + Keras Model API\nSequence to Sequence models: Prediction step\n\n\n\n\n\nNotebooks\nIntroduction\nPytorch: TensorDataset\nPytorch: Dataset and DataLoaders\nDeep Learning with PyTorch: nn.Sequential models\nDeep Learning with PyTorch: Loss functions\nDeep Learning with PyTorch: Stochastic Gradient Descent\nDeep Learning with PyTorch: Optimizers\nPytorch Model API\nPytorch in GPUs\nDeep Learning: Intro to Pytorch Lightning\n\n\n\n\n\nNotebooks\nTransfer Learning Introduction\nKaggle problem description\nPyTorch datasets + Torchvision\nPyTorch transfer learning with ResNet\nPyTorch Lightning Model\nPyTorch Lightning Trainer + Model evaluation\nDeep Learning for Cassava Leaf Classification\nCassava Leaf Dataset\nData Augmentation with Torchvision Transforms\nTrain vs Test Augmentations + DataLoader parameters\nDeep Learning: Transfer Learning Model with ResNet\nSetting up PyTorch Lightning for training\nCross Entropy Loss for Imbalanced Classes\nPyTorch Test dataset setup and evaluation\nWandB for logging experiments"
  },
  {
    "objectID": "DL_Course.html#advanced-level",
    "href": "DL_Course.html#advanced-level",
    "title": "Deep Learning Course",
    "section": "",
    "text": "Notebooks\nIntroduction\nCoco Dataset + Augmentations for Segmentation with Torchvision\nUnet Architecture overview\nPyTorch Model Architecture\nPyTorch Hooks\nPyTorch Hooks: Step through with breakpoints\nPyTorch Weighted CrossEntropy Loss\nWeights and Biases: Logging images.\nSemantic Segmentation training with PyTorch Lightning\n\n\n\n\n\nResources\nIntroduction to Transformers\nThe illustrated Transformer (blog post by Jay Alammar)\nEncoder Transformer Models: The Maths\nBERT - The theory\nKaggle Multi-lingual Toxic Comment Classification Challenge\nTokenizers and data prep for BERT models\nDistilbert (Smaller BERT) model\nPyTorch Lightning + DistilBERT for classification"
  }
]